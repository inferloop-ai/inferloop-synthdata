{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSIOT Forecasting Models Example\n",
    "\n",
    "This notebook demonstrates how to use TSIOT-generated synthetic data to train and evaluate various forecasting models.\n",
    "\n",
    "## Models Covered:\n",
    "1. ARIMA\n",
    "2. Prophet\n",
    "3. LSTM\n",
    "4. GRU\n",
    "5. Transformer-based models\n",
    "6. Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests pandas numpy matplotlib seaborn scikit-learn statsmodels prophet tensorflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSIOT API configuration\n",
    "TSIOT_BASE_URL = \"http://localhost:8080\"\n",
    "API_KEY = \"your-api-key-here\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "# Generate time series data\n",
    "def generate_forecast_data():\n",
    "    \"\"\"Generate time series suitable for forecasting.\"\"\"\n",
    "    data = {\n",
    "        \"type\": \"lstm\",\n",
    "        \"length\": 2000,\n",
    "        \"parameters\": {\n",
    "            \"trend\": 0.1,\n",
    "            \"seasonality\": 24,\n",
    "            \"noise\": 0.05,\n",
    "            \"complexity\": \"high\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{TSIOT_BASE_URL}/api/v1/generate\", json=data, headers=headers)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "# Generate data\n",
    "ts_data = generate_forecast_data()\n",
    "if ts_data:\n",
    "    values = np.array(ts_data['values'])\n",
    "    timestamps = pd.date_range(start='2023-01-01', periods=len(values), freq='H')\n",
    "    df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    print(f\"✅ Generated {len(df)} data points\")\n",
    "else:\n",
    "    # Use synthetic data if API is not available\n",
    "    print(\"⚠️ Using synthetic fallback data\")\n",
    "    np.random.seed(42)\n",
    "    t = np.arange(2000)\n",
    "    trend = 0.1 * t\n",
    "    seasonal = 10 * np.sin(2 * np.pi * t / 24)\n",
    "    noise = np.random.normal(0, 2, 2000)\n",
    "    values = 100 + trend + seasonal + noise\n",
    "    timestamps = pd.date_range(start='2023-01-01', periods=2000, freq='H')\n",
    "    df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(df))\n",
    "train_data = df[:train_size]\n",
    "test_data = df[train_size:]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} points\")\n",
    "print(f\"Test data: {len(test_data)} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train_data.index, train_data['value'], label='Training Data', alpha=0.7)\n",
    "plt.plot(test_data.index, test_data['value'], label='Test Data', alpha=0.7)\n",
    "plt.axvline(x=train_data.index[-1], color='red', linestyle='--', alpha=0.5)\n",
    "plt.title('Time Series Data Split')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA forecasting\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Check stationarity\n",
    "def check_stationarity(timeseries):\n",
    "    result = adfuller(timeseries.dropna())\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:', result[4])\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"✅ Data is stationary\")\n",
    "    else:\n",
    "        print(\"❌ Data is non-stationary\")\n",
    "\n",
    "check_stationarity(train_data['value'])\n",
    "\n",
    "# If non-stationary, difference the data\n",
    "train_diff = train_data['value'].diff().dropna()\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_acf(train_diff, lags=40, ax=axes[0])\n",
    "plot_pacf(train_diff, lags=40, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARIMA model\n",
    "print(\"Fitting ARIMA model...\")\n",
    "arima_model = ARIMA(train_data['value'], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "print(arima_fit.summary())\n",
    "\n",
    "# Forecast\n",
    "arima_forecast = arima_fit.forecast(steps=len(test_data))\n",
    "arima_forecast = pd.Series(arima_forecast, index=test_data.index)\n",
    "\n",
    "# Calculate metrics\n",
    "arima_mse = mean_squared_error(test_data['value'], arima_forecast)\n",
    "arima_mae = mean_absolute_error(test_data['value'], arima_forecast)\n",
    "arima_r2 = r2_score(test_data['value'], arima_forecast)\n",
    "\n",
    "print(f\"\\nARIMA Performance:\")\n",
    "print(f\"MSE: {arima_mse:.4f}\")\n",
    "print(f\"MAE: {arima_mae:.4f}\")\n",
    "print(f\"R²: {arima_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Prophet\n",
    "prophet_train = train_data.reset_index()\n",
    "prophet_train.columns = ['ds', 'y']\n",
    "\n",
    "# Fit Prophet model\n",
    "print(\"Fitting Prophet model...\")\n",
    "prophet_model = Prophet(\n",
    "    daily_seasonality=True,\n",
    "    yearly_seasonality=False,\n",
    "    changepoint_prior_scale=0.05\n",
    ")\n",
    "prophet_model.fit(prophet_train)\n",
    "\n",
    "# Make predictions\n",
    "future = prophet_model.make_future_dataframe(periods=len(test_data), freq='H')\n",
    "prophet_forecast = prophet_model.predict(future)\n",
    "\n",
    "# Extract test predictions\n",
    "prophet_test_pred = prophet_forecast[['ds', 'yhat']].iloc[-len(test_data):]\n",
    "prophet_test_pred.set_index('ds', inplace=True)\n",
    "\n",
    "# Calculate metrics\n",
    "prophet_mse = mean_squared_error(test_data['value'], prophet_test_pred['yhat'])\n",
    "prophet_mae = mean_absolute_error(test_data['value'], prophet_test_pred['yhat'])\n",
    "prophet_r2 = r2_score(test_data['value'], prophet_test_pred['yhat'])\n",
    "\n",
    "print(f\"\\nProphet Performance:\")\n",
    "print(f\"MSE: {prophet_mse:.4f}\")\n",
    "print(f\"MAE: {prophet_mae:.4f}\")\n",
    "print(f\"R²: {prophet_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(data, look_back=24):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[['value']])\n",
    "\n",
    "# Split scaled data\n",
    "scaled_train = scaled_data[:train_size]\n",
    "scaled_test = scaled_data[train_size-24:]  # Include look_back period\n",
    "\n",
    "# Create datasets\n",
    "look_back = 24\n",
    "X_train, y_train = create_lstm_dataset(scaled_train.flatten(), look_back)\n",
    "X_test, y_test = create_lstm_dataset(scaled_test.flatten(), look_back)\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, input_shape=(look_back, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "lstm_predictions = lstm_model.predict(X_test)\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions)\n",
    "y_test_inverse = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_mse = mean_squared_error(y_test_inverse, lstm_predictions)\n",
    "lstm_mae = mean_absolute_error(y_test_inverse, lstm_predictions)\n",
    "lstm_r2 = r2_score(y_test_inverse, lstm_predictions)\n",
    "\n",
    "print(f\"\\nLSTM Performance:\")\n",
    "print(f\"MSE: {lstm_mse:.4f}\")\n",
    "print(f\"MAE: {lstm_mae:.4f}\")\n",
    "print(f\"R²: {lstm_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GRU model\n",
    "gru_model = Sequential([\n",
    "    GRU(50, activation='relu', return_sequences=True, input_shape=(look_back, 1)),\n",
    "    Dropout(0.2),\n",
    "    GRU(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "print(\"Training GRU model...\")\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "gru_predictions = gru_model.predict(X_test)\n",
    "gru_predictions = scaler.inverse_transform(gru_predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "gru_mse = mean_squared_error(y_test_inverse, gru_predictions)\n",
    "gru_mae = mean_absolute_error(y_test_inverse, gru_predictions)\n",
    "gru_r2 = r2_score(y_test_inverse, gru_predictions)\n",
    "\n",
    "print(f\"\\nGRU Performance:\")\n",
    "print(f\"MSE: {gru_mse:.4f}\")\n",
    "print(f\"MAE: {gru_mae:.4f}\")\n",
    "print(f\"R²: {gru_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions\n",
    "def create_ensemble(predictions_dict, weights=None):\n",
    "    \"\"\"Create weighted ensemble of predictions.\"\"\"\n",
    "    if weights is None:\n",
    "        weights = {model: 1/len(predictions_dict) for model in predictions_dict}\n",
    "    \n",
    "    ensemble_pred = np.zeros_like(list(predictions_dict.values())[0])\n",
    "    \n",
    "    for model, pred in predictions_dict.items():\n",
    "        ensemble_pred += weights[model] * pred\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "# Align predictions\n",
    "min_length = min(len(arima_forecast), len(prophet_test_pred), len(lstm_predictions), len(gru_predictions))\n",
    "\n",
    "predictions_dict = {\n",
    "    'ARIMA': arima_forecast.values[:min_length],\n",
    "    'Prophet': prophet_test_pred['yhat'].values[:min_length],\n",
    "    'LSTM': lstm_predictions.flatten()[:min_length],\n",
    "    'GRU': gru_predictions.flatten()[:min_length]\n",
    "}\n",
    "\n",
    "# Create simple average ensemble\n",
    "ensemble_pred = create_ensemble(predictions_dict)\n",
    "\n",
    "# Create weighted ensemble based on individual R² scores\n",
    "r2_scores = {\n",
    "    'ARIMA': max(0, arima_r2),\n",
    "    'Prophet': max(0, prophet_r2),\n",
    "    'LSTM': max(0, lstm_r2),\n",
    "    'GRU': max(0, gru_r2)\n",
    "}\n",
    "\n",
    "# Normalize weights\n",
    "total_r2 = sum(r2_scores.values())\n",
    "weights = {model: score/total_r2 for model, score in r2_scores.items()}\n",
    "\n",
    "weighted_ensemble_pred = create_ensemble(predictions_dict, weights)\n",
    "\n",
    "# Calculate ensemble metrics\n",
    "test_values_aligned = test_data['value'].values[:min_length]\n",
    "\n",
    "ensemble_mse = mean_squared_error(test_values_aligned, ensemble_pred)\n",
    "ensemble_mae = mean_absolute_error(test_values_aligned, ensemble_pred)\n",
    "ensemble_r2 = r2_score(test_values_aligned, ensemble_pred)\n",
    "\n",
    "weighted_mse = mean_squared_error(test_values_aligned, weighted_ensemble_pred)\n",
    "weighted_mae = mean_absolute_error(test_values_aligned, weighted_ensemble_pred)\n",
    "weighted_r2 = r2_score(test_values_aligned, weighted_ensemble_pred)\n",
    "\n",
    "print(\"Ensemble Weights:\")\n",
    "for model, weight in weights.items():\n",
    "    print(f\"{model}: {weight:.3f}\")\n",
    "\n",
    "print(f\"\\nSimple Ensemble Performance:\")\n",
    "print(f\"MSE: {ensemble_mse:.4f}\")\n",
    "print(f\"MAE: {ensemble_mae:.4f}\")\n",
    "print(f\"R²: {ensemble_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nWeighted Ensemble Performance:\")\n",
    "print(f\"MSE: {weighted_mse:.4f}\")\n",
    "print(f\"MAE: {weighted_mae:.4f}\")\n",
    "print(f\"R²: {weighted_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all predictions\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "\n",
    "# Plot individual models\n",
    "models = ['ARIMA', 'Prophet', 'LSTM', 'GRU', 'Simple Ensemble', 'Weighted Ensemble']\n",
    "predictions = [\n",
    "    arima_forecast.values[:min_length],\n",
    "    prophet_test_pred['yhat'].values[:min_length],\n",
    "    lstm_predictions.flatten()[:min_length],\n",
    "    gru_predictions.flatten()[:min_length],\n",
    "    ensemble_pred,\n",
    "    weighted_ensemble_pred\n",
    "]\n",
    "\n",
    "test_index_aligned = test_data.index[:min_length]\n",
    "\n",
    "for i, (ax, model, pred) in enumerate(zip(axes.flat, models, predictions)):\n",
    "    ax.plot(test_index_aligned, test_values_aligned, label='Actual', alpha=0.7)\n",
    "    ax.plot(test_index_aligned, pred, label=f'{model} Forecast', alpha=0.7)\n",
    "    ax.set_title(f'{model} Forecasting Results')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add metrics text\n",
    "    mse = mean_squared_error(test_values_aligned, pred)\n",
    "    mae = mean_absolute_error(test_values_aligned, pred)\n",
    "    r2 = r2_score(test_values_aligned, pred)\n",
    "    \n",
    "    metrics_text = f'MSE: {mse:.2f}\\nMAE: {mae:.2f}\\nR²: {r2:.3f}'\n",
    "    ax.text(0.02, 0.98, metrics_text, transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "            verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "performance_data = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'Prophet', 'LSTM', 'GRU', 'Simple Ensemble', 'Weighted Ensemble'],\n",
    "    'MSE': [arima_mse, prophet_mse, lstm_mse, gru_mse, ensemble_mse, weighted_mse],\n",
    "    'MAE': [arima_mae, prophet_mae, lstm_mae, gru_mae, ensemble_mae, weighted_mae],\n",
    "    'R²': [arima_r2, prophet_r2, lstm_r2, gru_r2, ensemble_r2, weighted_r2]\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# MSE comparison\n",
    "axes[0].bar(performance_data['Model'], performance_data['MSE'], color='skyblue')\n",
    "axes[0].set_title('Mean Squared Error Comparison')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(performance_data['Model'], performance_data['MAE'], color='lightgreen')\n",
    "axes[1].set_title('Mean Absolute Error Comparison')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "axes[2].bar(performance_data['Model'], performance_data['R²'], color='salmon')\n",
    "axes[2].set_title('R² Score Comparison')\n",
    "axes[2].set_ylabel('R²')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Performance Summary:\")\n",
    "print(performance_data.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forecast Future Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 48 hours using the best model\n",
    "best_model_idx = performance_data['R²'].idxmax()\n",
    "best_model = performance_data.loc[best_model_idx, 'Model']\n",
    "print(f\"Best model based on R² score: {best_model}\")\n",
    "\n",
    "# Generate future forecast\n",
    "forecast_horizon = 48\n",
    "\n",
    "if best_model == 'ARIMA':\n",
    "    # Refit ARIMA on full data\n",
    "    full_arima = ARIMA(df['value'], order=(2, 1, 2))\n",
    "    full_arima_fit = full_arima.fit()\n",
    "    future_forecast = full_arima_fit.forecast(steps=forecast_horizon)\n",
    "    \n",
    "elif best_model in ['LSTM', 'GRU']:\n",
    "    # Use the last sequence to predict future values\n",
    "    last_sequence = scaled_data[-look_back:]\n",
    "    future_forecast = []\n",
    "    \n",
    "    model = lstm_model if best_model == 'LSTM' else gru_model\n",
    "    \n",
    "    for _ in range(forecast_horizon):\n",
    "        next_pred = model.predict(last_sequence.reshape(1, look_back, 1), verbose=0)\n",
    "        future_forecast.append(next_pred[0, 0])\n",
    "        last_sequence = np.append(last_sequence[1:], next_pred)\n",
    "    \n",
    "    future_forecast = scaler.inverse_transform(np.array(future_forecast).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create future dates\n",
    "future_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(hours=1), \n",
    "                            periods=forecast_horizon, freq='H')\n",
    "\n",
    "# Visualize future forecast\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(df.index[-100:], df['value'].iloc[-100:], label='Historical Data', alpha=0.7)\n",
    "plt.plot(future_dates, future_forecast, label=f'{best_model} Forecast', \n",
    "         color='red', linestyle='--', alpha=0.8)\n",
    "plt.axvline(x=df.index[-1], color='green', linestyle=':', alpha=0.5, label='Forecast Start')\n",
    "plt.fill_between(future_dates, \n",
    "                 future_forecast - np.std(future_forecast),\n",
    "                 future_forecast + np.std(future_forecast),\n",
    "                 alpha=0.2, color='red', label='Uncertainty')\n",
    "plt.title(f'Future Forecast using {best_model}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 FORECASTING ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Dataset Information:\")\n",
    "print(f\"   Total data points: {len(df)}\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"   Time range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n🏆 Model Rankings (by R² score):\")\n",
    "ranked_models = performance_data.sort_values('R²', ascending=False)\n",
    "for i, row in ranked_models.iterrows():\n",
    "    print(f\"   {i+1}. {row['Model']}: R²={row['R²']:.4f}, MSE={row['MSE']:.4f}, MAE={row['MAE']:.4f}\")\n",
    "\n",
    "print(f\"\\n💡 Key Insights:\")\n",
    "if weighted_r2 > max(arima_r2, prophet_r2, lstm_r2, gru_r2):\n",
    "    print(\"   ✅ Ensemble methods outperform individual models\")\n",
    "else:\n",
    "    print(\"   ⚠️ Individual models perform better than ensemble\")\n",
    "\n",
    "if lstm_r2 > arima_r2 and gru_r2 > arima_r2:\n",
    "    print(\"   ✅ Deep learning models capture complex patterns better\")\n",
    "else:\n",
    "    print(\"   ✅ Traditional models are competitive for this dataset\")\n",
    "\n",
    "print(f\"\\n🔧 Recommendations:\")\n",
    "print(f\"   1. Use {best_model} for production forecasting (best R² score)\")\n",
    "print(f\"   2. Consider ensemble methods for improved robustness\")\n",
    "print(f\"   3. Retrain models periodically with new data\")\n",
    "print(f\"   4. Monitor forecast accuracy and adjust parameters\")\n",
    "print(f\"   5. Implement prediction intervals for uncertainty quantification\")\n",
    "\n",
    "print(f\"\\n📈 Future Improvements:\")\n",
    "print(f\"   • Hyperparameter optimization for all models\")\n",
    "print(f\"   • Feature engineering (external variables, holidays, etc.)\")\n",
    "print(f\"   • Advanced architectures (Transformer, N-BEATS)\")\n",
    "print(f\"   • Online learning for adaptive forecasting\")\n",
    "print(f\"   • Multi-step ahead forecasting evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export forecasts and results\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = \"forecast_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save performance metrics\n",
    "performance_data.to_csv(f\"{output_dir}/model_performance_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save forecasts\n",
    "forecasts_df = pd.DataFrame({\n",
    "    'timestamp': test_index_aligned,\n",
    "    'actual': test_values_aligned,\n",
    "    'arima': predictions[0],\n",
    "    'prophet': predictions[1],\n",
    "    'lstm': predictions[2],\n",
    "    'gru': predictions[3],\n",
    "    'ensemble': predictions[4],\n",
    "    'weighted_ensemble': predictions[5]\n",
    "})\n",
    "forecasts_df.to_csv(f\"{output_dir}/forecasts_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save future forecast\n",
    "future_df = pd.DataFrame({\n",
    "    'timestamp': future_dates,\n",
    "    'forecast': future_forecast\n",
    "})\n",
    "future_df.to_csv(f\"{output_dir}/future_forecast_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Results exported to {output_dir}/\")\n",
    "print(f\"📁 Files created:\")\n",
    "print(f\"   - model_performance_{timestamp}.csv\")\n",
    "print(f\"   - forecasts_{timestamp}.csv\")\n",
    "print(f\"   - future_forecast_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook demonstrated various forecasting techniques on TSIOT-generated data. Consider:\n",
    "\n",
    "1. **Advanced Models**: Try Transformer-based models, N-BEATS, or DeepAR\n",
    "2. **Feature Engineering**: Add external variables, calendar features, or domain-specific indicators\n",
    "3. **Hyperparameter Tuning**: Use grid search or Bayesian optimization\n",
    "4. **Production Deployment**: Integrate the best model into your application\n",
    "5. **Real-time Forecasting**: Implement streaming predictions with Kafka integration\n",
    "\n",
    "For more examples, visit the [TSIOT documentation](../../docs/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}