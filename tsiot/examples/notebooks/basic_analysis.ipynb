{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSIOT Basic Time Series Analysis\n",
    "\n",
    "This notebook demonstrates basic time series analysis using TSIOT-generated synthetic data. We'll cover:\n",
    "\n",
    "1. Connecting to TSIOT API\n",
    "2. Generating synthetic time series\n",
    "3. Basic statistical analysis\n",
    "4. Visualization\n",
    "5. Quality assessment\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have TSIOT running locally or have access to a TSIOT instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests pandas numpy matplotlib seaborn scipy statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to TSIOT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSIOT API configuration\n",
    "TSIOT_BASE_URL = \"http://localhost:8080\"\n",
    "API_KEY = \"your-api-key-here\"  # Replace with your actual API key\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "def check_tsiot_connection():\n",
    "    \"\"\"Check if TSIOT API is accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{TSIOT_BASE_URL}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Successfully connected to TSIOT API\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå TSIOT API returned status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Failed to connect to TSIOT API: {e}\")\n",
    "        return False\n",
    "\n",
    "check_tsiot_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_generators():\n",
    "    \"\"\"Get list of available generators from TSIOT.\"\"\"\n",
    "    response = requests.get(f\"{TSIOT_BASE_URL}/api/v1/generators\", headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to get generators: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "generators = get_available_generators()\n",
    "if generators:\n",
    "    print(\"Available generators:\")\n",
    "    for gen in generators.get('generators', []):\n",
    "        print(f\"  - {gen['name']}: {gen['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(generator_type, length, parameters=None):\n",
    "    \"\"\"Generate a time series using TSIOT API.\"\"\"\n",
    "    data = {\n",
    "        \"type\": generator_type,\n",
    "        \"length\": length,\n",
    "        \"parameters\": parameters or {}\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{TSIOT_BASE_URL}/api/v1/generate\", \n",
    "        json=data, \n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Generation failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Generate different types of time series\n",
    "print(\"Generating synthetic time series...\")\n",
    "\n",
    "# 1. ARIMA series\n",
    "arima_data = generate_time_series(\n",
    "    \"arima\", \n",
    "    500, \n",
    "    {\n",
    "        \"ar_params\": [0.5, -0.3],\n",
    "        \"ma_params\": [0.2],\n",
    "        \"mean\": 100,\n",
    "        \"variance\": 25\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. LSTM series\n",
    "lstm_data = generate_time_series(\n",
    "    \"lstm\",\n",
    "    500,\n",
    "    {\n",
    "        \"trend\": 0.1,\n",
    "        \"seasonality\": 24,\n",
    "        \"noise\": 0.05\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Random walk\n",
    "rw_data = generate_time_series(\n",
    "    \"random_walk\",\n",
    "    500,\n",
    "    {\n",
    "        \"drift\": 0.02,\n",
    "        \"volatility\": 1.0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Generated 3 time series successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "def to_dataframe(tsiot_response, name):\n",
    "    \"\"\"Convert TSIOT response to pandas DataFrame.\"\"\"\n",
    "    if not tsiot_response:\n",
    "        return None\n",
    "    \n",
    "    values = tsiot_response.get('values', [])\n",
    "    timestamps = tsiot_response.get('timestamps', [])\n",
    "    \n",
    "    if not timestamps:\n",
    "        # Create synthetic timestamps if not provided\n",
    "        timestamps = pd.date_range(start='2023-01-01', periods=len(values), freq='H')\n",
    "    else:\n",
    "        timestamps = pd.to_datetime(timestamps)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'value': values,\n",
    "        'series': name\n",
    "    }).set_index('timestamp')\n",
    "\n",
    "# Create DataFrames\n",
    "df_arima = to_dataframe(arima_data, 'ARIMA')\n",
    "df_lstm = to_dataframe(lstm_data, 'LSTM')\n",
    "df_rw = to_dataframe(rw_data, 'Random Walk')\n",
    "\n",
    "# Combine all series\n",
    "df_combined = pd.concat([df_arima, df_lstm, df_rw])\n",
    "\n",
    "print(f\"DataFrame shape: {df_combined.shape}\")\n",
    "print(f\"Date range: {df_combined.index.min()} to {df_combined.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics for each series\n",
    "def calculate_basic_stats(df):\n",
    "    \"\"\"Calculate basic statistics for time series.\"\"\"\n",
    "    stats_dict = {\n",
    "        'count': len(df),\n",
    "        'mean': df['value'].mean(),\n",
    "        'std': df['value'].std(),\n",
    "        'min': df['value'].min(),\n",
    "        'max': df['value'].max(),\n",
    "        'median': df['value'].median(),\n",
    "        'skewness': df['value'].skew(),\n",
    "        'kurtosis': df['value'].kurtosis()\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "# Calculate stats for each series\n",
    "series_stats = {}\n",
    "for series_name in ['ARIMA', 'LSTM', 'Random Walk']:\n",
    "    series_df = df_combined[df_combined['series'] == series_name]\n",
    "    series_stats[series_name] = calculate_basic_stats(series_df)\n",
    "\n",
    "# Create summary table\n",
    "stats_df = pd.DataFrame(series_stats).T\n",
    "print(\"üìä Basic Statistics Summary:\")\n",
    "print(stats_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests\n",
    "def perform_statistical_tests(series, name):\n",
    "    \"\"\"Perform various statistical tests on time series.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Augmented Dickey-Fuller test for stationarity\n",
    "    adf_result = adfuller(series.dropna())\n",
    "    results['adf_statistic'] = adf_result[0]\n",
    "    results['adf_pvalue'] = adf_result[1]\n",
    "    results['is_stationary'] = adf_result[1] < 0.05\n",
    "    \n",
    "    # Jarque-Bera test for normality\n",
    "    jb_stat, jb_pvalue = stats.jarque_bera(series.dropna())\n",
    "    results['jb_statistic'] = jb_stat\n",
    "    results['jb_pvalue'] = jb_pvalue\n",
    "    results['is_normal'] = jb_pvalue > 0.05\n",
    "    \n",
    "    # Ljung-Box test for autocorrelation\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    lb_result = acorr_ljungbox(series.dropna(), lags=10, return_df=True)\n",
    "    results['lb_pvalue_min'] = lb_result['lb_pvalue'].min()\n",
    "    results['has_autocorr'] = lb_result['lb_pvalue'].min() < 0.05\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform tests for each series\n",
    "test_results = {}\n",
    "for series_name in ['ARIMA', 'LSTM', 'Random Walk']:\n",
    "    series_df = df_combined[df_combined['series'] == series_name]\n",
    "    test_results[series_name] = perform_statistical_tests(series_df['value'], series_name)\n",
    "\n",
    "# Create results table\n",
    "tests_df = pd.DataFrame(test_results).T\n",
    "print(\"üß™ Statistical Tests Results:\")\n",
    "print(tests_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all time series\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "series_names = ['ARIMA', 'LSTM', 'Random Walk']\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, (series_name, color) in enumerate(zip(series_names, colors)):\n",
    "    series_df = df_combined[df_combined['series'] == series_name]\n",
    "    axes[i].plot(series_df.index, series_df['value'], color=color, alpha=0.8, linewidth=1)\n",
    "    axes[i].set_title(f'{series_name} Time Series', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics as text\n",
    "    stats_text = f\"Mean: {series_stats[series_name]['mean']:.2f}, Std: {series_stats[series_name]['std']:.2f}\"\n",
    "    axes[i].text(0.02, 0.95, stats_text, transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "axes[-1].set_xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for i, series_name in enumerate(series_names):\n",
    "    series_df = df_combined[df_combined['series'] == series_name]\n",
    "    values = series_df['value']\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, i].hist(values, bins=30, alpha=0.7, color=colors[i], edgecolor='black')\n",
    "    axes[0, i].set_title(f'{series_name} - Distribution')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(values, dist=\"norm\", plot=axes[1, i])\n",
    "    axes[1, i].set_title(f'{series_name} - Q-Q Plot')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation and Partial Autocorrelation plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "\n",
    "for i, series_name in enumerate(series_names):\n",
    "    series_df = df_combined[df_combined['series'] == series_name]\n",
    "    values = series_df['value'].dropna()\n",
    "    \n",
    "    # ACF\n",
    "    acf_values = acf(values, nlags=40)\n",
    "    axes[i, 0].plot(acf_values, marker='o', markersize=3)\n",
    "    axes[i, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[i, 0].axhline(y=1.96/np.sqrt(len(values)), color='red', linestyle='--', alpha=0.5)\n",
    "    axes[i, 0].axhline(y=-1.96/np.sqrt(len(values)), color='red', linestyle='--', alpha=0.5)\n",
    "    axes[i, 0].set_title(f'{series_name} - Autocorrelation')\n",
    "    axes[i, 0].set_ylabel('ACF')\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PACF\n",
    "    pacf_values = pacf(values, nlags=40)\n",
    "    axes[i, 1].plot(pacf_values, marker='o', markersize=3)\n",
    "    axes[i, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[i, 1].axhline(y=1.96/np.sqrt(len(values)), color='red', linestyle='--', alpha=0.5)\n",
    "    axes[i, 1].axhline(y=-1.96/np.sqrt(len(values)), color='red', linestyle='--', alpha=0.5)\n",
    "    axes[i, 1].set_title(f'{series_name} - Partial Autocorrelation')\n",
    "    axes[i, 1].set_ylabel('PACF')\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1, 0].set_xlabel('Lag')\n",
    "axes[-1, 1].set_xlabel('Lag')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition for LSTM series (which has seasonality)\n",
    "lstm_series = df_combined[df_combined['series'] == 'LSTM']['value']\n",
    "\n",
    "# Ensure we have enough data points for seasonal decomposition\n",
    "if len(lstm_series) >= 48:  # At least 2 cycles of 24-hour seasonality\n",
    "    decomposition = seasonal_decompose(lstm_series, model='additive', period=24)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    decomposition.observed.plot(ax=axes[0], title='Original Series')\n",
    "    decomposition.trend.plot(ax=axes[1], title='Trend Component')\n",
    "    decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component')\n",
    "    decomposition.resid.plot(ax=axes[3], title='Residual Component')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate component statistics\n",
    "    print(\"üìä Seasonal Decomposition Statistics:\")\n",
    "    print(f\"Trend variance: {decomposition.trend.var():.4f}\")\n",
    "    print(f\"Seasonal variance: {decomposition.seasonal.var():.4f}\")\n",
    "    print(f\"Residual variance: {decomposition.resid.var():.4f}\")\n",
    "    \n",
    "    # Calculate signal-to-noise ratio\n",
    "    signal_var = decomposition.trend.var() + decomposition.seasonal.var()\n",
    "    noise_var = decomposition.resid.var()\n",
    "    snr = signal_var / noise_var\n",
    "    print(f\"Signal-to-Noise Ratio: {snr:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data points for seasonal decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_time_series_quality(series, name):\n",
    "    \"\"\"Assess the quality of a time series.\"\"\"\n",
    "    quality_metrics = {}\n",
    "    \n",
    "    # Missing values\n",
    "    quality_metrics['missing_count'] = series.isna().sum()\n",
    "    quality_metrics['missing_percentage'] = (series.isna().sum() / len(series)) * 100\n",
    "    \n",
    "    # Outliers (using IQR method)\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "    quality_metrics['outlier_count'] = outliers\n",
    "    quality_metrics['outlier_percentage'] = (outliers / len(series)) * 100\n",
    "    \n",
    "    # Data consistency\n",
    "    quality_metrics['coefficient_of_variation'] = series.std() / series.mean()\n",
    "    \n",
    "    # Temporal consistency (assuming regular intervals)\n",
    "    if hasattr(series, 'index') and hasattr(series.index, 'to_series'):\n",
    "        time_diffs = series.index.to_series().diff().dropna()\n",
    "        quality_metrics['irregular_intervals'] = (time_diffs != time_diffs.mode()[0]).sum()\n",
    "    else:\n",
    "        quality_metrics['irregular_intervals'] = 0\n",
    "    \n",
    "    # Overall quality score (simple heuristic)\n",
    "    quality_score = 100\n",
    "    quality_score -= quality_metrics['missing_percentage'] * 2  # Penalize missing values\n",
    "    quality_score -= quality_metrics['outlier_percentage']      # Penalize outliers\n",
    "    quality_score = max(0, quality_score)  # Ensure non-negative\n",
    "    quality_metrics['overall_quality_score'] = quality_score\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Assess quality for each series\n",
    "quality_results = {}\n",
    "for series_name in series_names:\n",
    "    series_df = df_combined[df_combined['series'] == series_name]\n",
    "    quality_results[series_name] = assess_time_series_quality(series_df['value'], series_name)\n",
    "\n",
    "# Create quality summary table\n",
    "quality_df = pd.DataFrame(quality_results).T\n",
    "print(\"üéØ Quality Assessment Results:\")\n",
    "print(quality_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Quality scores\n",
    "quality_scores = [quality_results[name]['overall_quality_score'] for name in series_names]\n",
    "axes[0, 0].bar(series_names, quality_scores, color=colors)\n",
    "axes[0, 0].set_title('Overall Quality Scores')\n",
    "axes[0, 0].set_ylabel('Quality Score')\n",
    "axes[0, 0].set_ylim(0, 100)\n",
    "\n",
    "# Missing value percentages\n",
    "missing_pcts = [quality_results[name]['missing_percentage'] for name in series_names]\n",
    "axes[0, 1].bar(series_names, missing_pcts, color=colors)\n",
    "axes[0, 1].set_title('Missing Values (%)')\n",
    "axes[0, 1].set_ylabel('Missing %')\n",
    "\n",
    "# Outlier percentages\n",
    "outlier_pcts = [quality_results[name]['outlier_percentage'] for name in series_names]\n",
    "axes[1, 0].bar(series_names, outlier_pcts, color=colors)\n",
    "axes[1, 0].set_title('Outliers (%)')\n",
    "axes[1, 0].set_ylabel('Outlier %')\n",
    "\n",
    "# Coefficient of variation\n",
    "cv_values = [quality_results[name]['coefficient_of_variation'] for name in series_names]\n",
    "axes[1, 1].bar(series_names, cv_values, color=colors)\n",
    "axes[1, 1].set_title('Coefficient of Variation')\n",
    "axes[1, 1].set_ylabel('CV')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã TSIOT TIME SERIES ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÖ Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìä Number of Series Analyzed: {len(series_names)}\")\n",
    "print(f\"üìè Data Points per Series: {len(df_arima)}\")\n",
    "print(f\"‚è±Ô∏è Time Range: {df_combined.index.min().strftime('%Y-%m-%d')} to {df_combined.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"üìà SERIES CHARACTERISTICS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for series_name in series_names:\n",
    "    stats = series_stats[series_name]\n",
    "    tests = test_results[series_name]\n",
    "    quality = quality_results[series_name]\n",
    "    \n",
    "    print(f\"\\nüî∏ {series_name} Series:\")\n",
    "    print(f\"   Mean: {stats['mean']:.2f} ¬± {stats['std']:.2f}\")\n",
    "    print(f\"   Range: [{stats['min']:.2f}, {stats['max']:.2f}]\")\n",
    "    print(f\"   Stationarity: {'‚úÖ Stationary' if tests['is_stationary'] else '‚ùå Non-stationary'}\")\n",
    "    print(f\"   Normality: {'‚úÖ Normal' if tests['is_normal'] else '‚ùå Non-normal'}\")\n",
    "    print(f\"   Autocorrelation: {'‚úÖ Present' if tests['has_autocorr'] else '‚ùå Absent'}\")\n",
    "    print(f\"   Quality Score: {quality['overall_quality_score']:.1f}/100\")\n",
    "    \n",
    "    if quality['outlier_count'] > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Outliers detected: {quality['outlier_count']} ({quality['outlier_percentage']:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"üéØ RECOMMENDATIONS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for series_name in series_names:\n",
    "    tests = test_results[series_name]\n",
    "    quality = quality_results[series_name]\n",
    "    \n",
    "    print(f\"\\nüî∏ {series_name} Series:\")\n",
    "    \n",
    "    if not tests['is_stationary']:\n",
    "        print(\"   üìå Apply differencing or detrending for stationarity\")\n",
    "    \n",
    "    if not tests['is_normal']:\n",
    "        print(\"   üìå Consider transformation (log, box-cox) for normality\")\n",
    "    \n",
    "    if tests['has_autocorr']:\n",
    "        print(\"   üìå Suitable for ARIMA/LSTM modeling\")\n",
    "    \n",
    "    if quality['outlier_percentage'] > 5:\n",
    "        print(\"   üìå Investigate and possibly remove outliers\")\n",
    "    \n",
    "    if quality['overall_quality_score'] > 90:\n",
    "        print(\"   ‚úÖ High quality - ready for analysis\")\n",
    "    elif quality['overall_quality_score'] > 70:\n",
    "        print(\"   ‚ö†Ô∏è Good quality - minor preprocessing needed\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Poor quality - significant preprocessing required\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Export data\n",
    "df_combined.to_csv(f\"{output_dir}/time_series_data_{timestamp}.csv\")\n",
    "stats_df.to_csv(f\"{output_dir}/basic_statistics_{timestamp}.csv\")\n",
    "tests_df.to_csv(f\"{output_dir}/statistical_tests_{timestamp}.csv\")\n",
    "quality_df.to_csv(f\"{output_dir}/quality_assessment_{timestamp}.csv\")\n",
    "\n",
    "print(f\"‚úÖ Results exported to {output_dir}/ directory\")\n",
    "print(f\"üìÅ Files created:\")\n",
    "print(f\"   - time_series_data_{timestamp}.csv\")\n",
    "print(f\"   - basic_statistics_{timestamp}.csv\")\n",
    "print(f\"   - statistical_tests_{timestamp}.csv\")\n",
    "print(f\"   - quality_assessment_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provided a comprehensive basic analysis of TSIOT-generated synthetic time series. Here are some suggested next steps:\n",
    "\n",
    "1. **Advanced Analysis**: Explore advanced notebooks for specific use cases:\n",
    "   - `anomaly_detection.ipynb` - Detect anomalies in time series\n",
    "   - `forecasting_models.ipynb` - Build forecasting models\n",
    "   - `feature_engineering.ipynb` - Extract meaningful features\n",
    "\n",
    "2. **Model Development**: Use the synthetic data to train and validate time series models\n",
    "\n",
    "3. **Quality Improvement**: If quality scores are low, consider:\n",
    "   - Adjusting generator parameters\n",
    "   - Applying preprocessing techniques\n",
    "   - Using different generator types\n",
    "\n",
    "4. **Integration**: Integrate TSIOT into your data pipeline for continuous synthetic data generation\n",
    "\n",
    "5. **Validation**: Compare synthetic data characteristics with real-world data to ensure realism\n",
    "\n",
    "For more information, visit the [TSIOT documentation](../../docs/) or explore other example notebooks in this directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}