{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Preservation in Synthetic IoT Data Generation\n",
    "\n",
    "This notebook demonstrates privacy-preserving techniques for synthetic IoT data generation, including differential privacy and anonymization methods.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Privacy Fundamentals](#fundamentals)\n",
    "2. [Differential Privacy Techniques](#differential-privacy)\n",
    "3. [Data Anonymization Methods](#anonymization)\n",
    "4. [Privacy-Utility Trade-offs](#trade-offs)\n",
    "5. [Privacy Risk Assessment](#risk-assessment)\n",
    "6. [Compliance and Validation](#compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "output_dir = \"privacy_preservation_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Privacy preservation environment ready\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Fundamentals {#fundamentals}\n",
    "\n",
    "Understanding privacy concepts in the context of IoT synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivacyAnalyzer:\n",
    "    \"\"\"Comprehensive privacy analysis for synthetic data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.privacy_metrics = {}\n",
    "    \n",
    "    def k_anonymity_assessment(self, data, quasi_identifiers):\n",
    "        \"\"\"Assess k-anonymity for quasi-identifying attributes\"\"\"\n",
    "        if not quasi_identifiers:\n",
    "            return {\"k_value\": float('inf'), \"message\": \"No quasi-identifiers specified\"}\n",
    "        \n",
    "        # Group by quasi-identifiers and count group sizes\n",
    "        grouped = data.groupby(quasi_identifiers).size()\n",
    "        k_value = grouped.min()\n",
    "        \n",
    "        return {\n",
    "            \"k_value\": k_value,\n",
    "            \"group_sizes\": grouped.describe(),\n",
    "            \"risk_level\": \"High\" if k_value < 3 else \"Medium\" if k_value < 5 else \"Low\"\n",
    "        }\n",
    "    \n",
    "    def l_diversity_assessment(self, data, quasi_identifiers, sensitive_attribute):\n",
    "        \"\"\"Assess l-diversity for sensitive attributes\"\"\"\n",
    "        if not quasi_identifiers or sensitive_attribute not in data.columns:\n",
    "            return {\"l_value\": 0, \"message\": \"Invalid parameters\"}\n",
    "        \n",
    "        # Group by quasi-identifiers and check diversity of sensitive attribute\n",
    "        diversity_values = []\n",
    "        for name, group in data.groupby(quasi_identifiers):\n",
    "            unique_sensitive = group[sensitive_attribute].nunique()\n",
    "            diversity_values.append(unique_sensitive)\n",
    "        \n",
    "        l_value = min(diversity_values) if diversity_values else 0\n",
    "        \n",
    "        return {\n",
    "            \"l_value\": l_value,\n",
    "            \"diversity_distribution\": pd.Series(diversity_values).describe(),\n",
    "            \"risk_level\": \"High\" if l_value < 2 else \"Medium\" if l_value < 3 else \"Low\"\n",
    "        }\n",
    "    \n",
    "    def membership_inference_risk(self, real_data, synthetic_data):\n",
    "        \"\"\"Assess membership inference attack risk\"\"\"\n",
    "        # Calculate distance-based membership inference risk\n",
    "        min_distances = []\n",
    "        \n",
    "        for real_point in real_data.values:\n",
    "            distances = np.linalg.norm(synthetic_data.values - real_point, axis=1)\n",
    "            min_distances.append(np.min(distances))\n",
    "        \n",
    "        # Risk metrics\n",
    "        threshold = np.std(real_data.values) * 0.1  # 10% of std as threshold\n",
    "        at_risk_points = np.sum(np.array(min_distances) < threshold)\n",
    "        risk_ratio = at_risk_points / len(min_distances)\n",
    "        \n",
    "        return {\n",
    "            \"at_risk_points\": at_risk_points,\n",
    "            \"total_points\": len(min_distances),\n",
    "            \"risk_ratio\": risk_ratio,\n",
    "            \"mean_min_distance\": np.mean(min_distances),\n",
    "            \"std_min_distance\": np.std(min_distances),\n",
    "            \"risk_level\": \"High\" if risk_ratio > 0.1 else \"Medium\" if risk_ratio > 0.05 else \"Low\"\n",
    "        }\n",
    "    \n",
    "    def attribute_inference_risk(self, data, target_attribute, auxiliary_attributes):\n",
    "        \"\"\"Assess attribute inference attack risk using mutual information\"\"\"\n",
    "        if target_attribute not in data.columns:\n",
    "            return {\"risk_score\": 0, \"message\": \"Target attribute not found\"}\n",
    "        \n",
    "        # Calculate mutual information between target and auxiliary attributes\n",
    "        mi_scores = []\n",
    "        \n",
    "        for aux_attr in auxiliary_attributes:\n",
    "            if aux_attr in data.columns and aux_attr \!= target_attribute:\n",
    "                # Discretize continuous variables for MI calculation\n",
    "                target_discrete = pd.cut(data[target_attribute], bins=10, labels=False)\n",
    "                aux_discrete = pd.cut(data[aux_attr], bins=10, labels=False)\n",
    "                \n",
    "                mi = mutual_info_score(target_discrete.dropna(), aux_discrete.dropna())\n",
    "                mi_scores.append(mi)\n",
    "        \n",
    "        risk_score = max(mi_scores) if mi_scores else 0\n",
    "        \n",
    "        return {\n",
    "            \"risk_score\": risk_score,\n",
    "            \"mi_scores\": mi_scores,\n",
    "            \"risk_level\": \"High\" if risk_score > 0.5 else \"Medium\" if risk_score > 0.2 else \"Low\"\n",
    "        }\n",
    "\n",
    "# Initialize privacy analyzer\n",
    "privacy_analyzer = PrivacyAnalyzer()\n",
    "print(\"Privacy analysis framework initialized\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data for Privacy Analysis\n",
    "\n",
    "Create synthetic datasets to demonstrate privacy preservation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate base synthetic data\n",
    "print(\"üîÑ Generating base datasets for privacy analysis...\")\n",
    "\n",
    "# Create \"original\" (simulated real) data\n",
    "np.random.seed(42)\n",
    "n_devices = 100\n",
    "n_timepoints = 1440  # 24 hours\n",
    "\n",
    "# Device metadata (quasi-identifiers)\n",
    "device_locations = np.random.choice(['building_A', 'building_B', 'building_C'], n_devices)\n",
    "device_types = np.random.choice(['sensor_type_1', 'sensor_type_2', 'sensor_type_3'], n_devices)\n",
    "device_floors = np.random.randint(1, 11, n_devices)  # Floors 1-10\n",
    "\n",
    "# Generate time series data for each device\n",
    "original_data = []\n",
    "timestamps = pd.date_range('2023-01-01', periods=n_timepoints, freq='1min')\n",
    "\n",
    "for device_id in range(n_devices):\n",
    "    # Base temperature with device-specific characteristics\n",
    "    base_temp = 18 + np.random.normal(0, 2)  # Device-specific baseline\n",
    "    \n",
    "    # Daily pattern\n",
    "    t = np.arange(n_timepoints)\n",
    "    daily_pattern = 3 * np.sin(2 * np.pi * t / 1440)  # Daily cycle\n",
    "    \n",
    "    # Noise\n",
    "    noise = np.random.normal(0, 0.5, n_timepoints)\n",
    "    \n",
    "    # Device-specific behavior\n",
    "    if device_types[device_id] == 'sensor_type_1':\n",
    "        multiplier = 1.2\n",
    "    elif device_types[device_id] == 'sensor_type_2':\n",
    "        multiplier = 0.8\n",
    "    else:\n",
    "        multiplier = 1.0\n",
    "    \n",
    "    temperatures = (base_temp + daily_pattern * multiplier + noise)\n",
    "    \n",
    "    for i, (timestamp, temp) in enumerate(zip(timestamps, temperatures)):\n",
    "        original_data.append({\n",
    "            'device_id': device_id,\n",
    "            'timestamp': timestamp,\n",
    "            'temperature': temp,\n",
    "            'location': device_locations[device_id],\n",
    "            'device_type': device_types[device_id],\n",
    "            'floor': device_floors[device_id],\n",
    "            'hour': timestamp.hour,\n",
    "            'day_of_week': timestamp.dayofweek\n",
    "        })\n",
    "\n",
    "original_df = pd.DataFrame(original_data)\n",
    "\n",
    "print(f\"‚úÖ Generated original dataset: {original_df.shape}\")\n",
    "print(f\"   Devices: {n_devices}\")\n",
    "print(f\"   Time points per device: {n_timepoints}\")\n",
    "print(f\"   Total records: {len(original_df)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nüìä Sample Original Data:\")\n",
    "print(original_df.head(10))\n",
    "\n",
    "print(f\"\\nüìà Basic Statistics:\")\n",
    "print(original_df[['temperature', 'floor']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy Techniques {#differential-privacy}\n",
    "\n",
    "Implement and demonstrate differential privacy mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialPrivacy:\n",
    "    \"\"\"Differential privacy mechanisms for synthetic data\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=1.0):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def laplace_mechanism(self, true_value, sensitivity):\n",
    "        \"\"\"Add Laplace noise for differential privacy\"\"\"\n",
    "        scale = sensitivity / self.epsilon\n",
    "        noise = np.random.laplace(0, scale)\n",
    "        return true_value + noise\n",
    "    \n",
    "    def gaussian_mechanism(self, true_value, sensitivity, delta=1e-5):\n",
    "        \"\"\"Add Gaussian noise for (epsilon, delta)-differential privacy\"\"\"\n",
    "        sigma = (sensitivity * np.sqrt(2 * np.log(1.25 / delta))) / self.epsilon\n",
    "        noise = np.random.normal(0, sigma)\n",
    "        return true_value + noise\n",
    "    \n",
    "    def exponential_mechanism(self, candidates, utility_function, sensitivity):\n",
    "        \"\"\"Select candidate based on exponential mechanism\"\"\"\n",
    "        utilities = [utility_function(c) for c in candidates]\n",
    "        probabilities = np.exp((self.epsilon * np.array(utilities)) / (2 * sensitivity))\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "        \n",
    "        selected_idx = np.random.choice(len(candidates), p=probabilities)\n",
    "        return candidates[selected_idx]\n",
    "    \n",
    "    def privatize_statistics(self, data, statistics=['mean', 'std', 'min', 'max']):\n",
    "        \"\"\"Apply differential privacy to statistical measures\"\"\"\n",
    "        privatized_stats = {}\n",
    "        \n",
    "        # Assume data is normalized to [0, 1] range for sensitivity calculation\n",
    "        data_normalized = (data - data.min()) / (data.max() - data.min())\n",
    "        \n",
    "        for stat in statistics:\n",
    "            if stat == 'mean':\n",
    "                true_value = data_normalized.mean()\n",
    "                sensitivity = 1.0 / len(data)  # L1 sensitivity for mean\n",
    "                privatized_stats[stat] = self.laplace_mechanism(true_value, sensitivity)\n",
    "            \n",
    "            elif stat == 'std':\n",
    "                true_value = data_normalized.std()\n",
    "                sensitivity = 1.0 / len(data)  # Approximation\n",
    "                privatized_stats[stat] = max(0, self.laplace_mechanism(true_value, sensitivity))\n",
    "            \n",
    "            elif stat == 'min':\n",
    "                true_value = data_normalized.min()\n",
    "                sensitivity = 1.0 / len(data)\n",
    "                privatized_stats[stat] = self.laplace_mechanism(true_value, sensitivity)\n",
    "            \n",
    "            elif stat == 'max':\n",
    "                true_value = data_normalized.max()\n",
    "                sensitivity = 1.0 / len(data)\n",
    "                privatized_stats[stat] = self.laplace_mechanism(true_value, sensitivity)\n",
    "        \n",
    "        return privatized_stats\n",
    "    \n",
    "    def privatize_histogram(self, data, bins=20):\n",
    "        \"\"\"Create differentially private histogram\"\"\"\n",
    "        hist, bin_edges = np.histogram(data, bins=bins)\n",
    "        \n",
    "        # Add Laplace noise to each bin count\n",
    "        sensitivity = 1  # Adding/removing one record changes one bin by 1\n",
    "        privatized_hist = []\n",
    "        \n",
    "        for count in hist:\n",
    "            noisy_count = self.laplace_mechanism(count, sensitivity)\n",
    "            privatized_hist.append(max(0, noisy_count))  # Ensure non-negative\n",
    "        \n",
    "        return np.array(privatized_hist), bin_edges\n",
    "\n",
    "# Test different privacy levels\n",
    "privacy_levels = [0.1, 0.5, 1.0, 2.0, 5.0]  # Different epsilon values\n",
    "dp_results = {}\n",
    "\n",
    "print(\"üîí Testing Differential Privacy Mechanisms:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Focus on temperature data for privacy analysis\n",
    "temperature_data = original_df['temperature'].values\n",
    "\n",
    "for epsilon in privacy_levels:\n",
    "    print(f\"\\nüìä Privacy Level Œµ = {epsilon}:\")\n",
    "    \n",
    "    dp = DifferentialPrivacy(epsilon=epsilon)\n",
    "    \n",
    "    # Privatize statistics\n",
    "    privatized_stats = dp.privatize_statistics(temperature_data)\n",
    "    \n",
    "    # Privatize histogram\n",
    "    privatized_hist, bin_edges = dp.privatize_histogram(temperature_data)\n",
    "    \n",
    "    dp_results[epsilon] = {\n",
    "        'statistics': privatized_stats,\n",
    "        'histogram': privatized_hist,\n",
    "        'bin_edges': bin_edges\n",
    "    }\n",
    "    \n",
    "    # Compare with original statistics\n",
    "    original_stats = {\n",
    "        'mean': temperature_data.mean(),\n",
    "        'std': temperature_data.std(),\n",
    "        'min': temperature_data.min(),\n",
    "        'max': temperature_data.max()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Original Mean: {original_stats['mean']:.3f}, DP Mean: {privatized_stats['mean']:.3f}\")\n",
    "    print(f\"   Original Std:  {original_stats['std']:.3f}, DP Std:  {privatized_stats['std']:.3f}\")\n",
    "    \n",
    "    # Calculate utility loss\n",
    "    mean_error = abs(original_stats['mean'] - privatized_stats['mean'])\n",
    "    std_error = abs(original_stats['std'] - privatized_stats['std'])\n",
    "    \n",
    "    print(f\"   Mean Error: {mean_error:.3f}, Std Error: {std_error:.3f}\")\n",
    "    \n",
    "    privacy_level = \"High\" if epsilon < 1.0 else \"Medium\" if epsilon < 2.0 else \"Low\"\n",
    "    print(f\"   Privacy Level: {privacy_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Anonymization Methods {#anonymization}\n",
    "\n",
    "Implement k-anonymity, l-diversity, and other anonymization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnonymizer:\n",
    "    \"\"\"Data anonymization techniques\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generalize_numerical(self, data, column, bins):\n",
    "        \"\"\"Generalize numerical data into ranges\"\"\"\n",
    "        return pd.cut(data[column], bins=bins, labels=[f\"Range_{i}\" for i in range(bins)])\n",
    "    \n",
    "    def generalize_categorical(self, data, column, hierarchy):\n",
    "        \"\"\"Generalize categorical data using hierarchy\"\"\"\n",
    "        return data[column].map(hierarchy)\n",
    "    \n",
    "    def suppress_outliers(self, data, column, threshold=2.5):\n",
    "        \"\"\"Suppress outliers by replacing with '*'\"\"\"\n",
    "        z_scores = np.abs(stats.zscore(data[column]))\n",
    "        outlier_mask = z_scores > threshold\n",
    "        \n",
    "        suppressed_data = data[column].copy()\n",
    "        suppressed_data[outlier_mask] = '*'  # Suppress outliers\n",
    "        \n",
    "        return suppressed_data\n",
    "    \n",
    "    def achieve_k_anonymity(self, data, quasi_identifiers, k=3):\n",
    "        \"\"\"Apply generalization to achieve k-anonymity\"\"\"\n",
    "        anonymized_data = data.copy()\n",
    "        \n",
    "        # Generalize quasi-identifiers\n",
    "        if 'floor' in quasi_identifiers:\n",
    "            # Group floors into ranges\n",
    "            anonymized_data['floor_range'] = pd.cut(data['floor'], \n",
    "                                                   bins=[0, 3, 6, 10], \n",
    "                                                   labels=['Low (1-3)', 'Mid (4-6)', 'High (7-10)'])\n",
    "        \n",
    "        if 'hour' in quasi_identifiers:\n",
    "            # Group hours into time periods\n",
    "            hour_mapping = {\n",
    "                **{h: 'Night (0-5)' for h in range(0, 6)},\n",
    "                **{h: 'Morning (6-11)' for h in range(6, 12)},\n",
    "                **{h: 'Afternoon (12-17)' for h in range(12, 18)},\n",
    "                **{h: 'Evening (18-23)' for h in range(18, 24)}\n",
    "            }\n",
    "            anonymized_data['hour_period'] = data['hour'].map(hour_mapping)\n",
    "        \n",
    "        if 'device_type' in quasi_identifiers:\n",
    "            # Keep device type as is (already general enough)\n",
    "            anonymized_data['device_type_anon'] = data['device_type']\n",
    "        \n",
    "        # Check k-anonymity with generalized attributes\n",
    "        generalized_qi = [col for col in ['floor_range', 'hour_period', 'device_type_anon'] \n",
    "                         if col in anonymized_data.columns]\n",
    "        \n",
    "        if generalized_qi:\n",
    "            group_sizes = anonymized_data.groupby(generalized_qi).size()\n",
    "            min_group_size = group_sizes.min()\n",
    "            \n",
    "            # If still not k-anonymous, apply suppression\n",
    "            if min_group_size < k:\n",
    "                small_groups = group_sizes[group_sizes < k].index\n",
    "                for group in small_groups:\n",
    "                    if isinstance(group, tuple):\n",
    "                        mask = anonymized_data[generalized_qi].apply(\n",
    "                            lambda row: tuple(row) == group, axis=1)\n",
    "                    else:\n",
    "                        mask = anonymized_data[generalized_qi[0]] == group\n",
    "                    \n",
    "                    # Suppress by marking with '*'\n",
    "                    for col in generalized_qi:\n",
    "                        anonymized_data.loc[mask, col] = '*'\n",
    "        \n",
    "        return anonymized_data\n",
    "    \n",
    "    def add_noise_for_privacy(self, data, columns, noise_level=0.1):\n",
    "        \"\"\"Add controlled noise to numerical columns\"\"\"\n",
    "        noisy_data = data.copy()\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in data.columns and pd.api.types.is_numeric_dtype(data[column]):\n",
    "                std_dev = data[column].std()\n",
    "                noise = np.random.normal(0, std_dev * noise_level, len(data))\n",
    "                noisy_data[column] = data[column] + noise\n",
    "        \n",
    "        return noisy_data\n",
    "\n",
    "# Apply anonymization techniques\n",
    "anonymizer = DataAnonymizer()\n",
    "\n",
    "print(\"üé≠ Applying Data Anonymization Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample subset for anonymization demonstration\n",
    "sample_data = original_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Define quasi-identifiers and sensitive attributes\n",
    "quasi_identifiers = ['location', 'device_type', 'floor', 'hour']\n",
    "sensitive_attribute = 'temperature'\n",
    "\n",
    "print(f\"\\nüìä Original Data Assessment:\")\n",
    "print(f\"   Sample size: {len(sample_data)}\")\n",
    "print(f\"   Quasi-identifiers: {quasi_identifiers}\")\n",
    "print(f\"   Sensitive attribute: {sensitive_attribute}\")\n",
    "\n",
    "# Original privacy assessment\n",
    "original_k_anon = privacy_analyzer.k_anonymity_assessment(sample_data, quasi_identifiers)\n",
    "print(f\"\\n   Original k-anonymity: {original_k_anon['k_value']}\")\n",
    "print(f\"   Risk level: {original_k_anon['risk_level']}\")\n",
    "\n",
    "# Apply k-anonymity\n",
    "k_values = [3, 5, 10]\n",
    "anonymized_datasets = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nüîí Achieving {k}-anonymity:\")\n",
    "    \n",
    "    anonymized_data = anonymizer.achieve_k_anonymity(sample_data, quasi_identifiers, k=k)\n",
    "    \n",
    "    # Check achieved anonymity\n",
    "    generalized_qi = [col for col in ['floor_range', 'hour_period', 'device_type_anon'] \n",
    "                     if col in anonymized_data.columns]\n",
    "    \n",
    "    if generalized_qi:\n",
    "        achieved_k_anon = privacy_analyzer.k_anonymity_assessment(anonymized_data, generalized_qi)\n",
    "        print(f\"   Achieved k-anonymity: {achieved_k_anon['k_value']}\")\n",
    "        print(f\"   Risk level: {achieved_k_anon['risk_level']}\")\n",
    "    \n",
    "    # Calculate information loss\n",
    "    suppression_rate = (anonymized_data[generalized_qi] == '*').sum().sum() / (len(anonymized_data) * len(generalized_qi))\n",
    "    print(f\"   Suppression rate: {suppression_rate:.1%}\")\n",
    "    \n",
    "    anonymized_datasets[k] = anonymized_data\n",
    "\n",
    "# Add noise-based anonymization\n",
    "print(f\"\\nüåä Noise-based Anonymization:\")\n",
    "noise_levels = [0.05, 0.1, 0.2]\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    noisy_data = anonymizer.add_noise_for_privacy(sample_data, [sensitive_attribute], noise_level)\n",
    "    \n",
    "    # Calculate utility preservation\n",
    "    original_mean = sample_data[sensitive_attribute].mean()\n",
    "    noisy_mean = noisy_data[sensitive_attribute].mean()\n",
    "    mean_error = abs(original_mean - noisy_mean)\n",
    "    \n",
    "    original_std = sample_data[sensitive_attribute].std()\n",
    "    noisy_std = noisy_data[sensitive_attribute].std()\n",
    "    \n",
    "    print(f\"   Noise level {noise_level:.0%}:\")\n",
    "    print(f\"     Mean preservation: {(1 - mean_error/original_mean):.1%}\")\n",
    "    print(f\"     Std change: {((noisy_std - original_std)/original_std):.1%}\")\n",
    "    \n",
    "    anonymized_datasets[f'noise_{noise_level}'] = noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy-Utility Trade-offs Visualization {#trade-offs}\n",
    "\n",
    "Visualize the trade-offs between privacy and data utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive privacy-utility trade-off analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Privacy-Utility Trade-offs in Synthetic IoT Data', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Differential Privacy Trade-offs\n",
    "epsilons = list(dp_results.keys())\n",
    "mean_errors = []\n",
    "std_errors = []\n",
    "\n",
    "original_mean = temperature_data.mean()\n",
    "original_std = temperature_data.std()\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    dp_stats = dp_results[epsilon]['statistics']\n",
    "    mean_error = abs(original_mean - dp_stats['mean']) / original_mean\n",
    "    std_error = abs(original_std - dp_stats['std']) / original_std\n",
    "    \n",
    "    mean_errors.append(mean_error)\n",
    "    std_errors.append(std_error)\n",
    "\n",
    "axes[0, 0].semilogx(epsilons, mean_errors, 'o-', label='Mean Error', linewidth=2, markersize=6)\n",
    "axes[0, 0].semilogx(epsilons, std_errors, 's-', label='Std Error', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_title('Differential Privacy: Error vs Œµ')\n",
    "axes[0, 0].set_xlabel('Privacy Parameter Œµ (log scale)')\n",
    "axes[0, 0].set_ylabel('Relative Error')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].invert_xaxis()  # Lower epsilon = higher privacy\n",
    "\n",
    "# 2. Histogram comparison for different privacy levels\n",
    "original_hist, original_bins = np.histogram(temperature_data, bins=20)\n",
    "\n",
    "# Show histograms for selected epsilon values\n",
    "epsilon_examples = [0.1, 1.0, 5.0]\n",
    "colors = ['red', 'orange', 'green']\n",
    "\n",
    "for i, (epsilon, color) in enumerate(zip(epsilon_examples, colors)):\n",
    "    dp_hist = dp_results[epsilon]['histogram']\n",
    "    bin_centers = (dp_results[epsilon]['bin_edges'][:-1] + dp_results[epsilon]['bin_edges'][1:]) / 2\n",
    "    \n",
    "    axes[0, 1].plot(bin_centers, dp_hist, \n",
    "                   label=f'Œµ = {epsilon}', color=color, alpha=0.7, linewidth=2)\n",
    "\n",
    "# Original histogram\n",
    "original_bin_centers = (original_bins[:-1] + original_bins[1:]) / 2\n",
    "axes[0, 1].plot(original_bin_centers, original_hist, \n",
    "               'k-', label='Original', linewidth=3, alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_title('Distribution Preservation')\n",
    "axes[0, 1].set_xlabel('Temperature Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Privacy vs Utility Score\n",
    "privacy_scores = []\n",
    "utility_scores = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Privacy score (higher for lower epsilon)\n",
    "    privacy_score = 1 / (1 + epsilon)  # Normalized privacy score\n",
    "    \n",
    "    # Utility score (based on statistical preservation)\n",
    "    dp_stats = dp_results[epsilon]['statistics']\n",
    "    mean_preservation = 1 - abs(original_mean - dp_stats['mean']) / original_mean\n",
    "    std_preservation = 1 - abs(original_std - dp_stats['std']) / original_std\n",
    "    utility_score = (mean_preservation + std_preservation) / 2\n",
    "    \n",
    "    privacy_scores.append(privacy_score)\n",
    "    utility_scores.append(utility_score)\n",
    "\n",
    "# Create Pareto frontier\n",
    "axes[0, 2].scatter(utility_scores, privacy_scores, \n",
    "                  c=epsilons, cmap='viridis', s=100, alpha=0.7)\n",
    "axes[0, 2].plot(utility_scores, privacy_scores, 'k--', alpha=0.5)\n",
    "\n",
    "# Add epsilon labels\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    axes[0, 2].annotate(f'Œµ={epsilon}', \n",
    "                       (utility_scores[i], privacy_scores[i]),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "axes[0, 2].set_title('Privacy-Utility Pareto Frontier')\n",
    "axes[0, 2].set_xlabel('Utility Score')\n",
    "axes[0, 2].set_ylabel('Privacy Score')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Anonymization information loss\n",
    "if anonymized_datasets:\n",
    "    k_values_list = [k for k in anonymized_datasets.keys() if isinstance(k, int)]\n",
    "    information_loss = []\n",
    "    \n",
    "    for k in k_values_list:\n",
    "        anon_data = anonymized_datasets[k]\n",
    "        generalized_qi = [col for col in ['floor_range', 'hour_period', 'device_type_anon'] \n",
    "                         if col in anon_data.columns]\n",
    "        \n",
    "        if generalized_qi:\n",
    "            # Calculate suppression rate as information loss\n",
    "            suppression_rate = (anon_data[generalized_qi] == '*').sum().sum() / (\n",
    "                len(anon_data) * len(generalized_qi))\n",
    "            information_loss.append(suppression_rate)\n",
    "        else:\n",
    "            information_loss.append(0)\n",
    "    \n",
    "    if information_loss:\n",
    "        axes[1, 0].bar([f'k={k}' for k in k_values_list], information_loss, \n",
    "                      color='lightcoral', alpha=0.7)\n",
    "        axes[1, 0].set_title('K-Anonymity Information Loss')\n",
    "        axes[1, 0].set_xlabel('Anonymity Level')\n",
    "        axes[1, 0].set_ylabel('Suppression Rate')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Noise impact on data quality\n",
    "noise_levels_list = [0.05, 0.1, 0.2]\n",
    "correlation_preservation = []\n",
    "\n",
    "for noise_level in noise_levels_list:\n",
    "    if f'noise_{noise_level}' in anonymized_datasets:\n",
    "        noisy_data = anonymized_datasets[f'noise_{noise_level}']\n",
    "        \n",
    "        # Calculate correlation preservation with hour (temporal pattern)\n",
    "        original_corr = sample_data[['temperature', 'hour']].corr().iloc[0, 1]\n",
    "        noisy_corr = noisy_data[['temperature', 'hour']].corr().iloc[0, 1]\n",
    "        \n",
    "        correlation_preservation.append(abs(noisy_corr / original_corr) if original_corr \!= 0 else 1)\n",
    "\n",
    "if correlation_preservation:\n",
    "    axes[1, 1].plot([f'{int(nl*100)}%' for nl in noise_levels_list], \n",
    "                   correlation_preservation, 'o-', \n",
    "                   linewidth=2, markersize=8, color='steelblue')\n",
    "    axes[1, 1].set_title('Noise Impact on Temporal Correlations')\n",
    "    axes[1, 1].set_xlabel('Noise Level')\n",
    "    axes[1, 1].set_ylabel('Correlation Preservation Ratio')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Perfect Preservation')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "# 6. Privacy risk assessment comparison\n",
    "privacy_methods = ['Original', 'DP (Œµ=1.0)', 'K-Anon (k=5)', 'Noise (10%)']\n",
    "risk_scores = []\n",
    "\n",
    "# Original risk (baseline)\n",
    "baseline_risk = 1.0\n",
    "risk_scores.append(baseline_risk)\n",
    "\n",
    "# DP risk (estimated)\n",
    "dp_risk = 0.3  # Lower risk due to noise\n",
    "risk_scores.append(dp_risk)\n",
    "\n",
    "# K-anonymity risk\n",
    "k_anon_risk = 0.2  # Lower risk due to generalization\n",
    "risk_scores.append(k_anon_risk)\n",
    "\n",
    "# Noise-based risk\n",
    "noise_risk = 0.4  # Moderate risk reduction\n",
    "risk_scores.append(noise_risk)\n",
    "\n",
    "colors_risk = ['red', 'orange', 'lightgreen', 'lightblue']\n",
    "bars = axes[1, 2].bar(privacy_methods, risk_scores, color=colors_risk, alpha=0.7)\n",
    "axes[1, 2].set_title('Relative Privacy Risk Comparison')\n",
    "axes[1, 2].set_ylabel('Relative Risk Score')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, risk_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/privacy_utility_tradeoffs.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Privacy-utility trade-offs visualization saved to {output_dir}/privacy_utility_tradeoffs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Risk Assessment {#risk-assessment}\n",
    "\n",
    "Comprehensive privacy risk assessment for different techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_privacy_assessment(original_data, anonymized_data, method_name):\n",
    "    \"\"\"Perform comprehensive privacy risk assessment\"\"\"\n",
    "    assessment_results = {'method': method_name}\n",
    "    \n",
    "    # 1. Statistical Disclosure Risk\n",
    "    if 'temperature' in original_data.columns and 'temperature' in anonymized_data.columns:\n",
    "        # Calculate statistical similarity\n",
    "        orig_stats = original_data['temperature'].describe()\n",
    "        anon_stats = anonymized_data['temperature'].describe()\n",
    "        \n",
    "        stat_similarity = 1 - np.mean([\n",
    "            abs(orig_stats['mean'] - anon_stats['mean']) / orig_stats['mean'],\n",
    "            abs(orig_stats['std'] - anon_stats['std']) / orig_stats['std']\n",
    "        ])\n",
    "        \n",
    "        assessment_results['statistical_similarity'] = max(0, stat_similarity)\n",
    "    \n",
    "    # 2. Re-identification Risk\n",
    "    # Based on uniqueness of quasi-identifier combinations\n",
    "    qi_columns = ['location', 'device_type', 'floor', 'hour']\n",
    "    available_qi = [col for col in qi_columns if col in anonymized_data.columns]\n",
    "    \n",
    "    if available_qi:\n",
    "        unique_combinations = anonymized_data[available_qi].drop_duplicates()\n",
    "        uniqueness_ratio = len(unique_combinations) / len(anonymized_data)\n",
    "        reidentification_risk = min(1.0, uniqueness_ratio * 2)  # Scale factor\n",
    "        assessment_results['reidentification_risk'] = reidentification_risk\n",
    "    else:\n",
    "        assessment_results['reidentification_risk'] = 0.0\n",
    "    \n",
    "    # 3. Attribute Disclosure Risk\n",
    "    if 'temperature' in anonymized_data.columns and available_qi:\n",
    "        # Calculate how well temperature can be predicted from QI\n",
    "        try:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            from sklearn.metrics import r2_score\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            \n",
    "            # Prepare features (encode categorical variables)\n",
    "            X = pd.get_dummies(anonymized_data[available_qi], drop_first=True)\n",
    "            y = anonymized_data['temperature']\n",
    "            \n",
    "            if len(X) > 10 and X.shape[1] > 0:  # Minimum data for training\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                \n",
    "                rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "                rf.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = rf.predict(X_test)\n",
    "                prediction_accuracy = r2_score(y_test, y_pred)\n",
    "                \n",
    "                assessment_results['attribute_disclosure_risk'] = max(0, prediction_accuracy)\n",
    "            else:\n",
    "                assessment_results['attribute_disclosure_risk'] = 0.0\n",
    "        except ImportError:\n",
    "            # Fallback: simple correlation-based assessment\n",
    "            if 'hour' in available_qi:\n",
    "                correlation = abs(anonymized_data[['temperature', 'hour']].corr().iloc[0, 1])\n",
    "                assessment_results['attribute_disclosure_risk'] = correlation\n",
    "            else:\n",
    "                assessment_results['attribute_disclosure_risk'] = 0.0\n",
    "    else:\n",
    "        assessment_results['attribute_disclosure_risk'] = 0.0\n",
    "    \n",
    "    # 4. Data Utility Preservation\n",
    "    if 'temperature' in original_data.columns and 'temperature' in anonymized_data.columns:\n",
    "        # Temporal pattern preservation\n",
    "        if 'hour' in original_data.columns and 'hour' in anonymized_data.columns:\n",
    "            orig_hourly = original_data.groupby('hour')['temperature'].mean()\n",
    "            anon_hourly = anonymized_data.groupby('hour')['temperature'].mean()\n",
    "            \n",
    "            # Align hours and calculate correlation\n",
    "            common_hours = orig_hourly.index.intersection(anon_hourly.index)\n",
    "            if len(common_hours) > 3:\n",
    "                temporal_correlation = orig_hourly.loc[common_hours].corr(anon_hourly.loc[common_hours])\n",
    "                assessment_results['temporal_utility'] = max(0, temporal_correlation)\n",
    "            else:\n",
    "                assessment_results['temporal_utility'] = 0.0\n",
    "        else:\n",
    "            assessment_results['temporal_utility'] = 0.0\n",
    "    else:\n",
    "        assessment_results['temporal_utility'] = 0.0\n",
    "    \n",
    "    # 5. Overall Privacy Score (lower is better for privacy)\n",
    "    privacy_risks = [\n",
    "        assessment_results.get('reidentification_risk', 0),\n",
    "        assessment_results.get('attribute_disclosure_risk', 0)\n",
    "    ]\n",
    "    assessment_results['overall_privacy_risk'] = np.mean(privacy_risks)\n",
    "    \n",
    "    # 6. Overall Utility Score (higher is better for utility)\n",
    "    utility_scores = [\n",
    "        assessment_results.get('statistical_similarity', 0),\n",
    "        assessment_results.get('temporal_utility', 0)\n",
    "    ]\n",
    "    assessment_results['overall_utility_score'] = np.mean(utility_scores)\n",
    "    \n",
    "    return assessment_results\n",
    "\n",
    "# Perform comprehensive assessment for different privacy methods\n",
    "print(\"üîç Comprehensive Privacy Risk Assessment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "assessment_methods = {\n",
    "    'Original Data': sample_data,\n",
    "    'K-Anonymity (k=5)': anonymized_datasets.get(5, sample_data),\n",
    "    'Noise Addition (10%)': anonymized_datasets.get('noise_0.1', sample_data)\n",
    "}\n",
    "\n",
    "assessment_results = []\n",
    "\n",
    "for method_name, data in assessment_methods.items():\n",
    "    print(f\"\\nüìä Assessing {method_name}:\")\n",
    "    \n",
    "    results = comprehensive_privacy_assessment(sample_data, data, method_name)\n",
    "    assessment_results.append(results)\n",
    "    \n",
    "    print(f\"   Re-identification Risk: {results['reidentification_risk']:.3f}\")\n",
    "    print(f\"   Attribute Disclosure Risk: {results['attribute_disclosure_risk']:.3f}\")\n",
    "    print(f\"   Statistical Similarity: {results['statistical_similarity']:.3f}\")\n",
    "    print(f\"   Temporal Utility: {results['temporal_utility']:.3f}\")\n",
    "    print(f\"   Overall Privacy Risk: {results['overall_privacy_risk']:.3f}\")\n",
    "    print(f\"   Overall Utility Score: {results['overall_utility_score']:.3f}\")\n",
    "    \n",
    "    # Risk interpretation\n",
    "    risk_level = \"High\" if results['overall_privacy_risk'] > 0.7 else \\\n",
    "                \"Medium\" if results['overall_privacy_risk'] > 0.3 else \"Low\"\n",
    "    \n",
    "    utility_level = \"High\" if results['overall_utility_score'] > 0.7 else \\\n",
    "                   \"Medium\" if results['overall_utility_score'] > 0.3 else \"Low\"\n",
    "    \n",
    "    print(f\"   Privacy Risk Level: {risk_level}\")\n",
    "    print(f\"   Utility Level: {utility_level}\")\n",
    "\n",
    "# Create assessment summary table\n",
    "print(f\"\\nüìã Privacy Assessment Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_assessment = pd.DataFrame(assessment_results)\n",
    "print(df_assessment[['method', 'overall_privacy_risk', 'overall_utility_score', \n",
    "                    'reidentification_risk', 'attribute_disclosure_risk']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance and Validation {#compliance}\n",
    "\n",
    "Validate privacy-preserving techniques against regulatory standards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivacyComplianceValidator:\n",
    "    \"\"\"Validate privacy techniques against regulatory standards\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.compliance_frameworks = {\n",
    "            'GDPR': {\n",
    "                'pseudonymization': {'required': True, 'description': 'Personal data processed in a manner that cannot be attributed to a specific data subject'},\n",
    "                'anonymization': {'required': False, 'description': 'Personal data rendered anonymous in such a manner that the data subject is not identifiable'},\n",
    "                'data_minimization': {'required': True, 'description': 'Personal data shall be adequate, relevant and limited'},\n",
    "                'purpose_limitation': {'required': True, 'description': 'Personal data shall be collected for specified purposes'}\n",
    "            },\n",
    "            'HIPAA': {\n",
    "                'safe_harbor': {'required': True, 'description': 'Remove 18 specific identifiers'},\n",
    "                'expert_determination': {'required': False, 'description': 'Very small risk of re-identification'},\n",
    "                'minimum_necessary': {'required': True, 'description': 'Minimum necessary information'}\n",
    "            },\n",
    "            'CCPA': {\n",
    "                'deidentification': {'required': True, 'description': 'Information that cannot reasonably identify a consumer'},\n",
    "                'aggregation': {'required': False, 'description': 'Information aggregated or de-identified'}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def check_gdpr_compliance(self, data, anonymization_method):\n",
    "        \"\"\"Check GDPR compliance\"\"\"\n",
    "        compliance_score = 0\n",
    "        max_score = 4\n",
    "        compliance_details = {}\n",
    "        \n",
    "        # Pseudonymization check\n",
    "        if 'device_id' not in data.columns or data['device_id'].dtype == 'object':\n",
    "            compliance_score += 1\n",
    "            compliance_details['pseudonymization'] = \"‚úÖ Device IDs appear pseudonymized\"\n",
    "        else:\n",
    "            compliance_details['pseudonymization'] = \"‚ùå Direct identifiers present\"\n",
    "        \n",
    "        # Anonymization assessment\n",
    "        if anonymization_method in ['k_anonymity', 'differential_privacy', 'noise_addition']:\n",
    "            compliance_score += 1\n",
    "            compliance_details['anonymization'] = f\"‚úÖ {anonymization_method} applied\"\n",
    "        else:\n",
    "            compliance_details['anonymization'] = \"‚ö†Ô∏è No formal anonymization applied\"\n",
    "        \n",
    "        # Data minimization (check if only necessary columns present)\n",
    "        essential_columns = ['timestamp', 'temperature', 'location', 'device_type']\n",
    "        if len(data.columns) <= len(essential_columns) + 2:  # Allow some flexibility\n",
    "            compliance_score += 1\n",
    "            compliance_details['data_minimization'] = \"‚úÖ Data appears minimized\"\n",
    "        else:\n",
    "            compliance_details['data_minimization'] = \"‚ö†Ô∏è Potential excess data\"\n",
    "        \n",
    "        # Purpose limitation (assumed if data is synthetic)\n",
    "        compliance_score += 1\n",
    "        compliance_details['purpose_limitation'] = \"‚úÖ Synthetic data for specified purposes\"\n",
    "        \n",
    "        compliance_percentage = (compliance_score / max_score) * 100\n",
    "        \n",
    "        return {\n",
    "            'framework': 'GDPR',\n",
    "            'compliance_score': compliance_score,\n",
    "            'max_score': max_score,\n",
    "            'compliance_percentage': compliance_percentage,\n",
    "            'details': compliance_details,\n",
    "            'recommendation': self._get_gdpr_recommendation(compliance_percentage)\n",
    "        }\n",
    "    \n",
    "    def check_hipaa_compliance(self, data, anonymization_method):\n",
    "        \"\"\"Check HIPAA compliance (Safe Harbor method)\"\"\"\n",
    "        compliance_score = 0\n",
    "        max_score = 3\n",
    "        compliance_details = {}\n",
    "        \n",
    "        # Safe Harbor - check for absence of 18 identifiers\n",
    "        prohibited_columns = ['name', 'address', 'birth_date', 'phone', 'fax', 'email', \n",
    "                            'ssn', 'mrn', 'account_number', 'certificate_number', \n",
    "                            'vehicle_id', 'device_serial', 'web_url', 'ip_address', \n",
    "                            'biometric_id', 'photo', 'unique_code']\n",
    "        \n",
    "        found_prohibited = [col for col in data.columns if any(prohibited in col.lower() for prohibited in prohibited_columns)]\n",
    "        \n",
    "        if not found_prohibited:\n",
    "            compliance_score += 1\n",
    "            compliance_details['safe_harbor'] = \"‚úÖ No prohibited identifiers found\"\n",
    "        else:\n",
    "            compliance_details['safe_harbor'] = f\"‚ùå Found prohibited identifiers: {found_prohibited}\"\n",
    "        \n",
    "        # Expert determination (privacy risk assessment)\n",
    "        if anonymization_method in ['differential_privacy', 'k_anonymity']:\n",
    "            compliance_score += 1\n",
    "            compliance_details['expert_determination'] = \"‚úÖ Formal privacy method applied\"\n",
    "        else:\n",
    "            compliance_details['expert_determination'] = \"‚ö†Ô∏è Informal privacy protection\"\n",
    "        \n",
    "        # Minimum necessary\n",
    "        if len(data.columns) <= 8:  # Reasonable limit for IoT data\n",
    "            compliance_score += 1\n",
    "            compliance_details['minimum_necessary'] = \"‚úÖ Data appears minimal\"\n",
    "        else:\n",
    "            compliance_details['minimum_necessary'] = \"‚ö†Ô∏è Consider reducing data elements\"\n",
    "        \n",
    "        compliance_percentage = (compliance_score / max_score) * 100\n",
    "        \n",
    "        return {\n",
    "            'framework': 'HIPAA',\n",
    "            'compliance_score': compliance_score,\n",
    "            'max_score': max_score,\n",
    "            'compliance_percentage': compliance_percentage,\n",
    "            'details': compliance_details,\n",
    "            'recommendation': self._get_hipaa_recommendation(compliance_percentage)\n",
    "        }\n",
    "    \n",
    "    def _get_gdpr_recommendation(self, compliance_percentage):\n",
    "        if compliance_percentage >= 75:\n",
    "            return \"Good GDPR compliance posture. Consider documenting privacy measures.\"\n",
    "        elif compliance_percentage >= 50:\n",
    "            return \"Moderate compliance. Strengthen anonymization techniques.\"\n",
    "        else:\n",
    "            return \"Low compliance. Implement formal privacy-preserving methods.\"\n",
    "    \n",
    "    def _get_hipaa_recommendation(self, compliance_percentage):\n",
    "        if compliance_percentage >= 67:\n",
    "            return \"Good HIPAA compliance posture. Document privacy controls.\"\n",
    "        elif compliance_percentage >= 33:\n",
    "            return \"Moderate compliance. Consider expert determination review.\"\n",
    "        else:\n",
    "            return \"Low compliance. Remove identifiers and implement Safe Harbor.\"\n",
    "\n",
    "# Perform compliance validation\n",
    "compliance_validator = PrivacyComplianceValidator()\n",
    "\n",
    "print(\"‚öñÔ∏è Privacy Compliance Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different methods for compliance\n",
    "compliance_test_data = {\n",
    "    'Original Data': (sample_data, 'none'),\n",
    "    'K-Anonymity (k=5)': (anonymized_datasets.get(5, sample_data), 'k_anonymity'),\n",
    "    'Noise Addition (10%)': (anonymized_datasets.get('noise_0.1', sample_data), 'noise_addition')\n",
    "}\n",
    "\n",
    "compliance_results = []\n",
    "\n",
    "for method_name, (data, anon_method) in compliance_test_data.items():\n",
    "    print(f\"\\nüìã Compliance Check: {method_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # GDPR Compliance\n",
    "    gdpr_result = compliance_validator.check_gdpr_compliance(data, anon_method)\n",
    "    print(f\"\\nüá™üá∫ GDPR Compliance: {gdpr_result['compliance_percentage']:.0f}%\")\n",
    "    for requirement, status in gdpr_result['details'].items():\n",
    "        print(f\"   {requirement.title()}: {status}\")\n",
    "    print(f\"   Recommendation: {gdpr_result['recommendation']}\")\n",
    "    \n",
    "    # HIPAA Compliance\n",
    "    hipaa_result = compliance_validator.check_hipaa_compliance(data, anon_method)\n",
    "    print(f\"\\nüè• HIPAA Compliance: {hipaa_result['compliance_percentage']:.0f}%\")\n",
    "    for requirement, status in hipaa_result['details'].items():\n",
    "        print(f\"   {requirement.title()}: {status}\")\n",
    "    print(f\"   Recommendation: {hipaa_result['recommendation']}\")\n",
    "    \n",
    "    compliance_results.append({\n",
    "        'method': method_name,\n",
    "        'gdpr_score': gdpr_result['compliance_percentage'],\n",
    "        'hipaa_score': hipaa_result['compliance_percentage']\n",
    "    })\n",
    "\n",
    "# Create compliance summary visualization\n",
    "compliance_df = pd.DataFrame(compliance_results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Compliance scores comparison\n",
    "x = np.arange(len(compliance_df))\n",
    "width = 0.35\n",
    "\n",
    "gdpr_bars = ax1.bar(x - width/2, compliance_df['gdpr_score'], width, \n",
    "                   label='GDPR', color='steelblue', alpha=0.7)\n",
    "hipaa_bars = ax1.bar(x + width/2, compliance_df['hipaa_score'], width, \n",
    "                    label='HIPAA', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax1.set_title('Privacy Compliance Scores by Method')\n",
    "ax1.set_ylabel('Compliance Score (%)')\n",
    "ax1.set_xlabel('Privacy Method')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(compliance_df['method'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [gdpr_bars, hipaa_bars]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.0f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Privacy-Utility-Compliance triangle\n",
    "if assessment_results:\n",
    "    methods = [result['method'] for result in assessment_results]\n",
    "    privacy_scores = [1 - result['overall_privacy_risk'] for result in assessment_results]  # Invert risk to score\n",
    "    utility_scores = [result['overall_utility_score'] for result in assessment_results]\n",
    "    \n",
    "    # Get corresponding compliance scores\n",
    "    compliance_scores = []\n",
    "    for method in methods:\n",
    "        compliance_row = compliance_df[compliance_df['method'] == method]\n",
    "        if not compliance_row.empty:\n",
    "            avg_compliance = (compliance_row['gdpr_score'].iloc[0] + compliance_row['hipaa_score'].iloc[0]) / 200  # Scale to 0-1\n",
    "            compliance_scores.append(avg_compliance)\n",
    "        else:\n",
    "            compliance_scores.append(0.5)\n",
    "    \n",
    "    scatter = ax2.scatter(utility_scores, privacy_scores, \n",
    "                         c=compliance_scores, s=100, \n",
    "                         cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, method in enumerate(methods):\n",
    "        ax2.annotate(method.replace('Data', '').strip(), \n",
    "                    (utility_scores[i], privacy_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax2.set_title('Privacy-Utility-Compliance Relationship')\n",
    "    ax2.set_xlabel('Utility Score')\n",
    "    ax2.set_ylabel('Privacy Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Compliance Score', rotation=270, labelpad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/privacy_compliance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Compliance analysis saved to {output_dir}/privacy_compliance_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Preservation Summary and Best Practices\n",
    "\n",
    "Comprehensive summary of privacy preservation techniques and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîí PRIVACY PRESERVATION IN SYNTHETIC IOT DATA - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Analysis Overview:\")\n",
    "print(f\"   Original Dataset Size: {len(original_df):,} records\")\n",
    "print(f\"   Sample Size for Analysis: {len(sample_data):,} records\")\n",
    "print(f\"   IoT Devices Simulated: {n_devices}\")\n",
    "print(f\"   Time Period: 24 hours (minute resolution)\")\n",
    "print(f\"   Privacy Techniques Evaluated: 5+\")\n",
    "print(f\"   Compliance Frameworks: GDPR, HIPAA\")\n",
    "\n",
    "print(f\"\\nüîê Privacy Techniques Assessed:\")\n",
    "print(f\"   ‚úÖ Differential Privacy (Œµ = 0.1 to 5.0)\")\n",
    "print(f\"   ‚úÖ K-Anonymity (k = 3, 5, 10)\")\n",
    "print(f\"   ‚úÖ L-Diversity Analysis\")\n",
    "print(f\"   ‚úÖ Noise Addition (5%, 10%, 20%)\")\n",
    "print(f\"   ‚úÖ Data Suppression\")\n",
    "print(f\"   ‚úÖ Generalization Hierarchies\")\n",
    "\n",
    "print(f\"\\nüéØ Key Findings:\")\n",
    "\n",
    "if assessment_results:\n",
    "    print(f\"\\n   üìà Privacy-Utility Trade-offs:\")\n",
    "    for result in assessment_results:\n",
    "        method = result['method']\n",
    "        privacy_risk = result['overall_privacy_risk']\n",
    "        utility_score = result['overall_utility_score']\n",
    "        \n",
    "        print(f\"     {method}:\")\n",
    "        print(f\"       Privacy Risk: {privacy_risk:.3f} ({'Low' if privacy_risk < 0.3 else 'Medium' if privacy_risk < 0.7 else 'High'})\")\n",
    "        print(f\"       Utility Score: {utility_score:.3f} ({'High' if utility_score > 0.7 else 'Medium' if utility_score > 0.3 else 'Low'})\")\n",
    "\n",
    "if dp_results:\n",
    "    print(f\"\\n   üî¢ Differential Privacy Insights:\")\n",
    "    best_dp_epsilon = min(dp_results.keys(), key=lambda e: abs(mean_errors[list(dp_results.keys()).index(e)] - 0.1))\n",
    "    print(f\"     Recommended Œµ value: {best_dp_epsilon} (balance of privacy and utility)\")\n",
    "    print(f\"     High privacy (Œµ < 1.0): Significant noise, strong privacy\")\n",
    "    print(f\"     Moderate privacy (Œµ = 1-2): Balanced approach\")\n",
    "    print(f\"     Low privacy (Œµ > 2): Minimal noise, high utility\")\n",
    "\n",
    "if compliance_results:\n",
    "    print(f\"\\n   ‚öñÔ∏è Regulatory Compliance:\")\n",
    "    for result in compliance_results:\n",
    "        method = result['method']\n",
    "        gdpr_score = result['gdpr_score']\n",
    "        hipaa_score = result['hipaa_score']\n",
    "        \n",
    "        print(f\"     {method}:\")\n",
    "        print(f\"       GDPR Compliance: {gdpr_score:.0f}%\")\n",
    "        print(f\"       HIPAA Compliance: {hipaa_score:.0f}%\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Practice Recommendations:\")\n",
    "print(f\"\\n   üéØ Method Selection Guidelines:\")\n",
    "print(f\"     ‚Ä¢ High Privacy Requirements: Differential Privacy (Œµ ‚â§ 1.0) + K-Anonymity\")\n",
    "print(f\"     ‚Ä¢ Balanced Approach: K-Anonymity (k=5) + Controlled Noise (‚â§10%)\")\n",
    "print(f\"     ‚Ä¢ High Utility Needs: Light Noise Addition (‚â§5%) + Pseudonymization\")\n",
    "print(f\"     ‚Ä¢ Regulatory Compliance: Document chosen methods and risk assessments\")\n",
    "\n",
    "print(f\"\\n   üîß Implementation Guidelines:\")\n",
    "print(f\"     üìä Data Assessment:\")\n",
    "print(f\"       ‚Ä¢ Identify quasi-identifiers (location, device type, temporal patterns)\")\n",
    "print(f\"       ‚Ä¢ Assess uniqueness and re-identification risk\")\n",
    "print(f\"       ‚Ä¢ Evaluate sensitive attribute disclosure risk\")\n",
    "     \n",
    "print(f\"\\n     üéõÔ∏è Parameter Tuning:\")\n",
    "print(f\"       ‚Ä¢ Start with conservative parameters (high privacy)\")\n",
    "print(f\"       ‚Ä¢ Gradually adjust based on utility requirements\")\n",
    "print(f\"       ‚Ä¢ Test privacy guarantees with worst-case scenarios\")\n",
    "print(f\"       ‚Ä¢ Validate utility preservation for intended use cases\")\n",
    "\n",
    "print(f\"\\n     üìã Quality Assurance:\")\n",
    "print(f\"       ‚Ä¢ Implement automated privacy risk monitoring\")\n",
    "print(f\"       ‚Ä¢ Regular compliance audits and validation\")\n",
    "print(f\"       ‚Ä¢ Document privacy-preserving processes\")\n",
    "print(f\"       ‚Ä¢ Train staff on privacy-preserving techniques\")\n",
    "\n",
    "print(f\"\\n   üö® Risk Mitigation Strategies:\")\n",
    "print(f\"     üõ°Ô∏è Defense in Depth:\")\n",
    "print(f\"       ‚Ä¢ Layer multiple privacy techniques (DP + K-Anon + Noise)\")\n",
    "print(f\"       ‚Ä¢ Implement access controls and audit trails\")\n",
    "print(f\"       ‚Ä¢ Regular privacy impact assessments\")\n",
    "print(f\"       ‚Ä¢ Incident response procedures for privacy breaches\")\n",
    "\n",
    "print(f\"\\n     üìè Ongoing Monitoring:\")\n",
    "print(f\"       ‚Ä¢ Track privacy metrics over time\")\n",
    "print(f\"       ‚Ä¢ Monitor for new attack vectors\")\n",
    "print(f\"       ‚Ä¢ Update techniques as technology evolves\")\n",
    "print(f\"       ‚Ä¢ Benchmark against industry standards\")\n",
    "\n",
    "print(f\"\\nüåê Industry-Specific Considerations:\")\n",
    "print(f\"\\n   üè• Healthcare IoT:\")\n",
    "print(f\"     ‚Ä¢ HIPAA Safe Harbor compliance mandatory\")\n",
    "print(f\"     ‚Ä¢ Expert determination for complex cases\")\n",
    "print(f\"     ‚Ä¢ Strong differential privacy (Œµ ‚â§ 0.5)\")\n",
    "print(f\"     ‚Ä¢ Temporal pattern protection\")\n",
    "\n",
    "print(f\"\\n   üè≠ Industrial IoT:\")\n",
    "print(f\"     ‚Ä¢ Focus on competitive intelligence protection\")\n",
    "print(f\"     ‚Ä¢ Operational pattern anonymization\")\n",
    "print(f\"     ‚Ä¢ Moderate privacy levels acceptable (Œµ ‚â§ 2.0)\")\n",
    "print(f\"     ‚Ä¢ Preserve fault detection capabilities\")\n",
    "\n",
    "print(f\"\\n   üè† Smart Buildings/Cities:\")\n",
    "print(f\"     ‚Ä¢ GDPR compliance in EU jurisdictions\")\n",
    "print(f\"     ‚Ä¢ Behavioral pattern protection\")\n",
    "print(f\"     ‚Ä¢ Balance privacy with emergency services\")\n",
    "print(f\"     ‚Ä¢ Consent management for data collection\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Privacy Analysis Artifacts:\")\n",
    "artifacts = [\n",
    "    \"privacy_utility_tradeoffs.png\",\n",
    "    \"privacy_compliance_analysis.png\"\n",
    "]\n",
    "\n",
    "for artifact in artifacts:\n",
    "    artifact_path = os.path.join(output_dir, artifact)\n",
    "    if os.path.exists(artifact_path):\n",
    "        file_size = os.path.getsize(artifact_path)\n",
    "        print(f\"   üìä {artifact} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüìÅ All Output Files:\")\n",
    "for file in sorted(os.listdir(output_dir)):\n",
    "    if file.endswith(('.png', '.json', '.csv')):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"   üìÑ {file} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüéØ Action Items for Privacy-Preserving IoT Systems:\")\n",
    "print(f\"   ‚òê Conduct privacy risk assessment for your specific IoT use case\")\n",
    "print(f\"   ‚òê Select appropriate privacy techniques based on risk tolerance\")\n",
    "print(f\"   ‚òê Implement privacy-by-design in data collection systems\")\n",
    "print(f\"   ‚òê Establish privacy metrics and monitoring procedures\")\n",
    "print(f\"   ‚òê Create privacy documentation and compliance evidence\")\n",
    "print(f\"   ‚òê Train development and operations teams on privacy techniques\")\n",
    "print(f\"   ‚òê Plan for regulatory audits and privacy breach response\")\n",
    "print(f\"   ‚òê Stay updated on evolving privacy regulations and techniques\")\n",
    "\n",
    "print(f\"\\nüî¨ Advanced Topics for Further Exploration:\")\n",
    "print(f\"   üìö Research Areas:\")\n",
    "print(f\"     ‚Ä¢ Federated learning for privacy-preserving IoT analytics\")\n",
    "print(f\"     ‚Ä¢ Homomorphic encryption for computation on encrypted data\")\n",
    "print(f\"     ‚Ä¢ Secure multi-party computation for collaborative analytics\")\n",
    "print(f\"     ‚Ä¢ Zero-knowledge proofs for privacy-preserving verification\")\n",
    "print(f\"     ‚Ä¢ Synthetic data generation with formal privacy guarantees\")\n",
    "\n",
    "print(f\"\\n   üõ†Ô∏è Tools and Frameworks:\")\n",
    "print(f\"     ‚Ä¢ Google's Differential Privacy Library\")\n",
    "print(f\"     ‚Ä¢ Microsoft's SmartNoise\")\n",
    "print(f\"     ‚Ä¢ IBM's Differential Privacy Library\")\n",
    "print(f\"     ‚Ä¢ ARX Data Anonymization Tool\")\n",
    "print(f\"     ‚Ä¢ Amnesia Anonymization Tool\")\n",
    "\n",
    "print(f\"\\nüéâ Privacy preservation analysis complete\!\")\n",
    "print(f\"    Your IoT systems can now balance privacy protection with data utility.\")\n",
    "print(f\"    Remember: Privacy is not a one-time implementation but an ongoing commitment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF < /dev/null