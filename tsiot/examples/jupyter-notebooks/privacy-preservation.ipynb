{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Preservation in Synthetic IoT Data Generation\n",
    "\n",
    "This notebook demonstrates privacy-preserving techniques for synthetic IoT data generation, including differential privacy and anonymization methods.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Privacy Fundamentals](#fundamentals)\n",
    "2. [Differential Privacy Techniques](#differential-privacy)\n",
    "3. [Data Anonymization Methods](#anonymization)\n",
    "4. [Privacy-Utility Trade-offs](#trade-offs)\n",
    "5. [Privacy Risk Assessment](#risk-assessment)\n",
    "6. [Compliance and Validation](#compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "output_dir = \"privacy_preservation_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Privacy preservation environment ready\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Fundamentals {#fundamentals}\n",
    "\n",
    "Understanding privacy concepts in the context of IoT synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivacyAnalyzer:\n",
    "    \"\"\"Comprehensive privacy analysis for synthetic data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.privacy_metrics = {}\n",
    "    \n",
    "    def k_anonymity_assessment(self, data, quasi_identifiers):\n",
    "        \"\"\"Assess k-anonymity for quasi-identifying attributes\"\"\"\n",
    "        if not quasi_identifiers:\n",
    "            return {\"k_value\": float('inf'), \"message\": \"No quasi-identifiers specified\"}\n",
    "        \n",
    "        # Group by quasi-identifiers and count group sizes\n",
    "        grouped = data.groupby(quasi_identifiers).size()\n",
    "        k_value = grouped.min()\n",
    "        \n",
    "        return {\n",
    "            \"k_value\": k_value,\n",
    "            \"group_sizes\": grouped.describe(),\n",
    "            \"risk_level\": \"High\" if k_value < 3 else \"Medium\" if k_value < 5 else \"Low\"\n",
    "        }\n",
    "    \n",
    "    def l_diversity_assessment(self, data, quasi_identifiers, sensitive_attribute):\n",
    "        \"\"\"Assess l-diversity for sensitive attributes\"\"\"\n",
    "        if not quasi_identifiers or sensitive_attribute not in data.columns:\n",
    "            return {\"l_value\": 0, \"message\": \"Invalid parameters\"}\n",
    "        \n",
    "        # Group by quasi-identifiers and check diversity of sensitive attribute\n",
    "        diversity_values = []\n",
    "        for name, group in data.groupby(quasi_identifiers):\n",
    "            unique_sensitive = group[sensitive_attribute].nunique()\n",
    "            diversity_values.append(unique_sensitive)\n",
    "        \n",
    "        l_value = min(diversity_values) if diversity_values else 0\n",
    "        \n",
    "        return {\n",
    "            \"l_value\": l_value,\n",
    "            \"diversity_distribution\": pd.Series(diversity_values).describe(),\n",
    "            \"risk_level\": \"High\" if l_value < 2 else \"Medium\" if l_value < 3 else \"Low\"\n",
    "        }\n",
    "    \n",
    "    def membership_inference_risk(self, real_data, synthetic_data):\n",
    "        \"\"\"Assess membership inference attack risk\"\"\"\n",
    "        # Calculate distance-based membership inference risk\n",
    "        min_distances = []\n",
    "        \n",
    "        for real_point in real_data.values:\n",
    "            distances = np.linalg.norm(synthetic_data.values - real_point, axis=1)\n",
    "            min_distances.append(np.min(distances))\n",
    "        \n",
    "        # Risk metrics\n",
    "        threshold = np.std(real_data.values) * 0.1  # 10% of std as threshold\n",
    "        at_risk_points = np.sum(np.array(min_distances) < threshold)\n",
    "        risk_ratio = at_risk_points / len(min_distances)\n",
    "        \n",
    "        return {\n",
    "            \"at_risk_points\": at_risk_points,\n",
    "            \"total_points\": len(min_distances),\n",
    "            \"risk_ratio\": risk_ratio,\n",
    "            \"mean_min_distance\": np.mean(min_distances),\n",
    "            \"std_min_distance\": np.std(min_distances),\n",
    "            \"risk_level\": \"High\" if risk_ratio > 0.1 else \"Medium\" if risk_ratio > 0.05 else \"Low\"\n",
    "        }\n",
    "    \n",
    "    def attribute_inference_risk(self, data, target_attribute, auxiliary_attributes):\n",
    "        \"\"\"Assess attribute inference attack risk using mutual information\"\"\"\n",
    "        if target_attribute not in data.columns:\n",
    "            return {\"risk_score\": 0, \"message\": \"Target attribute not found\"}\n",
    "        \n",
    "        # Calculate mutual information between target and auxiliary attributes\n",
    "        mi_scores = []\n",
    "        \n",
    "        for aux_attr in auxiliary_attributes:\n",
    "            if aux_attr in data.columns and aux_attr \!= target_attribute:\n",
    "                # Discretize continuous variables for MI calculation\n",
    "                target_discrete = pd.cut(data[target_attribute], bins=10, labels=False)\n",
    "                aux_discrete = pd.cut(data[aux_attr], bins=10, labels=False)\n",
    "                \n",
    "                mi = mutual_info_score(target_discrete.dropna(), aux_discrete.dropna())\n",
    "                mi_scores.append(mi)\n",
    "        \n",
    "        risk_score = max(mi_scores) if mi_scores else 0\n",
    "        \n",
    "        return {\n",
    "            \"risk_score\": risk_score,\n",
    "            \"mi_scores\": mi_scores,\n",
    "            \"risk_level\": \"High\" if risk_score > 0.5 else \"Medium\" if risk_score > 0.2 else \"Low\"\n",
    "        }\n",
    "\n",
    "# Initialize privacy analyzer\n",
    "privacy_analyzer = PrivacyAnalyzer()\n",
    "print(\"Privacy analysis framework initialized\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data for Privacy Analysis\n",
    "\n",
    "Create synthetic datasets to demonstrate privacy preservation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate base synthetic data\n",
    "print(\"🔄 Generating base datasets for privacy analysis...\")\n",
    "\n",
    "# Create \"original\" (simulated real) data\n",
    "np.random.seed(42)\n",
    "n_devices = 100\n",
    "n_timepoints = 1440  # 24 hours\n",
    "\n",
    "# Device metadata (quasi-identifiers)\n",
    "device_locations = np.random.choice(['building_A', 'building_B', 'building_C'], n_devices)\n",
    "device_types = np.random.choice(['sensor_type_1', 'sensor_type_2', 'sensor_type_3'], n_devices)\n",
    "device_floors = np.random.randint(1, 11, n_devices)  # Floors 1-10\n",
    "\n",
    "# Generate time series data for each device\n",
    "original_data = []\n",
    "timestamps = pd.date_range('2023-01-01', periods=n_timepoints, freq='1min')\n",
    "\n",
    "for device_id in range(n_devices):\n",
    "    # Base temperature with device-specific characteristics\n",
    "    base_temp = 18 + np.random.normal(0, 2)  # Device-specific baseline\n",
    "    \n",
    "    # Daily pattern\n",
    "    t = np.arange(n_timepoints)\n",
    "    daily_pattern = 3 * np.sin(2 * np.pi * t / 1440)  # Daily cycle\n",
    "    \n",
    "    # Noise\n",
    "    noise = np.random.normal(0, 0.5, n_timepoints)\n",
    "    \n",
    "    # Device-specific behavior\n",
    "    if device_types[device_id] == 'sensor_type_1':\n",
    "        multiplier = 1.2\n",
    "    elif device_types[device_id] == 'sensor_type_2':\n",
    "        multiplier = 0.8\n",
    "    else:\n",
    "        multiplier = 1.0\n",
    "    \n",
    "    temperatures = (base_temp + daily_pattern * multiplier + noise)\n",
    "    \n",
    "    for i, (timestamp, temp) in enumerate(zip(timestamps, temperatures)):\n",
    "        original_data.append({\n",
    "            'device_id': device_id,\n",
    "            'timestamp': timestamp,\n",
    "            'temperature': temp,\n",
    "            'location': device_locations[device_id],\n",
    "            'device_type': device_types[device_id],\n",
    "            'floor': device_floors[device_id],\n",
    "            'hour': timestamp.hour,\n",
    "            'day_of_week': timestamp.dayofweek\n",
    "        })\n",
    "\n",
    "original_df = pd.DataFrame(original_data)\n",
    "\n",
    "print(f\"✅ Generated original dataset: {original_df.shape}\")\n",
    "print(f\"   Devices: {n_devices}\")\n",
    "print(f\"   Time points per device: {n_timepoints}\")\n",
    "print(f\"   Total records: {len(original_df)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\n📊 Sample Original Data:\")\n",
    "print(original_df.head(10))\n",
    "\n",
    "print(f\"\\n📈 Basic Statistics:\")\n",
    "print(original_df[['temperature', 'floor']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy Techniques {#differential-privacy}\n",
    "\n",
    "Implement and demonstrate differential privacy mechanisms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialPrivacy:\n",
    "    \"\"\"Differential privacy mechanisms for synthetic data\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=1.0):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def laplace_mechanism(self, true_value, sensitivity):\n",
    "        \"\"\"Add Laplace noise for differential privacy\"\"\"\n",
    "        scale = sensitivity / self.epsilon\n",
    "        noise = np.random.laplace(0, scale)\n",
    "        return true_value + noise\n",
    "    \n",
    "    def gaussian_mechanism(self, true_value, sensitivity, delta=1e-5):\n",
    "        \"\"\"Add Gaussian noise for (epsilon, delta)-differential privacy\"\"\"\n",
    "        sigma = (sensitivity * np.sqrt(2 * np.log(1.25 / delta))) / self.epsilon\n",
    "        noise = np.random.normal(0, sigma)\n",
    "        return true_value + noise\n",
    "    \n",
    "    def exponential_mechanism(self, candidates, utility_function, sensitivity):\n",
    "        \"\"\"Select candidate based on exponential mechanism\"\"\"\n",
    "        utilities = [utility_function(c) for c in candidates]\n",
    "        probabilities = np.exp((self.epsilon * np.array(utilities)) / (2 * sensitivity))\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "        \n",
    "        selected_idx = np.random.choice(len(candidates), p=probabilities)\n",
    "        return candidates[selected_idx]\n",
    "    \n",
    "    def privatize_statistics(self, data, statistics=['mean', 'std', 'min', 'max']):\n",
    "        \"\"\"Apply differential privacy to statistical measures\"\"\"\n",
    "        privatized_stats = {}\n",
    "        \n",
    "        # Assume data is normalized to [0, 1] range for sensitivity calculation\n",
    "        data_normalized = (data - data.min()) / (data.max() - data.min())\n",
    "        \n",
    "        for stat in statistics:\n",
    "            if stat == 'mean':\n",
    "                true_value = data_normalized.mean()\n",
    "                sensitivity = 1.0 / len(data)  # L1 sensitivity for mean\n",
    "                privatized_stats[stat] = self.laplace_mechanism(true_value, sensitivity)\n",
    "            \n",
    "            elif stat == 'std':\n",
    "                true_value = data_normalized.std()\n",
    "                sensitivity = 1.0 / len(data)  # Approximation\n",
    "                privatized_stats[stat] = max(0, self.laplace_mechanism(true_value, sensitivity))\n",
    "            \n",
    "            elif stat == 'min':\n",
    "                true_value = data_normalized.min()\n",
    "                sensitivity = 1.0 / len(data)\n",
    "                privatized_stats[stat] = self.laplace_mechanism(true_value, sensitivity)\n",
    "            \n",
    "            elif stat == 'max':\n",
    "                true_value = data_normalized.max()\n",
    "                sensitivity = 1.0 / len(data)\n",
    "                privatized_stats[stat] = self.laplace_mechanism(true_value, sensitivity)\n",
    "        \n",
    "        return privatized_stats\n",
    "    \n",
    "    def privatize_histogram(self, data, bins=20):\n",
    "        \"\"\"Create differentially private histogram\"\"\"\n",
    "        hist, bin_edges = np.histogram(data, bins=bins)\n",
    "        \n",
    "        # Add Laplace noise to each bin count\n",
    "        sensitivity = 1  # Adding/removing one record changes one bin by 1\n",
    "        privatized_hist = []\n",
    "        \n",
    "        for count in hist:\n",
    "            noisy_count = self.laplace_mechanism(count, sensitivity)\n",
    "            privatized_hist.append(max(0, noisy_count))  # Ensure non-negative\n",
    "        \n",
    "        return np.array(privatized_hist), bin_edges\n",
    "\n",
    "# Test different privacy levels\n",
    "privacy_levels = [0.1, 0.5, 1.0, 2.0, 5.0]  # Different epsilon values\n",
    "dp_results = {}\n",
    "\n",
    "print(\"🔒 Testing Differential Privacy Mechanisms:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Focus on temperature data for privacy analysis\n",
    "temperature_data = original_df['temperature'].values\n",
    "\n",
    "for epsilon in privacy_levels:\n",
    "    print(f\"\\n📊 Privacy Level ε = {epsilon}:\")\n",
    "    \n",
    "    dp = DifferentialPrivacy(epsilon=epsilon)\n",
    "    \n",
    "    # Privatize statistics\n",
    "    privatized_stats = dp.privatize_statistics(temperature_data)\n",
    "    \n",
    "    # Privatize histogram\n",
    "    privatized_hist, bin_edges = dp.privatize_histogram(temperature_data)\n",
    "    \n",
    "    dp_results[epsilon] = {\n",
    "        'statistics': privatized_stats,\n",
    "        'histogram': privatized_hist,\n",
    "        'bin_edges': bin_edges\n",
    "    }\n",
    "    \n",
    "    # Compare with original statistics\n",
    "    original_stats = {\n",
    "        'mean': temperature_data.mean(),\n",
    "        'std': temperature_data.std(),\n",
    "        'min': temperature_data.min(),\n",
    "        'max': temperature_data.max()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Original Mean: {original_stats['mean']:.3f}, DP Mean: {privatized_stats['mean']:.3f}\")\n",
    "    print(f\"   Original Std:  {original_stats['std']:.3f}, DP Std:  {privatized_stats['std']:.3f}\")\n",
    "    \n",
    "    # Calculate utility loss\n",
    "    mean_error = abs(original_stats['mean'] - privatized_stats['mean'])\n",
    "    std_error = abs(original_stats['std'] - privatized_stats['std'])\n",
    "    \n",
    "    print(f\"   Mean Error: {mean_error:.3f}, Std Error: {std_error:.3f}\")\n",
    "    \n",
    "    privacy_level = \"High\" if epsilon < 1.0 else \"Medium\" if epsilon < 2.0 else \"Low\"\n",
    "    print(f\"   Privacy Level: {privacy_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Anonymization Methods {#anonymization}\n",
    "\n",
    "Implement k-anonymity, l-diversity, and other anonymization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnonymizer:\n",
    "    \"\"\"Data anonymization techniques\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generalize_numerical(self, data, column, bins):\n",
    "        \"\"\"Generalize numerical data into ranges\"\"\"\n",
    "        return pd.cut(data[column], bins=bins, labels=[f\"Range_{i}\" for i in range(bins)])\n",
    "    \n",
    "    def generalize_categorical(self, data, column, hierarchy):\n",
    "        \"\"\"Generalize categorical data using hierarchy\"\"\"\n",
    "        return data[column].map(hierarchy)\n",
    "    \n",
    "    def suppress_outliers(self, data, column, threshold=2.5):\n",
    "        \"\"\"Suppress outliers by replacing with '*'\"\"\"\n",
    "        z_scores = np.abs(stats.zscore(data[column]))\n",
    "        outlier_mask = z_scores > threshold\n",
    "        \n",
    "        suppressed_data = data[column].copy()\n",
    "        suppressed_data[outlier_mask] = '*'  # Suppress outliers\n",
    "        \n",
    "        return suppressed_data\n",
    "    \n",
    "    def achieve_k_anonymity(self, data, quasi_identifiers, k=3):\n",
    "        \"\"\"Apply generalization to achieve k-anonymity\"\"\"\n",
    "        anonymized_data = data.copy()\n",
    "        \n",
    "        # Generalize quasi-identifiers\n",
    "        if 'floor' in quasi_identifiers:\n",
    "            # Group floors into ranges\n",
    "            anonymized_data['floor_range'] = pd.cut(data['floor'], \n",
    "                                                   bins=[0, 3, 6, 10], \n",
    "                                                   labels=['Low (1-3)', 'Mid (4-6)', 'High (7-10)'])\n",
    "        \n",
    "        if 'hour' in quasi_identifiers:\n",
    "            # Group hours into time periods\n",
    "            hour_mapping = {\n",
    "                **{h: 'Night (0-5)' for h in range(0, 6)},\n",
    "                **{h: 'Morning (6-11)' for h in range(6, 12)},\n",
    "                **{h: 'Afternoon (12-17)' for h in range(12, 18)},\n",
    "                **{h: 'Evening (18-23)' for h in range(18, 24)}\n",
    "            }\n",
    "            anonymized_data['hour_period'] = data['hour'].map(hour_mapping)\n",
    "        \n",
    "        if 'device_type' in quasi_identifiers:\n",
    "            # Keep device type as is (already general enough)\n",
    "            anonymized_data['device_type_anon'] = data['device_type']\n",
    "        \n",
    "        # Check k-anonymity with generalized attributes\n",
    "        generalized_qi = [col for col in ['floor_range', 'hour_period', 'device_type_anon'] \n",
    "                         if col in anonymized_data.columns]\n",
    "        \n",
    "        if generalized_qi:\n",
    "            group_sizes = anonymized_data.groupby(generalized_qi).size()\n",
    "            min_group_size = group_sizes.min()\n",
    "            \n",
    "            # If still not k-anonymous, apply suppression\n",
    "            if min_group_size < k:\n",
    "                small_groups = group_sizes[group_sizes < k].index\n",
    "                for group in small_groups:\n",
    "                    if isinstance(group, tuple):\n",
    "                        mask = anonymized_data[generalized_qi].apply(\n",
    "                            lambda row: tuple(row) == group, axis=1)\n",
    "                    else:\n",
    "                        mask = anonymized_data[generalized_qi[0]] == group\n",
    "                    \n",
    "                    # Suppress by marking with '*'\n",
    "                    for col in generalized_qi:\n",
    "                        anonymized_data.loc[mask, col] = '*'\n",
    "        \n",
    "        return anonymized_data\n",
    "    \n",
    "    def add_noise_for_privacy(self, data, columns, noise_level=0.1):\n",
    "        \"\"\"Add controlled noise to numerical columns\"\"\"\n",
    "        noisy_data = data.copy()\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in data.columns and pd.api.types.is_numeric_dtype(data[column]):\n",
    "                std_dev = data[column].std()\n",
    "                noise = np.random.normal(0, std_dev * noise_level, len(data))\n",
    "                noisy_data[column] = data[column] + noise\n",
    "        \n",
    "        return noisy_data\n",
    "\n",
    "# Apply anonymization techniques\n",
    "anonymizer = DataAnonymizer()\n",
    "\n",
    "print(\"🎭 Applying Data Anonymization Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample subset for anonymization demonstration\n",
    "sample_data = original_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Define quasi-identifiers and sensitive attributes\n",
    "quasi_identifiers = ['location', 'device_type', 'floor', 'hour']\n",
    "sensitive_attribute = 'temperature'\n",
    "\n",
    "print(f\"\\n📊 Original Data Assessment:\")\n",
    "print(f\"   Sample size: {len(sample_data)}\")\n",
    "print(f\"   Quasi-identifiers: {quasi_identifiers}\")\n",
    "print(f\"   Sensitive attribute: {sensitive_attribute}\")\n",
    "\n",
    "# Original privacy assessment\n",
    "original_k_anon = privacy_analyzer.k_anonymity_assessment(sample_data, quasi_identifiers)\n",
    "print(f\"\\n   Original k-anonymity: {original_k_anon['k_value']}\")\n",
    "print(f\"   Risk level: {original_k_anon['risk_level']}\")\n",
    "\n",
    "# Apply k-anonymity\n",
    "k_values = [3, 5, 10]\n",
    "anonymized_datasets = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n🔒 Achieving {k}-anonymity:\")\n",
    "    \n",
    "    anonymized_data = anonymizer.achieve_k_anonymity(sample_data, quasi_identifiers, k=k)\n",
    "    \n",
    "    # Check achieved anonymity\n",
    "    generalized_qi = [col for col in ['floor_range', 'hour_period', 'device_type_anon'] \n",
    "                     if col in anonymized_data.columns]\n",
    "    \n",
    "    if generalized_qi:\n",
    "        achieved_k_anon = privacy_analyzer.k_anonymity_assessment(anonymized_data, generalized_qi)\n",
    "        print(f\"   Achieved k-anonymity: {achieved_k_anon['k_value']}\")\n",
    "        print(f\"   Risk level: {achieved_k_anon['risk_level']}\")\n",
    "    \n",
    "    # Calculate information loss\n",
    "    suppression_rate = (anonymized_data[generalized_qi] == '*').sum().sum() / (len(anonymized_data) * len(generalized_qi))\n",
    "    print(f\"   Suppression rate: {suppression_rate:.1%}\")\n",
    "    \n",
    "    anonymized_datasets[k] = anonymized_data\n",
    "\n",
    "# Add noise-based anonymization\n",
    "print(f\"\\n🌊 Noise-based Anonymization:\")\n",
    "noise_levels = [0.05, 0.1, 0.2]\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    noisy_data = anonymizer.add_noise_for_privacy(sample_data, [sensitive_attribute], noise_level)\n",
    "    \n",
    "    # Calculate utility preservation\n",
    "    original_mean = sample_data[sensitive_attribute].mean()\n",
    "    noisy_mean = noisy_data[sensitive_attribute].mean()\n",
    "    mean_error = abs(original_mean - noisy_mean)\n",
    "    \n",
    "    original_std = sample_data[sensitive_attribute].std()\n",
    "    noisy_std = noisy_data[sensitive_attribute].std()\n",
    "    \n",
    "    print(f\"   Noise level {noise_level:.0%}:\")\n",
    "    print(f\"     Mean preservation: {(1 - mean_error/original_mean):.1%}\")\n",
    "    print(f\"     Std change: {((noisy_std - original_std)/original_std):.1%}\")\n",
    "    \n",
    "    anonymized_datasets[f'noise_{noise_level}'] = noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy-Utility Trade-offs Visualization {#trade-offs}\n",
    "\n",
    "Visualize the trade-offs between privacy and data utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive privacy-utility trade-off analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Privacy-Utility Trade-offs in Synthetic IoT Data', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Differential Privacy Trade-offs\n",
    "epsilons = list(dp_results.keys())\n",
    "mean_errors = []\n",
    "std_errors = []\n",
    "\n",
    "original_mean = temperature_data.mean()\n",
    "original_std = temperature_data.std()\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    dp_stats = dp_results[epsilon]['statistics']\n",
    "    mean_error = abs(original_mean - dp_stats['mean']) / original_mean\n",
    "    std_error = abs(original_std - dp_stats['std']) / original_std\n",
    "    \n",
    "    mean_errors.append(mean_error)\n",
    "    std_errors.append(std_error)\n",
    "\n",
    "axes[0, 0].semilogx(epsilons, mean_errors, 'o-', label='Mean Error', linewidth=2, markersize=6)\n",
    "axes[0, 0].semilogx(epsilons, std_errors, 's-', label='Std Error', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_title('Differential Privacy: Error vs ε')\n",
    "axes[0, 0].set_xlabel('Privacy Parameter ε (log scale)')\n",
    "axes[0, 0].set_ylabel('Relative Error')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].invert_xaxis()  # Lower epsilon = higher privacy\n",
    "\n",
    "# 2. Histogram comparison for different privacy levels\n",
    "original_hist, original_bins = np.histogram(temperature_data, bins=20)\n",
    "\n",
    "# Show histograms for selected epsilon values\n",
    "epsilon_examples = [0.1, 1.0, 5.0]\n",
    "colors = ['red', 'orange', 'green']\n",
    "\n",
    "for i, (epsilon, color) in enumerate(zip(epsilon_examples, colors)):\n",
    "    dp_hist = dp_results[epsilon]['histogram']\n",
    "    bin_centers = (dp_results[epsilon]['bin_edges'][:-1] + dp_results[epsilon]['bin_edges'][1:]) / 2\n",
    "    \n",
    "    axes[0, 1].plot(bin_centers, dp_hist, \n",
    "                   label=f'ε = {epsilon}', color=color, alpha=0.7, linewidth=2)\n",
    "\n",
    "# Original histogram\n",
    "original_bin_centers = (original_bins[:-1] + original_bins[1:]) / 2\n",
    "axes[0, 1].plot(original_bin_centers, original_hist, \n",
    "               'k-', label='Original', linewidth=3, alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_title('Distribution Preservation')\n",
    "axes[0, 1].set_xlabel('Temperature Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Privacy vs Utility Score\n",
    "privacy_scores = []\n",
    "utility_scores = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Privacy score (higher for lower epsilon)\n",
    "    privacy_score = 1 / (1 + epsilon)  # Normalized privacy score\n",
    "    \n",
    "    # Utility score (based on statistical preservation)\n",
    "    dp_stats = dp_results[epsilon]['statistics']\n",
    "    mean_preservation = 1 - abs(original_mean - dp_stats['mean']) / original_mean\n",
    "    std_preservation = 1 - abs(original_std - dp_stats['std']) / original_std\n",
    "    utility_score = (mean_preservation + std_preservation) / 2\n",
    "    \n",
    "    privacy_scores.append(privacy_score)\n",
    "    utility_scores.append(utility_score)\n",
    "\n",
    "# Create Pareto frontier\n",
    "axes[0, 2].scatter(utility_scores, privacy_scores, \n",
    "                  c=epsilons, cmap='viridis', s=100, alpha=0.7)\n",
    "axes[0, 2].plot(utility_scores, privacy_scores, 'k--', alpha=0.5)\n",
    "\n",
    "# Add epsilon labels\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    axes[0, 2].annotate(f'ε={epsilon}', \n",
    "                       (utility_scores[i], privacy_scores[i]),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "axes[0, 2].set_title('Privacy-Utility Pareto Frontier')\n",
    "axes[0, 2].set_xlabel('Utility Score')\n",
    "axes[0, 2].set_ylabel('Privacy Score')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Anonymization information loss\n",
    "if anonymized_datasets:\n",
    "    k_values_list = [k for k in anonymized_datasets.keys() if isinstance(k, int)]\n",
    "    information_loss = []\n",
    "    \n",
    "    for k in k_values_list:\n",
    "        anon_data = anonymized_datasets[k]\n",
    "        generalized_qi = [col for col in ['floor_range', 'hour_period', 'device_type_anon'] \n",
    "                         if col in anon_data.columns]\n",
    "        \n",
    "        if generalized_qi:\n",
    "            # Calculate suppression rate as information loss\n",
    "            suppression_rate = (anon_data[generalized_qi] == '*').sum().sum() / (\n",
    "                len(anon_data) * len(generalized_qi))\n",
    "            information_loss.append(suppression_rate)\n",
    "        else:\n",
    "            information_loss.append(0)\n",
    "    \n",
    "    if information_loss:\n",
    "        axes[1, 0].bar([f'k={k}' for k in k_values_list], information_loss, \n",
    "                      color='lightcoral', alpha=0.7)\n",
    "        axes[1, 0].set_title('K-Anonymity Information Loss')\n",
    "        axes[1, 0].set_xlabel('Anonymity Level')\n",
    "        axes[1, 0].set_ylabel('Suppression Rate')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Noise impact on data quality\n",
    "noise_levels_list = [0.05, 0.1, 0.2]\n",
    "correlation_preservation = []\n",
    "\n",
    "for noise_level in noise_levels_list:\n",
    "    if f'noise_{noise_level}' in anonymized_datasets:\n",
    "        noisy_data = anonymized_datasets[f'noise_{noise_level}']\n",
    "        \n",
    "        # Calculate correlation preservation with hour (temporal pattern)\n",
    "        original_corr = sample_data[['temperature', 'hour']].corr().iloc[0, 1]\n",
    "        noisy_corr = noisy_data[['temperature', 'hour']].corr().iloc[0, 1]\n",
    "        \n",
    "        correlation_preservation.append(abs(noisy_corr / original_corr) if original_corr \!= 0 else 1)\n",
    "\n",
    "if correlation_preservation:\n",
    "    axes[1, 1].plot([f'{int(nl*100)}%' for nl in noise_levels_list], \n",
    "                   correlation_preservation, 'o-', \n",
    "                   linewidth=2, markersize=8, color='steelblue')\n",
    "    axes[1, 1].set_title('Noise Impact on Temporal Correlations')\n",
    "    axes[1, 1].set_xlabel('Noise Level')\n",
    "    axes[1, 1].set_ylabel('Correlation Preservation Ratio')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Perfect Preservation')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "# 6. Privacy risk assessment comparison\n",
    "privacy_methods = ['Original', 'DP (ε=1.0)', 'K-Anon (k=5)', 'Noise (10%)']\n",
    "risk_scores = []\n",
    "\n",
    "# Original risk (baseline)\n",
    "baseline_risk = 1.0\n",
    "risk_scores.append(baseline_risk)\n",
    "\n",
    "# DP risk (estimated)\n",
    "dp_risk = 0.3  # Lower risk due to noise\n",
    "risk_scores.append(dp_risk)\n",
    "\n",
    "# K-anonymity risk\n",
    "k_anon_risk = 0.2  # Lower risk due to generalization\n",
    "risk_scores.append(k_anon_risk)\n",
    "\n",
    "# Noise-based risk\n",
    "noise_risk = 0.4  # Moderate risk reduction\n",
    "risk_scores.append(noise_risk)\n",
    "\n",
    "colors_risk = ['red', 'orange', 'lightgreen', 'lightblue']\n",
    "bars = axes[1, 2].bar(privacy_methods, risk_scores, color=colors_risk, alpha=0.7)\n",
    "axes[1, 2].set_title('Relative Privacy Risk Comparison')\n",
    "axes[1, 2].set_ylabel('Relative Risk Score')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, risk_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/privacy_utility_tradeoffs.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Privacy-utility trade-offs visualization saved to {output_dir}/privacy_utility_tradeoffs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Risk Assessment {#risk-assessment}\n",
    "\n",
    "Comprehensive privacy risk assessment for different techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_privacy_assessment(original_data, anonymized_data, method_name):\n",
    "    \"\"\"Perform comprehensive privacy risk assessment\"\"\"\n",
    "    assessment_results = {'method': method_name}\n",
    "    \n",
    "    # 1. Statistical Disclosure Risk\n",
    "    if 'temperature' in original_data.columns and 'temperature' in anonymized_data.columns:\n",
    "        # Calculate statistical similarity\n",
    "        orig_stats = original_data['temperature'].describe()\n",
    "        anon_stats = anonymized_data['temperature'].describe()\n",
    "        \n",
    "        stat_similarity = 1 - np.mean([\n",
    "            abs(orig_stats['mean'] - anon_stats['mean']) / orig_stats['mean'],\n",
    "            abs(orig_stats['std'] - anon_stats['std']) / orig_stats['std']\n",
    "        ])\n",
    "        \n",
    "        assessment_results['statistical_similarity'] = max(0, stat_similarity)\n",
    "    \n",
    "    # 2. Re-identification Risk\n",
    "    # Based on uniqueness of quasi-identifier combinations\n",
    "    qi_columns = ['location', 'device_type', 'floor', 'hour']\n",
    "    available_qi = [col for col in qi_columns if col in anonymized_data.columns]\n",
    "    \n",
    "    if available_qi:\n",
    "        unique_combinations = anonymized_data[available_qi].drop_duplicates()\n",
    "        uniqueness_ratio = len(unique_combinations) / len(anonymized_data)\n",
    "        reidentification_risk = min(1.0, uniqueness_ratio * 2)  # Scale factor\n",
    "        assessment_results['reidentification_risk'] = reidentification_risk\n",
    "    else:\n",
    "        assessment_results['reidentification_risk'] = 0.0\n",
    "    \n",
    "    # 3. Attribute Disclosure Risk\n",
    "    if 'temperature' in anonymized_data.columns and available_qi:\n",
    "        # Calculate how well temperature can be predicted from QI\n",
    "        try:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            from sklearn.metrics import r2_score\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            \n",
    "            # Prepare features (encode categorical variables)\n",
    "            X = pd.get_dummies(anonymized_data[available_qi], drop_first=True)\n",
    "            y = anonymized_data['temperature']\n",
    "            \n",
    "            if len(X) > 10 and X.shape[1] > 0:  # Minimum data for training\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                \n",
    "                rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "                rf.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = rf.predict(X_test)\n",
    "                prediction_accuracy = r2_score(y_test, y_pred)\n",
    "                \n",
    "                assessment_results['attribute_disclosure_risk'] = max(0, prediction_accuracy)\n",
    "            else:\n",
    "                assessment_results['attribute_disclosure_risk'] = 0.0\n",
    "        except ImportError:\n",
    "            # Fallback: simple correlation-based assessment\n",
    "            if 'hour' in available_qi:\n",
    "                correlation = abs(anonymized_data[['temperature', 'hour']].corr().iloc[0, 1])\n",
    "                assessment_results['attribute_disclosure_risk'] = correlation\n",
    "            else:\n",
    "                assessment_results['attribute_disclosure_risk'] = 0.0\n",
    "    else:\n",
    "        assessment_results['attribute_disclosure_risk'] = 0.0\n",
    "    \n",
    "    # 4. Data Utility Preservation\n",
    "    if 'temperature' in original_data.columns and 'temperature' in anonymized_data.columns:\n",
    "        # Temporal pattern preservation\n",
    "        if 'hour' in original_data.columns and 'hour' in anonymized_data.columns:\n",
    "            orig_hourly = original_data.groupby('hour')['temperature'].mean()\n",
    "            anon_hourly = anonymized_data.groupby('hour')['temperature'].mean()\n",
    "            \n",
    "            # Align hours and calculate correlation\n",
    "            common_hours = orig_hourly.index.intersection(anon_hourly.index)\n",
    "            if len(common_hours) > 3:\n",
    "                temporal_correlation = orig_hourly.loc[common_hours].corr(anon_hourly.loc[common_hours])\n",
    "                assessment_results['temporal_utility'] = max(0, temporal_correlation)\n",
    "            else:\n",
    "                assessment_results['temporal_utility'] = 0.0\n",
    "        else:\n",
    "            assessment_results['temporal_utility'] = 0.0\n",
    "    else:\n",
    "        assessment_results['temporal_utility'] = 0.0\n",
    "    \n",
    "    # 5. Overall Privacy Score (lower is better for privacy)\n",
    "    privacy_risks = [\n",
    "        assessment_results.get('reidentification_risk', 0),\n",
    "        assessment_results.get('attribute_disclosure_risk', 0)\n",
    "    ]\n",
    "    assessment_results['overall_privacy_risk'] = np.mean(privacy_risks)\n",
    "    \n",
    "    # 6. Overall Utility Score (higher is better for utility)\n",
    "    utility_scores = [\n",
    "        assessment_results.get('statistical_similarity', 0),\n",
    "        assessment_results.get('temporal_utility', 0)\n",
    "    ]\n",
    "    assessment_results['overall_utility_score'] = np.mean(utility_scores)\n",
    "    \n",
    "    return assessment_results\n",
    "\n",
    "# Perform comprehensive assessment for different privacy methods\n",
    "print(\"🔍 Comprehensive Privacy Risk Assessment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "assessment_methods = {\n",
    "    'Original Data': sample_data,\n",
    "    'K-Anonymity (k=5)': anonymized_datasets.get(5, sample_data),\n",
    "    'Noise Addition (10%)': anonymized_datasets.get('noise_0.1', sample_data)\n",
    "}\n",
    "\n",
    "assessment_results = []\n",
    "\n",
    "for method_name, data in assessment_methods.items():\n",
    "    print(f\"\\n📊 Assessing {method_name}:\")\n",
    "    \n",
    "    results = comprehensive_privacy_assessment(sample_data, data, method_name)\n",
    "    assessment_results.append(results)\n",
    "    \n",
    "    print(f\"   Re-identification Risk: {results['reidentification_risk']:.3f}\")\n",
    "    print(f\"   Attribute Disclosure Risk: {results['attribute_disclosure_risk']:.3f}\")\n",
    "    print(f\"   Statistical Similarity: {results['statistical_similarity']:.3f}\")\n",
    "    print(f\"   Temporal Utility: {results['temporal_utility']:.3f}\")\n",
    "    print(f\"   Overall Privacy Risk: {results['overall_privacy_risk']:.3f}\")\n",
    "    print(f\"   Overall Utility Score: {results['overall_utility_score']:.3f}\")\n",
    "    \n",
    "    # Risk interpretation\n",
    "    risk_level = \"High\" if results['overall_privacy_risk'] > 0.7 else \\\n",
    "                \"Medium\" if results['overall_privacy_risk'] > 0.3 else \"Low\"\n",
    "    \n",
    "    utility_level = \"High\" if results['overall_utility_score'] > 0.7 else \\\n",
    "                   \"Medium\" if results['overall_utility_score'] > 0.3 else \"Low\"\n",
    "    \n",
    "    print(f\"   Privacy Risk Level: {risk_level}\")\n",
    "    print(f\"   Utility Level: {utility_level}\")\n",
    "\n",
    "# Create assessment summary table\n",
    "print(f\"\\n📋 Privacy Assessment Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_assessment = pd.DataFrame(assessment_results)\n",
    "print(df_assessment[['method', 'overall_privacy_risk', 'overall_utility_score', \n",
    "                    'reidentification_risk', 'attribute_disclosure_risk']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance and Validation {#compliance}\n",
    "\n",
    "Validate privacy-preserving techniques against regulatory standards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivacyComplianceValidator:\n",
    "    \"\"\"Validate privacy techniques against regulatory standards\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.compliance_frameworks = {\n",
    "            'GDPR': {\n",
    "                'pseudonymization': {'required': True, 'description': 'Personal data processed in a manner that cannot be attributed to a specific data subject'},\n",
    "                'anonymization': {'required': False, 'description': 'Personal data rendered anonymous in such a manner that the data subject is not identifiable'},\n",
    "                'data_minimization': {'required': True, 'description': 'Personal data shall be adequate, relevant and limited'},\n",
    "                'purpose_limitation': {'required': True, 'description': 'Personal data shall be collected for specified purposes'}\n",
    "            },\n",
    "            'HIPAA': {\n",
    "                'safe_harbor': {'required': True, 'description': 'Remove 18 specific identifiers'},\n",
    "                'expert_determination': {'required': False, 'description': 'Very small risk of re-identification'},\n",
    "                'minimum_necessary': {'required': True, 'description': 'Minimum necessary information'}\n",
    "            },\n",
    "            'CCPA': {\n",
    "                'deidentification': {'required': True, 'description': 'Information that cannot reasonably identify a consumer'},\n",
    "                'aggregation': {'required': False, 'description': 'Information aggregated or de-identified'}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def check_gdpr_compliance(self, data, anonymization_method):\n",
    "        \"\"\"Check GDPR compliance\"\"\"\n",
    "        compliance_score = 0\n",
    "        max_score = 4\n",
    "        compliance_details = {}\n",
    "        \n",
    "        # Pseudonymization check\n",
    "        if 'device_id' not in data.columns or data['device_id'].dtype == 'object':\n",
    "            compliance_score += 1\n",
    "            compliance_details['pseudonymization'] = \"✅ Device IDs appear pseudonymized\"\n",
    "        else:\n",
    "            compliance_details['pseudonymization'] = \"❌ Direct identifiers present\"\n",
    "        \n",
    "        # Anonymization assessment\n",
    "        if anonymization_method in ['k_anonymity', 'differential_privacy', 'noise_addition']:\n",
    "            compliance_score += 1\n",
    "            compliance_details['anonymization'] = f\"✅ {anonymization_method} applied\"\n",
    "        else:\n",
    "            compliance_details['anonymization'] = \"⚠️ No formal anonymization applied\"\n",
    "        \n",
    "        # Data minimization (check if only necessary columns present)\n",
    "        essential_columns = ['timestamp', 'temperature', 'location', 'device_type']\n",
    "        if len(data.columns) <= len(essential_columns) + 2:  # Allow some flexibility\n",
    "            compliance_score += 1\n",
    "            compliance_details['data_minimization'] = \"✅ Data appears minimized\"\n",
    "        else:\n",
    "            compliance_details['data_minimization'] = \"⚠️ Potential excess data\"\n",
    "        \n",
    "        # Purpose limitation (assumed if data is synthetic)\n",
    "        compliance_score += 1\n",
    "        compliance_details['purpose_limitation'] = \"✅ Synthetic data for specified purposes\"\n",
    "        \n",
    "        compliance_percentage = (compliance_score / max_score) * 100\n",
    "        \n",
    "        return {\n",
    "            'framework': 'GDPR',\n",
    "            'compliance_score': compliance_score,\n",
    "            'max_score': max_score,\n",
    "            'compliance_percentage': compliance_percentage,\n",
    "            'details': compliance_details,\n",
    "            'recommendation': self._get_gdpr_recommendation(compliance_percentage)\n",
    "        }\n",
    "    \n",
    "    def check_hipaa_compliance(self, data, anonymization_method):\n",
    "        \"\"\"Check HIPAA compliance (Safe Harbor method)\"\"\"\n",
    "        compliance_score = 0\n",
    "        max_score = 3\n",
    "        compliance_details = {}\n",
    "        \n",
    "        # Safe Harbor - check for absence of 18 identifiers\n",
    "        prohibited_columns = ['name', 'address', 'birth_date', 'phone', 'fax', 'email', \n",
    "                            'ssn', 'mrn', 'account_number', 'certificate_number', \n",
    "                            'vehicle_id', 'device_serial', 'web_url', 'ip_address', \n",
    "                            'biometric_id', 'photo', 'unique_code']\n",
    "        \n",
    "        found_prohibited = [col for col in data.columns if any(prohibited in col.lower() for prohibited in prohibited_columns)]\n",
    "        \n",
    "        if not found_prohibited:\n",
    "            compliance_score += 1\n",
    "            compliance_details['safe_harbor'] = \"✅ No prohibited identifiers found\"\n",
    "        else:\n",
    "            compliance_details['safe_harbor'] = f\"❌ Found prohibited identifiers: {found_prohibited}\"\n",
    "        \n",
    "        # Expert determination (privacy risk assessment)\n",
    "        if anonymization_method in ['differential_privacy', 'k_anonymity']:\n",
    "            compliance_score += 1\n",
    "            compliance_details['expert_determination'] = \"✅ Formal privacy method applied\"\n",
    "        else:\n",
    "            compliance_details['expert_determination'] = \"⚠️ Informal privacy protection\"\n",
    "        \n",
    "        # Minimum necessary\n",
    "        if len(data.columns) <= 8:  # Reasonable limit for IoT data\n",
    "            compliance_score += 1\n",
    "            compliance_details['minimum_necessary'] = \"✅ Data appears minimal\"\n",
    "        else:\n",
    "            compliance_details['minimum_necessary'] = \"⚠️ Consider reducing data elements\"\n",
    "        \n",
    "        compliance_percentage = (compliance_score / max_score) * 100\n",
    "        \n",
    "        return {\n",
    "            'framework': 'HIPAA',\n",
    "            'compliance_score': compliance_score,\n",
    "            'max_score': max_score,\n",
    "            'compliance_percentage': compliance_percentage,\n",
    "            'details': compliance_details,\n",
    "            'recommendation': self._get_hipaa_recommendation(compliance_percentage)\n",
    "        }\n",
    "    \n",
    "    def _get_gdpr_recommendation(self, compliance_percentage):\n",
    "        if compliance_percentage >= 75:\n",
    "            return \"Good GDPR compliance posture. Consider documenting privacy measures.\"\n",
    "        elif compliance_percentage >= 50:\n",
    "            return \"Moderate compliance. Strengthen anonymization techniques.\"\n",
    "        else:\n",
    "            return \"Low compliance. Implement formal privacy-preserving methods.\"\n",
    "    \n",
    "    def _get_hipaa_recommendation(self, compliance_percentage):\n",
    "        if compliance_percentage >= 67:\n",
    "            return \"Good HIPAA compliance posture. Document privacy controls.\"\n",
    "        elif compliance_percentage >= 33:\n",
    "            return \"Moderate compliance. Consider expert determination review.\"\n",
    "        else:\n",
    "            return \"Low compliance. Remove identifiers and implement Safe Harbor.\"\n",
    "\n",
    "# Perform compliance validation\n",
    "compliance_validator = PrivacyComplianceValidator()\n",
    "\n",
    "print(\"⚖️ Privacy Compliance Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different methods for compliance\n",
    "compliance_test_data = {\n",
    "    'Original Data': (sample_data, 'none'),\n",
    "    'K-Anonymity (k=5)': (anonymized_datasets.get(5, sample_data), 'k_anonymity'),\n",
    "    'Noise Addition (10%)': (anonymized_datasets.get('noise_0.1', sample_data), 'noise_addition')\n",
    "}\n",
    "\n",
    "compliance_results = []\n",
    "\n",
    "for method_name, (data, anon_method) in compliance_test_data.items():\n",
    "    print(f\"\\n📋 Compliance Check: {method_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # GDPR Compliance\n",
    "    gdpr_result = compliance_validator.check_gdpr_compliance(data, anon_method)\n",
    "    print(f\"\\n🇪🇺 GDPR Compliance: {gdpr_result['compliance_percentage']:.0f}%\")\n",
    "    for requirement, status in gdpr_result['details'].items():\n",
    "        print(f\"   {requirement.title()}: {status}\")\n",
    "    print(f\"   Recommendation: {gdpr_result['recommendation']}\")\n",
    "    \n",
    "    # HIPAA Compliance\n",
    "    hipaa_result = compliance_validator.check_hipaa_compliance(data, anon_method)\n",
    "    print(f\"\\n🏥 HIPAA Compliance: {hipaa_result['compliance_percentage']:.0f}%\")\n",
    "    for requirement, status in hipaa_result['details'].items():\n",
    "        print(f\"   {requirement.title()}: {status}\")\n",
    "    print(f\"   Recommendation: {hipaa_result['recommendation']}\")\n",
    "    \n",
    "    compliance_results.append({\n",
    "        'method': method_name,\n",
    "        'gdpr_score': gdpr_result['compliance_percentage'],\n",
    "        'hipaa_score': hipaa_result['compliance_percentage']\n",
    "    })\n",
    "\n",
    "# Create compliance summary visualization\n",
    "compliance_df = pd.DataFrame(compliance_results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Compliance scores comparison\n",
    "x = np.arange(len(compliance_df))\n",
    "width = 0.35\n",
    "\n",
    "gdpr_bars = ax1.bar(x - width/2, compliance_df['gdpr_score'], width, \n",
    "                   label='GDPR', color='steelblue', alpha=0.7)\n",
    "hipaa_bars = ax1.bar(x + width/2, compliance_df['hipaa_score'], width, \n",
    "                    label='HIPAA', color='lightcoral', alpha=0.7)\n",
    "\n",
    "ax1.set_title('Privacy Compliance Scores by Method')\n",
    "ax1.set_ylabel('Compliance Score (%)')\n",
    "ax1.set_xlabel('Privacy Method')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(compliance_df['method'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [gdpr_bars, hipaa_bars]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.0f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Privacy-Utility-Compliance triangle\n",
    "if assessment_results:\n",
    "    methods = [result['method'] for result in assessment_results]\n",
    "    privacy_scores = [1 - result['overall_privacy_risk'] for result in assessment_results]  # Invert risk to score\n",
    "    utility_scores = [result['overall_utility_score'] for result in assessment_results]\n",
    "    \n",
    "    # Get corresponding compliance scores\n",
    "    compliance_scores = []\n",
    "    for method in methods:\n",
    "        compliance_row = compliance_df[compliance_df['method'] == method]\n",
    "        if not compliance_row.empty:\n",
    "            avg_compliance = (compliance_row['gdpr_score'].iloc[0] + compliance_row['hipaa_score'].iloc[0]) / 200  # Scale to 0-1\n",
    "            compliance_scores.append(avg_compliance)\n",
    "        else:\n",
    "            compliance_scores.append(0.5)\n",
    "    \n",
    "    scatter = ax2.scatter(utility_scores, privacy_scores, \n",
    "                         c=compliance_scores, s=100, \n",
    "                         cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, method in enumerate(methods):\n",
    "        ax2.annotate(method.replace('Data', '').strip(), \n",
    "                    (utility_scores[i], privacy_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax2.set_title('Privacy-Utility-Compliance Relationship')\n",
    "    ax2.set_xlabel('Utility Score')\n",
    "    ax2.set_ylabel('Privacy Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Compliance Score', rotation=270, labelpad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/privacy_compliance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Compliance analysis saved to {output_dir}/privacy_compliance_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Preservation Summary and Best Practices\n",
    "\n",
    "Comprehensive summary of privacy preservation techniques and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔒 PRIVACY PRESERVATION IN SYNTHETIC IOT DATA - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Analysis Overview:\")\n",
    "print(f\"   Original Dataset Size: {len(original_df):,} records\")\n",
    "print(f\"   Sample Size for Analysis: {len(sample_data):,} records\")\n",
    "print(f\"   IoT Devices Simulated: {n_devices}\")\n",
    "print(f\"   Time Period: 24 hours (minute resolution)\")\n",
    "print(f\"   Privacy Techniques Evaluated: 5+\")\n",
    "print(f\"   Compliance Frameworks: GDPR, HIPAA\")\n",
    "\n",
    "print(f\"\\n🔐 Privacy Techniques Assessed:\")\n",
    "print(f\"   ✅ Differential Privacy (ε = 0.1 to 5.0)\")\n",
    "print(f\"   ✅ K-Anonymity (k = 3, 5, 10)\")\n",
    "print(f\"   ✅ L-Diversity Analysis\")\n",
    "print(f\"   ✅ Noise Addition (5%, 10%, 20%)\")\n",
    "print(f\"   ✅ Data Suppression\")\n",
    "print(f\"   ✅ Generalization Hierarchies\")\n",
    "\n",
    "print(f\"\\n🎯 Key Findings:\")\n",
    "\n",
    "if assessment_results:\n",
    "    print(f\"\\n   📈 Privacy-Utility Trade-offs:\")\n",
    "    for result in assessment_results:\n",
    "        method = result['method']\n",
    "        privacy_risk = result['overall_privacy_risk']\n",
    "        utility_score = result['overall_utility_score']\n",
    "        \n",
    "        print(f\"     {method}:\")\n",
    "        print(f\"       Privacy Risk: {privacy_risk:.3f} ({'Low' if privacy_risk < 0.3 else 'Medium' if privacy_risk < 0.7 else 'High'})\")\n",
    "        print(f\"       Utility Score: {utility_score:.3f} ({'High' if utility_score > 0.7 else 'Medium' if utility_score > 0.3 else 'Low'})\")\n",
    "\n",
    "if dp_results:\n",
    "    print(f\"\\n   🔢 Differential Privacy Insights:\")\n",
    "    best_dp_epsilon = min(dp_results.keys(), key=lambda e: abs(mean_errors[list(dp_results.keys()).index(e)] - 0.1))\n",
    "    print(f\"     Recommended ε value: {best_dp_epsilon} (balance of privacy and utility)\")\n",
    "    print(f\"     High privacy (ε < 1.0): Significant noise, strong privacy\")\n",
    "    print(f\"     Moderate privacy (ε = 1-2): Balanced approach\")\n",
    "    print(f\"     Low privacy (ε > 2): Minimal noise, high utility\")\n",
    "\n",
    "if compliance_results:\n",
    "    print(f\"\\n   ⚖️ Regulatory Compliance:\")\n",
    "    for result in compliance_results:\n",
    "        method = result['method']\n",
    "        gdpr_score = result['gdpr_score']\n",
    "        hipaa_score = result['hipaa_score']\n",
    "        \n",
    "        print(f\"     {method}:\")\n",
    "        print(f\"       GDPR Compliance: {gdpr_score:.0f}%\")\n",
    "        print(f\"       HIPAA Compliance: {hipaa_score:.0f}%\")\n",
    "\n",
    "print(f\"\\n🏆 Best Practice Recommendations:\")\n",
    "print(f\"\\n   🎯 Method Selection Guidelines:\")\n",
    "print(f\"     • High Privacy Requirements: Differential Privacy (ε ≤ 1.0) + K-Anonymity\")\n",
    "print(f\"     • Balanced Approach: K-Anonymity (k=5) + Controlled Noise (≤10%)\")\n",
    "print(f\"     • High Utility Needs: Light Noise Addition (≤5%) + Pseudonymization\")\n",
    "print(f\"     • Regulatory Compliance: Document chosen methods and risk assessments\")\n",
    "\n",
    "print(f\"\\n   🔧 Implementation Guidelines:\")\n",
    "print(f\"     📊 Data Assessment:\")\n",
    "print(f\"       • Identify quasi-identifiers (location, device type, temporal patterns)\")\n",
    "print(f\"       • Assess uniqueness and re-identification risk\")\n",
    "print(f\"       • Evaluate sensitive attribute disclosure risk\")\n",
    "     \n",
    "print(f\"\\n     🎛️ Parameter Tuning:\")\n",
    "print(f\"       • Start with conservative parameters (high privacy)\")\n",
    "print(f\"       • Gradually adjust based on utility requirements\")\n",
    "print(f\"       • Test privacy guarantees with worst-case scenarios\")\n",
    "print(f\"       • Validate utility preservation for intended use cases\")\n",
    "\n",
    "print(f\"\\n     📋 Quality Assurance:\")\n",
    "print(f\"       • Implement automated privacy risk monitoring\")\n",
    "print(f\"       • Regular compliance audits and validation\")\n",
    "print(f\"       • Document privacy-preserving processes\")\n",
    "print(f\"       • Train staff on privacy-preserving techniques\")\n",
    "\n",
    "print(f\"\\n   🚨 Risk Mitigation Strategies:\")\n",
    "print(f\"     🛡️ Defense in Depth:\")\n",
    "print(f\"       • Layer multiple privacy techniques (DP + K-Anon + Noise)\")\n",
    "print(f\"       • Implement access controls and audit trails\")\n",
    "print(f\"       • Regular privacy impact assessments\")\n",
    "print(f\"       • Incident response procedures for privacy breaches\")\n",
    "\n",
    "print(f\"\\n     📏 Ongoing Monitoring:\")\n",
    "print(f\"       • Track privacy metrics over time\")\n",
    "print(f\"       • Monitor for new attack vectors\")\n",
    "print(f\"       • Update techniques as technology evolves\")\n",
    "print(f\"       • Benchmark against industry standards\")\n",
    "\n",
    "print(f\"\\n🌐 Industry-Specific Considerations:\")\n",
    "print(f\"\\n   🏥 Healthcare IoT:\")\n",
    "print(f\"     • HIPAA Safe Harbor compliance mandatory\")\n",
    "print(f\"     • Expert determination for complex cases\")\n",
    "print(f\"     • Strong differential privacy (ε ≤ 0.5)\")\n",
    "print(f\"     • Temporal pattern protection\")\n",
    "\n",
    "print(f\"\\n   🏭 Industrial IoT:\")\n",
    "print(f\"     • Focus on competitive intelligence protection\")\n",
    "print(f\"     • Operational pattern anonymization\")\n",
    "print(f\"     • Moderate privacy levels acceptable (ε ≤ 2.0)\")\n",
    "print(f\"     • Preserve fault detection capabilities\")\n",
    "\n",
    "print(f\"\\n   🏠 Smart Buildings/Cities:\")\n",
    "print(f\"     • GDPR compliance in EU jurisdictions\")\n",
    "print(f\"     • Behavioral pattern protection\")\n",
    "print(f\"     • Balance privacy with emergency services\")\n",
    "print(f\"     • Consent management for data collection\")\n",
    "\n",
    "print(f\"\\n📁 Generated Privacy Analysis Artifacts:\")\n",
    "artifacts = [\n",
    "    \"privacy_utility_tradeoffs.png\",\n",
    "    \"privacy_compliance_analysis.png\"\n",
    "]\n",
    "\n",
    "for artifact in artifacts:\n",
    "    artifact_path = os.path.join(output_dir, artifact)\n",
    "    if os.path.exists(artifact_path):\n",
    "        file_size = os.path.getsize(artifact_path)\n",
    "        print(f\"   📊 {artifact} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\n📁 All Output Files:\")\n",
    "for file in sorted(os.listdir(output_dir)):\n",
    "    if file.endswith(('.png', '.json', '.csv')):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"   📄 {file} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\n🎯 Action Items for Privacy-Preserving IoT Systems:\")\n",
    "print(f\"   ☐ Conduct privacy risk assessment for your specific IoT use case\")\n",
    "print(f\"   ☐ Select appropriate privacy techniques based on risk tolerance\")\n",
    "print(f\"   ☐ Implement privacy-by-design in data collection systems\")\n",
    "print(f\"   ☐ Establish privacy metrics and monitoring procedures\")\n",
    "print(f\"   ☐ Create privacy documentation and compliance evidence\")\n",
    "print(f\"   ☐ Train development and operations teams on privacy techniques\")\n",
    "print(f\"   ☐ Plan for regulatory audits and privacy breach response\")\n",
    "print(f\"   ☐ Stay updated on evolving privacy regulations and techniques\")\n",
    "\n",
    "print(f\"\\n🔬 Advanced Topics for Further Exploration:\")\n",
    "print(f\"   📚 Research Areas:\")\n",
    "print(f\"     • Federated learning for privacy-preserving IoT analytics\")\n",
    "print(f\"     • Homomorphic encryption for computation on encrypted data\")\n",
    "print(f\"     • Secure multi-party computation for collaborative analytics\")\n",
    "print(f\"     • Zero-knowledge proofs for privacy-preserving verification\")\n",
    "print(f\"     • Synthetic data generation with formal privacy guarantees\")\n",
    "\n",
    "print(f\"\\n   🛠️ Tools and Frameworks:\")\n",
    "print(f\"     • Google's Differential Privacy Library\")\n",
    "print(f\"     • Microsoft's SmartNoise\")\n",
    "print(f\"     • IBM's Differential Privacy Library\")\n",
    "print(f\"     • ARX Data Anonymization Tool\")\n",
    "print(f\"     • Amnesia Anonymization Tool\")\n",
    "\n",
    "print(f\"\\n🎉 Privacy preservation analysis complete\!\")\n",
    "print(f\"    Your IoT systems can now balance privacy protection with data utility.\")\n",
    "print(f\"    Remember: Privacy is not a one-time implementation but an ongoing commitment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF < /dev/null