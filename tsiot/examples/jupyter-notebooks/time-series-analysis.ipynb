{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Time Series Analysis with TSIoT\n",
    "\n",
    "This notebook demonstrates advanced time series analysis techniques using TSIoT-generated synthetic data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Time Series Decomposition](#decomposition)\n",
    "2. [Seasonality Detection](#seasonality)\n",
    "3. [Anomaly Detection](#anomalies)\n",
    "4. [Trend Analysis](#trends)\n",
    "5. [Forecasting](#forecasting)\n",
    "6. [Multi-variate Analysis](#multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "output_dir = \"ts_analysis_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Time series analysis environment ready\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data for Analysis\n",
    "\n",
    "Let's generate some complex time series data with multiple patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex sensor data with seasonality\n",
    "sensors = ['temperature', 'humidity', 'pressure', 'co2']\n",
    "datasets = {}\n",
    "\n",
    "for sensor in sensors:\n",
    "    filename = f\"{output_dir}/{sensor}_weekly_data.json\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"tsiot\", \"generate\",\n",
    "        \"--generator\", \"statistical\",\n",
    "        \"--sensor-type\", sensor,\n",
    "        \"--count\", \"2016\",  # 1 week of 5-minute data\n",
    "        \"--frequency\", \"5m\",\n",
    "        \"--start-time\", \"2023-01-01T00:00:00Z\",\n",
    "        \"--output\", filename\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"‚úÖ Generated {sensor} data\")\n",
    "        \n",
    "        # Load the data\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data.get('points', []))\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "        datasets[sensor] = df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {sensor}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(datasets)} sensor datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Decomposition {#decomposition}\n",
    "\n",
    "Decompose time series into trend, seasonal, and residual components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition on temperature data\n",
    "if 'temperature' in datasets:\n",
    "    temp_data = datasets['temperature']['value']\n",
    "    \n",
    "    # Perform decomposition with daily period (288 points per day at 5-min intervals)\n",
    "    decomposition = seasonal_decompose(temp_data, model='additive', period=288)\n",
    "    \n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    decomposition.observed.plot(ax=axes[0], title='Original Time Series')\n",
    "    decomposition.trend.plot(ax=axes[1], title='Trend Component')\n",
    "    decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component')\n",
    "    decomposition.resid.plot(ax=axes[3], title='Residual Component')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/decomposition_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis of components\n",
    "    print(\"üìä Decomposition Statistics:\")\n",
    "    print(f\"   Trend variance: {decomposition.trend.var():.4f}\")\n",
    "    print(f\"   Seasonal variance: {decomposition.seasonal.var():.4f}\")\n",
    "    print(f\"   Residual variance: {decomposition.resid.var():.4f}\")\n",
    "    \n",
    "    # Strength of seasonality\n",
    "    seasonal_strength = 1 - (decomposition.resid.var() / (decomposition.seasonal + decomposition.resid).var())\n",
    "    print(f\"   Seasonal strength: {seasonal_strength:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality Detection {#seasonality}\n",
    "\n",
    "Analyze different types of seasonality patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_seasonality(data, max_period=500):\n",
    "    \"\"\"Detect seasonality using FFT and autocorrelation\"\"\"\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    \n",
    "    # FFT analysis\n",
    "    n = len(data)\n",
    "    fft_vals = fft(data.values)\n",
    "    freqs = fftfreq(n)\n",
    "    \n",
    "    # Find dominant frequencies\n",
    "    power = np.abs(fft_vals) ** 2\n",
    "    dominant_freq_idx = np.argsort(power)[-10:]  # Top 10 frequencies\n",
    "    \n",
    "    periods = []\n",
    "    for idx in dominant_freq_idx:\n",
    "        if freqs[idx] \!= 0:  # Avoid division by zero\n",
    "            period = int(1 / abs(freqs[idx]))\n",
    "            if 2 < period < max_period:  # Reasonable period range\n",
    "                periods.append(period)\n",
    "    \n",
    "    # Autocorrelation analysis\n",
    "    autocorr = [data.autocorr(lag=lag) for lag in range(1, min(max_period, len(data)//2))]\n",
    "    \n",
    "    return {\n",
    "        'periods': sorted(set(periods))[:5],  # Top 5 unique periods\n",
    "        'max_autocorr': max(autocorr) if autocorr else 0,\n",
    "        'autocorr_lag': np.argmax(autocorr) + 1 if autocorr else 0\n",
    "    }\n",
    "\n",
    "# Analyze seasonality for all sensors\n",
    "seasonality_results = {}\n",
    "for sensor, data in datasets.items():\n",
    "    if len(data) > 100:  # Ensure sufficient data\n",
    "        result = detect_seasonality(data['value'])\n",
    "        seasonality_results[sensor] = result\n",
    "        \n",
    "        print(f\"\\nüìà {sensor.capitalize()} Seasonality:\")\n",
    "        print(f\"   Detected periods: {result['periods']}\")\n",
    "        print(f\"   Max autocorrelation: {result['max_autocorr']:.4f}\")\n",
    "        print(f\"   at lag: {result['autocorr_lag']}\")\n",
    "        \n",
    "        # Interpret periods\n",
    "        for period in result['periods'][:3]:\n",
    "            hours = period * 5 / 60  # Convert 5-min intervals to hours\n",
    "            if 0.8 <= hours <= 1.2:\n",
    "                print(f\"   ‚Üí {period} points ‚âà Hourly pattern\")\n",
    "            elif 11 <= hours <= 13:\n",
    "                print(f\"   ‚Üí {period} points ‚âà Half-daily pattern\")\n",
    "            elif 22 <= hours <= 26:\n",
    "                print(f\"   ‚Üí {period} points ‚âà Daily pattern\")\n",
    "            elif 160 <= hours <= 200:\n",
    "                print(f\"   ‚Üí {period} points ‚âà Weekly pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection {#anomalies}\n",
    "\n",
    "Identify anomalous patterns in the time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(data, method='isolation_forest'):\n",
    "    \"\"\"Detect anomalies using multiple methods\"\"\"\n",
    "    anomalies = {}\n",
    "    \n",
    "    # Statistical outliers (Z-score)\n",
    "    z_scores = np.abs(stats.zscore(data.dropna()))\n",
    "    z_outliers = z_scores > 3\n",
    "    anomalies['z_score'] = z_outliers\n",
    "    \n",
    "    # IQR method\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = (data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))\n",
    "    anomalies['iqr'] = iqr_outliers\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    iso_predictions = iso_forest.fit_predict(data.values.reshape(-1, 1))\n",
    "    anomalies['isolation_forest'] = iso_predictions == -1\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Detect anomalies in temperature data\n",
    "if 'temperature' in datasets:\n",
    "    temp_data = datasets['temperature']['value']\n",
    "    anomalies = detect_anomalies(temp_data)\n",
    "    \n",
    "    # Visualize anomalies\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    methods = ['z_score', 'iqr', 'isolation_forest']\n",
    "    colors = ['red', 'orange', 'purple']\n",
    "    \n",
    "    for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "        # Plot time series\n",
    "        axes[i].plot(temp_data.index, temp_data.values, 'b-', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        # Highlight anomalies\n",
    "        anomaly_points = temp_data[anomalies[method]]\n",
    "        if len(anomaly_points) > 0:\n",
    "            axes[i].scatter(anomaly_points.index, anomaly_points.values, \n",
    "                          c=color, s=50, alpha=0.8, label=f'Anomalies ({len(anomaly_points)})')\n",
    "        \n",
    "        axes[i].set_title(f'Anomaly Detection: {method.replace(\"_\", \" \").title()}')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/anomaly_detection.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print anomaly statistics\n",
    "    print(\"üö® Anomaly Detection Results:\")\n",
    "    for method in methods:\n",
    "        count = sum(anomalies[method])\n",
    "        percentage = count / len(temp_data) * 100\n",
    "        print(f\"   {method.replace('_', ' ').title()}: {count} anomalies ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Analysis {#multivariate}\n",
    "\n",
    "Analyze relationships between multiple sensor streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all sensor data for correlation analysis\n",
    "if len(datasets) > 1:\n",
    "    # Align all datasets by timestamp\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    for sensor, data in datasets.items():\n",
    "        combined_df[sensor] = data['value']\n",
    "    \n",
    "    # Remove any rows with missing data\n",
    "    combined_df = combined_df.dropna()\n",
    "    \n",
    "    print(f\"üìä Combined dataset shape: {combined_df.shape}\")\n",
    "    \n",
    "    # Correlation matrix\n",
    "    correlation_matrix = combined_df.corr()\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.3f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    plt.title('Sensor Cross-Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/correlation_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strong correlations\n",
    "    print(\"\\nüîó Strong Correlations (|r| > 0.5):\")\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr) > 0.5:\n",
    "                sensor1 = correlation_matrix.columns[i]\n",
    "                sensor2 = correlation_matrix.columns[j]\n",
    "                print(f\"   {sensor1} ‚Üî {sensor2}: {corr:.3f}\")\n",
    "    \n",
    "    # Time-lagged correlations\n",
    "    print(\"\\n‚è±Ô∏è  Time-lagged Correlations:\")\n",
    "    if 'temperature' in combined_df.columns and 'humidity' in combined_df.columns:\n",
    "        temp_series = combined_df['temperature']\n",
    "        humid_series = combined_df['humidity']\n",
    "        \n",
    "        max_lag = min(50, len(temp_series) // 4)  # Up to 50 time steps or 1/4 of data\n",
    "        lagged_corrs = []\n",
    "        \n",
    "        for lag in range(-max_lag, max_lag + 1):\n",
    "            if lag == 0:\n",
    "                corr = temp_series.corr(humid_series)\n",
    "            elif lag > 0:\n",
    "                corr = temp_series[:-lag].corr(humid_series[lag:])\n",
    "            else:\n",
    "                corr = temp_series[-lag:].corr(humid_series[:lag])\n",
    "            \n",
    "            lagged_corrs.append((lag, corr))\n",
    "        \n",
    "        # Find best correlation\n",
    "        best_lag, best_corr = max(lagged_corrs, key=lambda x: abs(x[1]))\n",
    "        print(f\"   Temperature-Humidity best correlation: {best_corr:.3f} at lag {best_lag}\")\n",
    "        \n",
    "        # Plot lagged correlations\n",
    "        lags, corrs = zip(*lagged_corrs)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(lags, corrs, 'b-', linewidth=2)\n",
    "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        plt.axvline(x=best_lag, color='r', linestyle='--', alpha=0.7, \n",
    "                   label=f'Best correlation at lag {best_lag}')\n",
    "        plt.xlabel('Lag (time steps)')\n",
    "        plt.ylabel('Correlation Coefficient')\n",
    "        plt.title('Temperature-Humidity Lagged Correlation')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/lagged_correlation.png', dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Advanced Insights\n",
    "\n",
    "Comprehensive summary of time series analysis findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Advanced Time Series Analysis Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Sensors analyzed: {list(datasets.keys())}\")\n",
    "print(f\"   Total data points per sensor: ~{len(list(datasets.values())[0]) if datasets else 0}\")\n",
    "print(f\"   Time resolution: 5-minute intervals\")\n",
    "print(f\"   Coverage: ~1 week of data\")\n",
    "\n",
    "if 'seasonality_results' in locals():\n",
    "    print(f\"\\nüîÑ Seasonality Findings:\")\n",
    "    for sensor, results in seasonality_results.items():\n",
    "        print(f\"   {sensor.capitalize()}:\")\n",
    "        print(f\"     Detected periods: {results['periods'][:3]}\")\n",
    "        print(f\"     Strongest pattern: {results['autocorr_lag']} steps\")\n",
    "\n",
    "if 'anomalies' in locals():\n",
    "    print(f\"\\nüö® Anomaly Detection:\")\n",
    "    for method in ['z_score', 'iqr', 'isolation_forest']:\n",
    "        count = sum(anomalies[method])\n",
    "        print(f\"   {method.replace('_', ' ').title()}: {count} anomalies\")\n",
    "\n",
    "if 'correlation_matrix' in locals():\n",
    "    print(f\"\\nüîó Sensor Relationships:\")\n",
    "    avg_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()\n",
    "    print(f\"   Average cross-correlation: {avg_correlation:.3f}\")\n",
    "    print(f\"   Strongest relationship: {correlation_matrix.abs().max().max():.3f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Analysis Files:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith(('.png', '.json')):\n",
    "        print(f\"   üìÑ {file}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "print(f\"   ‚úÖ Time series decomposition reveals trend and seasonal components\")\n",
    "print(f\"   ‚úÖ Multiple seasonality detection methods identify daily/weekly patterns\")\n",
    "print(f\"   ‚úÖ Anomaly detection using statistical and ML approaches\")\n",
    "print(f\"   ‚úÖ Cross-correlation analysis reveals sensor interdependencies\")\n",
    "print(f\"   ‚úÖ Lagged correlation analysis identifies temporal relationships\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   üìà Explore forecasting models (ARIMA, Prophet, LSTM)\")\n",
    "print(f\"   üîç Implement real-time anomaly detection systems\")\n",
    "print(f\"   üßÆ Apply advanced signal processing techniques\")\n",
    "print(f\"   ü§ñ Train ML models for pattern recognition\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF < /dev/null