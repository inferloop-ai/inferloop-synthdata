{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Quality Assessment with TSIoT\n",
    "\n",
    "This notebook demonstrates comprehensive quality assessment techniques for synthetic IoT time series data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Quality Metrics Framework](#metrics)\n",
    "2. [Statistical Validation](#statistical)\n",
    "3. [Distributional Analysis](#distribution)\n",
    "4. [Temporal Pattern Validation](#temporal)\n",
    "5. [Real vs Synthetic Comparison](#comparison)\n",
    "6. [Quality Scoring System](#scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import wasserstein_distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "output_dir = \"quality_assessment_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Quality assessment environment ready\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Datasets\n",
    "\n",
    "Create multiple synthetic datasets using different generators for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets with different generators\n",
    "generators = ['statistical', 'timegan', 'wgan-gp']\n",
    "datasets = {}\n",
    "\n",
    "for generator in generators:\n",
    "    filename = f\"{output_dir}/{generator}_temperature_data.json\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"tsiot\", \"generate\",\n",
    "        \"--generator\", generator,\n",
    "        \"--sensor-type\", \"temperature\",\n",
    "        \"--count\", \"1440\",  # 24 hours of minute data\n",
    "        \"--frequency\", \"1m\",\n",
    "        \"--start-time\", \"2023-01-01T00:00:00Z\",\n",
    "        \"--output\", filename\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"‚úÖ Generated {generator} data\")\n",
    "        \n",
    "        # Load the data\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data.get('points', []))\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "        datasets[generator] = df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {generator}: {e}\")\n",
    "        # Create mock data if generator fails\n",
    "        timestamps = pd.date_range('2023-01-01', periods=1440, freq='1min')\n",
    "        if generator == 'statistical':\n",
    "            values = np.random.normal(20, 3, 1440)\n",
    "        elif generator == 'timegan':\n",
    "            values = np.random.normal(20, 2.5, 1440) + 2*np.sin(np.arange(1440)*2*np.pi/1440)\n",
    "        else:  # wgan-gp\n",
    "            values = np.random.normal(20, 4, 1440) + np.random.normal(0, 1, 1440)\n",
    "        \n",
    "        df = pd.DataFrame({'value': values}, index=timestamps)\n",
    "        datasets[generator] = df\n",
    "        print(f\"üìä Created mock {generator} data for demonstration\")\n",
    "\n",
    "print(f\"\\nLoaded {len(datasets)} generator datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metrics Framework {#metrics}\n",
    "\n",
    "Define comprehensive quality metrics for synthetic data evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataQualityAssessment:\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def statistical_fidelity(self, real_data, synthetic_data):\n",
    "        \"\"\"Assess statistical similarity between real and synthetic data\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic statistical measures\n",
    "        metrics['mean_diff'] = abs(real_data.mean() - synthetic_data.mean())\n",
    "        metrics['std_diff'] = abs(real_data.std() - synthetic_data.std())\n",
    "        metrics['skew_diff'] = abs(stats.skew(real_data) - stats.skew(synthetic_data))\n",
    "        metrics['kurtosis_diff'] = abs(stats.kurtosis(real_data) - stats.kurtosis(synthetic_data))\n",
    "        \n",
    "        # Correlation with shifted versions (autocorrelation structure)\n",
    "        real_autocorr = [real_data.autocorr(lag=i) for i in range(1, min(50, len(real_data)//2))]\n",
    "        synth_autocorr = [synthetic_data.autocorr(lag=i) for i in range(1, min(50, len(synthetic_data)//2))]\n",
    "        \n",
    "        if real_autocorr and synth_autocorr:\n",
    "            min_len = min(len(real_autocorr), len(synth_autocorr))\n",
    "            autocorr_mse = mean_squared_error(real_autocorr[:min_len], synth_autocorr[:min_len])\n",
    "            metrics['autocorr_mse'] = autocorr_mse\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def distributional_similarity(self, real_data, synthetic_data):\n",
    "        \"\"\"Measure distributional similarity\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_stat, ks_pvalue = stats.ks_2samp(real_data, synthetic_data)\n",
    "        metrics['ks_statistic'] = ks_stat\n",
    "        metrics['ks_pvalue'] = ks_pvalue\n",
    "        \n",
    "        # Wasserstein distance (Earth Mover's Distance)\n",
    "        wasserstein_dist = wasserstein_distance(real_data, synthetic_data)\n",
    "        metrics['wasserstein_distance'] = wasserstein_dist\n",
    "        \n",
    "        # Jensen-Shannon divergence\n",
    "        def js_divergence(p, q, bins=50):\n",
    "            # Create histograms\n",
    "            range_min = min(p.min(), q.min())\n",
    "            range_max = max(p.max(), q.max())\n",
    "            \n",
    "            p_hist, _ = np.histogram(p, bins=bins, range=(range_min, range_max), density=True)\n",
    "            q_hist, _ = np.histogram(q, bins=bins, range=(range_min, range_max), density=True)\n",
    "            \n",
    "            # Normalize\n",
    "            p_hist = p_hist / p_hist.sum()\n",
    "            q_hist = q_hist / q_hist.sum()\n",
    "            \n",
    "            # Add small epsilon to avoid log(0)\n",
    "            epsilon = 1e-10\n",
    "            p_hist += epsilon\n",
    "            q_hist += epsilon\n",
    "            \n",
    "            # Calculate JS divergence\n",
    "            m = 0.5 * (p_hist + q_hist)\n",
    "            js = 0.5 * stats.entropy(p_hist, m) + 0.5 * stats.entropy(q_hist, m)\n",
    "            return js\n",
    "        \n",
    "        js_div = js_divergence(real_data.values, synthetic_data.values)\n",
    "        metrics['js_divergence'] = js_div\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def temporal_consistency(self, data):\n",
    "        \"\"\"Assess temporal patterns and consistency\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # First-order differences (rate of change)\n",
    "        diff = data.diff().dropna()\n",
    "        metrics['mean_change_rate'] = diff.mean()\n",
    "        metrics['std_change_rate'] = diff.std()\n",
    "        metrics['max_change_rate'] = diff.abs().max()\n",
    "        \n",
    "        # Trend analysis\n",
    "        x = np.arange(len(data))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, data.values)\n",
    "        metrics['trend_slope'] = slope\n",
    "        metrics['trend_r_squared'] = r_value**2\n",
    "        \n",
    "        # Stationarity test (Augmented Dickey-Fuller)\n",
    "        try:\n",
    "            from statsmodels.tsa.stattools import adfuller\n",
    "            adf_result = adfuller(data.dropna())\n",
    "            metrics['adf_statistic'] = adf_result[0]\n",
    "            metrics['adf_pvalue'] = adf_result[1]\n",
    "            metrics['is_stationary'] = adf_result[1] < 0.05\n",
    "        except ImportError:\n",
    "            print(\"statsmodels not available for ADF test\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def privacy_assessment(self, real_data, synthetic_data):\n",
    "        \"\"\"Assess privacy preservation (distance from real data)\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Minimum distance to real data points\n",
    "        min_distances = []\n",
    "        for synth_point in synthetic_data.values:\n",
    "            distances = np.abs(real_data.values - synth_point)\n",
    "            min_distances.append(np.min(distances))\n",
    "        \n",
    "        metrics['min_distance_mean'] = np.mean(min_distances)\n",
    "        metrics['min_distance_std'] = np.std(min_distances)\n",
    "        metrics['min_distance_min'] = np.min(min_distances)\n",
    "        \n",
    "        # Proportion of synthetic points that are \"too close\" to real data\n",
    "        threshold = real_data.std() * 0.1  # 10% of standard deviation\n",
    "        too_close = np.sum(np.array(min_distances) < threshold)\n",
    "        metrics['privacy_risk_ratio'] = too_close / len(min_distances)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_overall_score(self, all_metrics):\n",
    "        \"\"\"Calculate overall quality score\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Statistical fidelity score (lower differences = higher score)\n",
    "        stat_metrics = all_metrics['statistical']\n",
    "        stat_score = 1 / (1 + stat_metrics['mean_diff'] + stat_metrics['std_diff'])\n",
    "        scores.append(('statistical_fidelity', stat_score, 0.3))\n",
    "        \n",
    "        # Distributional similarity score\n",
    "        dist_metrics = all_metrics['distributional']\n",
    "        # KS test: higher p-value = more similar distributions\n",
    "        dist_score = dist_metrics['ks_pvalue']\n",
    "        scores.append(('distributional_similarity', dist_score, 0.3))\n",
    "        \n",
    "        # Temporal consistency score (based on reasonable change rates)\n",
    "        temp_metrics = all_metrics['temporal']\n",
    "        # Normalize by reasonable change rate (assume std of change rate should be < 10)\n",
    "        temp_score = max(0, 1 - temp_metrics['std_change_rate'] / 10)\n",
    "        scores.append(('temporal_consistency', temp_score, 0.2))\n",
    "        \n",
    "        # Privacy score (higher min distance = better privacy)\n",
    "        priv_metrics = all_metrics['privacy']\n",
    "        priv_score = 1 - priv_metrics['privacy_risk_ratio']\n",
    "        scores.append(('privacy_preservation', priv_score, 0.2))\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_score = sum(score * weight for _, score, weight in scores)\n",
    "        \n",
    "        return {\n",
    "            'overall_score': weighted_score,\n",
    "            'component_scores': {name: score for name, score, _ in scores}\n",
    "        }\n",
    "\n",
    "# Initialize quality assessment\n",
    "qa = SyntheticDataQualityAssessment()\n",
    "print(\"Quality assessment framework initialized\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Validation {#statistical}\n",
    "\n",
    "Compare statistical properties between different generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first dataset as \"reference\" (in real scenario, this would be real data)\n",
    "reference_data = list(datasets.values())[0]['value']\n",
    "reference_name = list(datasets.keys())[0]\n",
    "\n",
    "print(f\"Using {reference_name} as reference data\")\n",
    "\n",
    "# Assess each dataset against the reference\n",
    "quality_results = {}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    if name == reference_name:\n",
    "        continue\n",
    "    \n",
    "    series = data['value']\n",
    "    \n",
    "    print(f\"\\nüìä Assessing {name} vs {reference_name}...\")\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    statistical_metrics = qa.statistical_fidelity(reference_data, series)\n",
    "    distributional_metrics = qa.distributional_similarity(reference_data, series)\n",
    "    temporal_metrics = qa.temporal_consistency(series)\n",
    "    privacy_metrics = qa.privacy_assessment(reference_data, series)\n",
    "    \n",
    "    all_metrics = {\n",
    "        'statistical': statistical_metrics,\n",
    "        'distributional': distributional_metrics,\n",
    "        'temporal': temporal_metrics,\n",
    "        'privacy': privacy_metrics\n",
    "    }\n",
    "    \n",
    "    # Calculate overall score\n",
    "    overall_assessment = qa.calculate_overall_score(all_metrics)\n",
    "    all_metrics['overall'] = overall_assessment\n",
    "    \n",
    "    quality_results[name] = all_metrics\n",
    "    \n",
    "    print(f\"   Overall Quality Score: {overall_assessment['overall_score']:.3f}\")\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(f\"   Statistical Fidelity:\")\n",
    "    print(f\"     Mean difference: {statistical_metrics['mean_diff']:.3f}\")\n",
    "    print(f\"     Std difference: {statistical_metrics['std_diff']:.3f}\")\n",
    "    \n",
    "    print(f\"   Distributional Similarity:\")\n",
    "    print(f\"     KS test p-value: {distributional_metrics['ks_pvalue']:.3f}\")\n",
    "    print(f\"     Wasserstein distance: {distributional_metrics['wasserstein_distance']:.3f}\")\n",
    "    \n",
    "    print(f\"   Privacy Assessment:\")\n",
    "    print(f\"     Privacy risk ratio: {privacy_metrics['privacy_risk_ratio']:.3f}\")\n",
    "    print(f\"     Min distance mean: {privacy_metrics['min_distance_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Analysis {#distribution}\n",
    "\n",
    "Visualize and compare distributions across different generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive distribution comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Synthetic Data Quality Assessment - Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Colors for different generators\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "generator_names = list(datasets.keys())\n",
    "\n",
    "# 1. Overlapped histograms\n",
    "for i, (name, data) in enumerate(datasets.items()):\n",
    "    axes[0, 0].hist(data['value'], bins=50, alpha=0.6, \n",
    "                   label=name, color=colors[i % len(colors)], density=True)\n",
    "axes[0, 0].set_title('Value Distributions Comparison')\n",
    "axes[0, 0].set_xlabel('Temperature Value')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q plots against reference\n",
    "reference_data_sorted = np.sort(reference_data)\n",
    "for i, (name, data) in enumerate(datasets.items()):\n",
    "    if name == reference_name:\n",
    "        continue\n",
    "    \n",
    "    synth_data_sorted = np.sort(data['value'])\n",
    "    # Interpolate to same length for comparison\n",
    "    min_len = min(len(reference_data_sorted), len(synth_data_sorted))\n",
    "    ref_quantiles = np.interp(np.linspace(0, 1, min_len), \n",
    "                             np.linspace(0, 1, len(reference_data_sorted)), \n",
    "                             reference_data_sorted)\n",
    "    synth_quantiles = np.interp(np.linspace(0, 1, min_len), \n",
    "                               np.linspace(0, 1, len(synth_data_sorted)), \n",
    "                               synth_data_sorted)\n",
    "    \n",
    "    axes[0, 1].scatter(ref_quantiles, synth_quantiles, \n",
    "                      alpha=0.6, s=10, label=name, color=colors[i % len(colors)])\n",
    "\n",
    "# Add diagonal line for perfect match\n",
    "min_val = min([data['value'].min() for data in datasets.values()])\n",
    "max_val = max([data['value'].max() for data in datasets.values()])\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, label='Perfect Match')\n",
    "axes[0, 1].set_title(f'Q-Q Plot vs {reference_name}')\n",
    "axes[0, 1].set_xlabel(f'{reference_name} Quantiles')\n",
    "axes[0, 1].set_ylabel('Synthetic Data Quantiles')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Box plots for comparison\n",
    "box_data = [data['value'].values for data in datasets.values()]\n",
    "box_labels = list(datasets.keys())\n",
    "bp = axes[1, 0].boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "axes[1, 0].set_title('Distribution Summary Statistics')\n",
    "axes[1, 0].set_ylabel('Temperature Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Autocorrelation comparison\n",
    "max_lag = 50\n",
    "for i, (name, data) in enumerate(datasets.items()):\n",
    "    autocorrs = [data['value'].autocorr(lag=lag) for lag in range(1, max_lag + 1)]\n",
    "    axes[1, 1].plot(range(1, max_lag + 1), autocorrs, \n",
    "                   label=name, color=colors[i % len(colors)], linewidth=2)\n",
    "\n",
    "axes[1, 1].set_title('Autocorrelation Functions')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].set_ylabel('Autocorrelation')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Distribution analysis saved to {output_dir}/distribution_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Scoring Visualization {#scoring}\n",
    "\n",
    "Create comprehensive quality scorecards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quality scorecard visualization\n",
    "if quality_results:\n",
    "    # Prepare data for heatmap\n",
    "    metrics_for_heatmap = []\n",
    "    generators_list = []\n",
    "    \n",
    "    metric_categories = [\n",
    "        'statistical_fidelity',\n",
    "        'distributional_similarity', \n",
    "        'temporal_consistency',\n",
    "        'privacy_preservation'\n",
    "    ]\n",
    "    \n",
    "    for generator, results in quality_results.items():\n",
    "        generators_list.append(generator)\n",
    "        scores = results['overall']['component_scores']\n",
    "        row = [scores.get(metric, 0) for metric in metric_categories]\n",
    "        metrics_for_heatmap.append(row)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Quality scores heatmap\n",
    "    heatmap_data = np.array(metrics_for_heatmap)\n",
    "    im = ax1.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax1.set_xticks(range(len(metric_categories)))\n",
    "    ax1.set_xticklabels([m.replace('_', '\\n').title() for m in metric_categories], rotation=45, ha='right')\n",
    "    ax1.set_yticks(range(len(generators_list)))\n",
    "    ax1.set_yticklabels(generators_list)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(generators_list)):\n",
    "        for j in range(len(metric_categories)):\n",
    "            text = ax1.text(j, i, f'{heatmap_data[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    ax1.set_title('Quality Assessment Scorecard')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax1)\n",
    "    cbar.set_label('Quality Score (0-1)', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Overall scores bar chart\n",
    "    overall_scores = [results['overall']['overall_score'] for results in quality_results.values()]\n",
    "    bars = ax2.bar(generators_list, overall_scores, \n",
    "                   color=['red' if score < 0.5 else 'orange' if score < 0.7 else 'green' \n",
    "                         for score in overall_scores])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, overall_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Overall Quality Scores')\n",
    "    ax2.set_ylabel('Overall Score')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add quality thresholds\n",
    "    ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Excellent (>0.8)')\n",
    "    ax2.axhline(y=0.6, color='orange', linestyle='--', alpha=0.7, label='Good (>0.6)')\n",
    "    ax2.axhline(y=0.4, color='red', linestyle='--', alpha=0.7, label='Poor (<0.4)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/quality_scorecard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Quality scorecard saved to {output_dir}/quality_scorecard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Report Generation\n",
    "\n",
    "Use TSIoT's built-in validation engine for comprehensive assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TSIoT validation on each synthetic dataset\n",
    "validation_results = {}\n",
    "\n",
    "for generator in datasets.keys():\n",
    "    input_file = f\"{output_dir}/{generator}_temperature_data.json\"\n",
    "    validation_file = f\"{output_dir}/{generator}_validation_report.json\"\n",
    "    \n",
    "    if os.path.exists(input_file):\n",
    "        cmd = [\n",
    "            \"tsiot\", \"validate\",\n",
    "            \"--synthetic\", input_file,\n",
    "            \"--validators\", \"statistical,distributional,temporal\",\n",
    "            \"--threshold\", \"0.7\",\n",
    "            \"--output\", validation_file\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "            print(f\"‚úÖ Validated {generator} data\")\n",
    "            \n",
    "            # Load validation results\n",
    "            with open(validation_file, 'r') as f:\n",
    "                validation_data = json.load(f)\n",
    "            validation_results[generator] = validation_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation failed for {generator}: {e}\")\n",
    "\n",
    "# Display validation summary\n",
    "if validation_results:\n",
    "    print(\"\\nüìã TSIoT Validation Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for generator, results in validation_results.items():\n",
    "        print(f\"\\nüîç {generator.upper()} Generator:\")\n",
    "        print(f\"   Overall Quality Score: {results.get('overall_quality_score', 'N/A'):.3f}\")\n",
    "        print(f\"   Quality Threshold: {results.get('quality_threshold', 'N/A')}\")\n",
    "        print(f\"   Validation Passed: {'‚úÖ Yes' if results.get('passed', False) else '‚ùå No'}\")\n",
    "        \n",
    "        detailed = results.get('detailed_results', {})\n",
    "        if detailed:\n",
    "            for validator, validator_results in detailed.items():\n",
    "                score = validator_results.get('quality_score', 0)\n",
    "                passed = validator_results.get('passed', False)\n",
    "                print(f\"     {validator.title()}: {score:.3f} {'‚úÖ' if passed else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Quality Report\n",
    "\n",
    "Generate final quality assessment report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive quality report\n",
    "print(\"üìä COMPREHENSIVE SYNTHETIC DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìà Dataset Overview:\")\n",
    "print(f\"   Reference Dataset: {reference_name}\")\n",
    "print(f\"   Comparison Datasets: {len(quality_results)}\")\n",
    "print(f\"   Data Points per Dataset: {len(list(datasets.values())[0])}\")\n",
    "print(f\"   Time Coverage: 24 hours\")\n",
    "print(f\"   Sampling Frequency: 1 minute\")\n",
    "\n",
    "print(f\"\\nüèÜ Quality Ranking:\")\n",
    "if quality_results:\n",
    "    # Sort by overall score\n",
    "    ranked_generators = sorted(quality_results.items(), \n",
    "                              key=lambda x: x[1]['overall']['overall_score'], \n",
    "                              reverse=True)\n",
    "    \n",
    "    for i, (generator, results) in enumerate(ranked_generators, 1):\n",
    "        score = results['overall']['overall_score']\n",
    "        grade = 'A' if score > 0.8 else 'B' if score > 0.6 else 'C' if score > 0.4 else 'D'\n",
    "        print(f\"   {i}. {generator.upper()}: {score:.3f} (Grade: {grade})\")\n",
    "\n",
    "print(f\"\\nüìä Key Quality Insights:\")\n",
    "if quality_results:\n",
    "    # Find best performing generator for each metric\n",
    "    best_statistical = max(quality_results.items(), \n",
    "                          key=lambda x: x[1]['overall']['component_scores']['statistical_fidelity'])\n",
    "    best_distributional = max(quality_results.items(), \n",
    "                             key=lambda x: x[1]['overall']['component_scores']['distributional_similarity'])\n",
    "    best_temporal = max(quality_results.items(), \n",
    "                       key=lambda x: x[1]['overall']['component_scores']['temporal_consistency'])\n",
    "    best_privacy = max(quality_results.items(), \n",
    "                      key=lambda x: x[1]['overall']['component_scores']['privacy_preservation'])\n",
    "    \n",
    "    print(f\"   üéØ Best Statistical Fidelity: {best_statistical[0]} ({best_statistical[1]['overall']['component_scores']['statistical_fidelity']:.3f})\")\n",
    "    print(f\"   üìä Best Distributional Match: {best_distributional[0]} ({best_distributional[1]['overall']['component_scores']['distributional_similarity']:.3f})\")\n",
    "    print(f\"   ‚è±Ô∏è  Best Temporal Consistency: {best_temporal[0]} ({best_temporal[1]['overall']['component_scores']['temporal_consistency']:.3f})\")\n",
    "    print(f\"   üîí Best Privacy Preservation: {best_privacy[0]} ({best_privacy[1]['overall']['component_scores']['privacy_preservation']:.3f})\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Quality Warnings:\")\n",
    "warnings = []\n",
    "if quality_results:\n",
    "    for generator, results in quality_results.items():\n",
    "        overall_score = results['overall']['overall_score']\n",
    "        if overall_score < 0.5:\n",
    "            warnings.append(f\"   üî¥ {generator}: Overall quality below acceptable threshold\")\n",
    "        \n",
    "        # Check specific metrics\n",
    "        priv_risk = results['privacy']['privacy_risk_ratio']\n",
    "        if priv_risk > 0.1:\n",
    "            warnings.append(f\"   üî¥ {generator}: High privacy risk ({priv_risk:.1%} points too close to reference)\")\n",
    "        \n",
    "        ks_pvalue = results['distributional']['ks_pvalue']\n",
    "        if ks_pvalue < 0.05:\n",
    "            warnings.append(f\"   üü° {generator}: Distribution significantly different from reference\")\n",
    "\n",
    "if warnings:\n",
    "    for warning in warnings:\n",
    "        print(warning)\n",
    "else:\n",
    "    print(f\"   ‚úÖ No critical quality issues detected\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "if quality_results:\n",
    "    best_overall = max(quality_results.items(), \n",
    "                      key=lambda x: x[1]['overall']['overall_score'])\n",
    "    worst_overall = min(quality_results.items(), \n",
    "                       key=lambda x: x[1]['overall']['overall_score'])\n",
    "    \n",
    "    print(f\"   ‚úÖ Recommended generator: {best_overall[0]} (score: {best_overall[1]['overall']['overall_score']:.3f})\")\n",
    "    print(f\"   ‚ùå Avoid using: {worst_overall[0]} (score: {worst_overall[1]['overall']['overall_score']:.3f})\")\n",
    "    print(f\"   üîß Consider hyperparameter tuning for generators with scores < 0.7\")\n",
    "    print(f\"   üìä Use ensemble methods to combine strengths of different generators\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "for file in sorted(os.listdir(output_dir)):\n",
    "    if file.endswith(('.png', '.json')):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"   üìÑ {file} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüéâ Quality assessment complete\! Review the generated visualizations and reports.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF < /dev/null