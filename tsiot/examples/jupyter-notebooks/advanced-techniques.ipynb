{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Synthetic Data Generation Techniques with TSIoT\n",
    "\n",
    "This notebook explores advanced synthetic data generation methods, including deep learning approaches and complex multi-sensor scenarios.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Advanced Generator Comparison](#generators)\n",
    "2. [Multi-variate Time Series Generation](#multivariate)\n",
    "3. [Conditional Generation](#conditional)\n",
    "4. [Adversarial Training Insights](#adversarial)\n",
    "5. [Custom Pattern Injection](#patterns)\n",
    "6. [Scalability and Performance](#performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal, stats\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "output_dir = \"advanced_techniques_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Advanced techniques environment ready\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Generator Comparison {#generators}\n",
    "\n",
    "Compare different generation approaches for complex scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define advanced generator configurations\n",
    "advanced_configs = {\n",
    "    'statistical_basic': {\n",
    "        'generator': 'statistical',\n",
    "        'description': 'Basic statistical model',\n",
    "        'use_case': 'Quick prototyping, simple patterns'\n",
    "    },\n",
    "    'timegan_standard': {\n",
    "        'generator': 'timegan',\n",
    "        'description': 'TimeGAN with default parameters',\n",
    "        'use_case': 'Complex temporal dependencies'\n",
    "    },\n",
    "    'wgan_gp': {\n",
    "        'generator': 'wgan-gp',\n",
    "        'description': 'WGAN with Gradient Penalty',\n",
    "        'use_case': 'High-fidelity generation with stability'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate datasets with different complexities\n",
    "datasets_advanced = {}\n",
    "generation_times = {}\n",
    "\n",
    "for config_name, config in advanced_configs.items():\n",
    "    print(f\"\\nüîÑ Generating with {config_name}...\")\n",
    "    print(f\"   Description: {config['description']}\")\n",
    "    print(f\"   Use case: {config['use_case']}\")\n",
    "    \n",
    "    filename = f\"{output_dir}/{config_name}_complex_data.json\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"tsiot\", \"generate\",\n",
    "        \"--generator\", config['generator'],\n",
    "        \"--sensor-type\", \"temperature\",\n",
    "        \"--count\", \"2880\",  # 48 hours of minute data\n",
    "        \"--frequency\", \"1m\",\n",
    "        \"--start-time\", \"2023-01-01T00:00:00Z\",\n",
    "        \"--output\", filename\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        generation_time = time.time() - start_time\n",
    "        generation_times[config_name] = generation_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated in {generation_time:.2f} seconds\")\n",
    "        \n",
    "        # Load the data\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data.get('points', []))\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "        datasets_advanced[config_name] = df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        # Create mock data for demonstration\n",
    "        timestamps = pd.date_range('2023-01-01', periods=2880, freq='1min')\n",
    "        \n",
    "        if 'timegan' in config_name:\n",
    "            # More complex temporal patterns\n",
    "            t = np.arange(2880)\n",
    "            base = 20 + 5*np.sin(2*np.pi*t/1440)  # Daily cycle\n",
    "            weekly = 2*np.sin(2*np.pi*t/(1440*7))  # Weekly cycle\n",
    "            noise = np.random.normal(0, 1, 2880)\n",
    "            values = base + weekly + noise\n",
    "        elif 'wgan' in config_name:\n",
    "            # High variability with stable distribution\n",
    "            values = np.random.normal(20, 3, 2880)\n",
    "            # Add some structured noise\n",
    "            values += 2*np.sin(np.arange(2880)*2*np.pi/360)  # 6-hour cycle\n",
    "        else:\n",
    "            # Basic statistical\n",
    "            values = np.random.normal(20, 2, 2880)\n",
    "        \n",
    "        df = pd.DataFrame({'value': values}, index=timestamps)\n",
    "        datasets_advanced[config_name] = df\n",
    "        generation_times[config_name] = time.time() - start_time\n",
    "        print(f\"   üìä Created mock data for demonstration\")\n",
    "\n",
    "print(f\"\\nGenerated {len(datasets_advanced)} advanced datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Time Series Generation {#multivariate}\n",
    "\n",
    "Generate correlated multi-sensor data with complex interdependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-sensor correlated data\n",
    "sensors_config = {\n",
    "    'temperature': {'base': 20, 'std': 3, 'trend': 0.001},\n",
    "    'humidity': {'base': 60, 'std': 8, 'trend': -0.002},\n",
    "    'pressure': {'base': 1013, 'std': 5, 'trend': 0.0005},\n",
    "    'co2': {'base': 400, 'std': 20, 'trend': 0.01}\n",
    "}\n",
    "\n",
    "multivariate_datasets = {}\n",
    "\n",
    "print(\"üîó Generating correlated multi-sensor data...\")\n",
    "\n",
    "for sensor, config in sensors_config.items():\n",
    "    filename = f\"{output_dir}/multivariate_{sensor}_data.json\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"tsiot\", \"generate\",\n",
    "        \"--generator\", \"statistical\",\n",
    "        \"--sensor-type\", sensor,\n",
    "        \"--count\", \"1440\",  # 24 hours\n",
    "        \"--frequency\", \"1m\",\n",
    "        \"--start-time\", \"2023-01-01T00:00:00Z\",\n",
    "        \"--output\", filename\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"   ‚úÖ Generated {sensor} data\")\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data.get('points', []))\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "        multivariate_datasets[sensor] = df['value']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   üìä Creating synthetic {sensor} data\")\n",
    "        # Create correlated synthetic data\n",
    "        timestamps = pd.date_range('2023-01-01', periods=1440, freq='1min')\n",
    "        t = np.arange(1440)\n",
    "        \n",
    "        # Base signal with daily pattern\n",
    "        daily_pattern = config['std'] * 0.5 * np.sin(2*np.pi*t/1440)\n",
    "        trend = config['trend'] * t\n",
    "        noise = np.random.normal(0, config['std'] * 0.3, 1440)\n",
    "        \n",
    "        values = config['base'] + daily_pattern + trend + noise\n",
    "        multivariate_datasets[sensor] = pd.Series(values, index=timestamps)\n",
    "\n",
    "# Add cross-correlations\n",
    "print(\"\\nüîó Adding cross-sensor correlations...\")\n",
    "\n",
    "# Temperature-Humidity inverse correlation\n",
    "if 'temperature' in multivariate_datasets and 'humidity' in multivariate_datasets:\n",
    "    temp_normalized = (multivariate_datasets['temperature'] - multivariate_datasets['temperature'].mean()) / multivariate_datasets['temperature'].std()\n",
    "    humidity_correction = -0.3 * temp_normalized * multivariate_datasets['humidity'].std()\n",
    "    multivariate_datasets['humidity'] += humidity_correction\n",
    "    print(\"   ‚úÖ Added temperature-humidity inverse correlation\")\n",
    "\n",
    "# Pressure-Temperature weak correlation\n",
    "if 'pressure' in multivariate_datasets and 'temperature' in multivariate_datasets:\n",
    "    temp_normalized = (multivariate_datasets['temperature'] - multivariate_datasets['temperature'].mean()) / multivariate_datasets['temperature'].std()\n",
    "    pressure_correction = 0.15 * temp_normalized * multivariate_datasets['pressure'].std()\n",
    "    multivariate_datasets['pressure'] += pressure_correction\n",
    "    print(\"   ‚úÖ Added temperature-pressure weak correlation\")\n",
    "\n",
    "# Combine into single DataFrame\n",
    "multivariate_df = pd.DataFrame(multivariate_datasets)\n",
    "\n",
    "print(f\"\\nüìä Multivariate dataset shape: {multivariate_df.shape}\")\n",
    "print(f\"   Sensors: {list(multivariate_df.columns)}\")\n",
    "print(f\"   Time span: {multivariate_df.index[0]} to {multivariate_df.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Visualization and Analysis\n",
    "\n",
    "Create comprehensive visualizations for advanced generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced analysis dashboard\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Generator comparison - Time series\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (name, data) in enumerate(datasets_advanced.items()):\n",
    "    sample_data = data['value'].iloc[:1440]  # First 24 hours\n",
    "    ax1.plot(sample_data.index, sample_data.values, \n",
    "            label=name.replace('_', ' ').title(), \n",
    "            color=colors[i % len(colors)], alpha=0.7, linewidth=1)\n",
    "ax1.set_title('Advanced Generator Comparison - Time Series (24 hours)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Temperature Value')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Spectral analysis\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "for i, (name, data) in enumerate(datasets_advanced.items()):\n",
    "    sample_data = data['value'].iloc[:1440]\n",
    "    freqs, psd = signal.welch(sample_data.values, fs=1/60, nperseg=256)  # fs in Hz (1/60 for minute data)\n",
    "    ax2.semilogy(freqs * 3600, psd, label=name.replace('_', ' ').title(), \n",
    "                color=colors[i % len(colors)], alpha=0.7)  # Convert to cycles per hour\n",
    "ax2.set_title('Power Spectral Density')\n",
    "ax2.set_xlabel('Frequency (cycles/hour)')\n",
    "ax2.set_ylabel('PSD')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Generation performance\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "gen_names = list(generation_times.keys())\n",
    "gen_times = list(generation_times.values())\n",
    "bars = ax3.bar(gen_names, gen_times, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax3.set_title('Generation Performance')\n",
    "ax3.set_ylabel('Time (seconds)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "# Add value labels\n",
    "for bar, time_val in zip(bars, gen_times):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Statistical comparison\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "stats_data = []\n",
    "for name, data in datasets_advanced.items():\n",
    "    stats_data.append([\n",
    "        data['value'].mean(),\n",
    "        data['value'].std(),\n",
    "        stats.skew(data['value']),\n",
    "        stats.kurtosis(data['value'])\n",
    "    ])\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data, \n",
    "                       columns=['Mean', 'Std', 'Skewness', 'Kurtosis'],\n",
    "                       index=[name.replace('_', ' ').title() for name in datasets_advanced.keys()])\n",
    "\n",
    "# Normalize for better visualization\n",
    "stats_normalized = stats_df.div(stats_df.max(), axis=1)\n",
    "im = ax4.imshow(stats_normalized.values, cmap='viridis', aspect='auto')\n",
    "ax4.set_xticks(range(len(stats_df.columns)))\n",
    "ax4.set_xticklabels(stats_df.columns)\n",
    "ax4.set_yticks(range(len(stats_df.index)))\n",
    "ax4.set_yticklabels(stats_df.index)\n",
    "ax4.set_title('Statistical Properties\\n(Normalized)')\n",
    "plt.colorbar(im, ax=ax4, shrink=0.8)\n",
    "\n",
    "# 5. Multivariate correlation matrix\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "if not multivariate_df.empty:\n",
    "    correlation_matrix = multivariate_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', ax=ax5, cbar_kws={'shrink': 0.8})\n",
    "    ax5.set_title('Multi-Sensor Correlations')\n",
    "\n",
    "# 6. Multivariate time series\n",
    "ax6 = fig.add_subplot(gs[2, 1:])\n",
    "if not multivariate_df.empty:\n",
    "    # Normalize for plotting\n",
    "    normalized_df = (multivariate_df - multivariate_df.mean()) / multivariate_df.std()\n",
    "    \n",
    "    for i, column in enumerate(normalized_df.columns):\n",
    "        sample_data = normalized_df[column].iloc[:1440]  # 24 hours\n",
    "        ax6.plot(sample_data.index, sample_data.values + i*3,  # Offset for visibility\n",
    "                label=f'{column.title()} (offset +{i*3})',\n",
    "                alpha=0.7, linewidth=1)\n",
    "    \n",
    "    ax6.set_title('Multi-Sensor Time Series (Normalized & Offset)')\n",
    "    ax6.set_ylabel('Normalized Value + Offset')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Pattern analysis - autocorrelation\n",
    "ax7 = fig.add_subplot(gs[3, 0])\n",
    "max_lag = 100\n",
    "for i, (name, data) in enumerate(datasets_advanced.items()):\n",
    "    autocorrs = [data['value'].autocorr(lag=lag) for lag in range(1, max_lag + 1)]\n",
    "    ax7.plot(range(1, max_lag + 1), autocorrs, \n",
    "            label=name.replace('_', ' ').title(), \n",
    "            color=colors[i % len(colors)], linewidth=2, alpha=0.7)\n",
    "ax7.set_title('Autocorrelation Functions')\n",
    "ax7.set_xlabel('Lag (minutes)')\n",
    "ax7.set_ylabel('Autocorrelation')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "ax7.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 8. Distribution comparison\n",
    "ax8 = fig.add_subplot(gs[3, 1])\n",
    "for i, (name, data) in enumerate(datasets_advanced.items()):\n",
    "    ax8.hist(data['value'], bins=50, alpha=0.6, \n",
    "            label=name.replace('_', ' ').title(), \n",
    "            color=colors[i % len(colors)], density=True)\n",
    "ax8.set_title('Value Distributions')\n",
    "ax8.set_xlabel('Temperature Value')\n",
    "ax8.set_ylabel('Density')\n",
    "ax8.legend()\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Complexity metrics\n",
    "ax9 = fig.add_subplot(gs[3, 2])\n",
    "complexity_metrics = {}\n",
    "for name, data in datasets_advanced.items():\n",
    "    # Calculate various complexity measures\n",
    "    values = data['value'].values\n",
    "    \n",
    "    # Approximate entropy (measure of regularity)\n",
    "    def approximate_entropy(data, m=2, r=None):\n",
    "        if r is None:\n",
    "            r = 0.2 * np.std(data)\n",
    "        \n",
    "        def _maxdist(xi, xj, m):\n",
    "            return max([abs(xi[k] - xj[k]) for k in range(m)])\n",
    "        \n",
    "        def _phi(m):\n",
    "            patterns = np.array([data[i:i+m] for i in range(len(data) - m + 1)])\n",
    "            C = np.zeros(len(patterns))\n",
    "            \n",
    "            for i in range(len(patterns)):\n",
    "                template = patterns[i]\n",
    "                for j in range(len(patterns)):\n",
    "                    if _maxdist(template, patterns[j], m) <= r:\n",
    "                        C[i] += 1.0\n",
    "            \n",
    "            phi = np.mean(np.log(C / len(patterns)))\n",
    "            return phi\n",
    "        \n",
    "        return _phi(m) - _phi(m + 1)\n",
    "    \n",
    "    # Sample entropy (simpler version)\n",
    "    sample_ent = approximate_entropy(values[:500])  # Use subset for speed\n",
    "    \n",
    "    # Coefficient of variation\n",
    "    cv = np.std(values) / np.mean(values)\n",
    "    \n",
    "    # Number of turning points\n",
    "    diff = np.diff(values)\n",
    "    turning_points = np.sum(np.diff(np.sign(diff)) \!= 0)\n",
    "    \n",
    "    complexity_metrics[name] = {\n",
    "        'Sample Entropy': sample_ent,\n",
    "        'Coeff. Variation': cv,\n",
    "        'Turning Points': turning_points / len(values)\n",
    "    }\n",
    "\n",
    "# Plot complexity metrics\n",
    "metrics_df = pd.DataFrame(complexity_metrics).T\n",
    "metrics_normalized = metrics_df.div(metrics_df.max(), axis=1)\n",
    "\n",
    "x = np.arange(len(metrics_df.index))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics_df.columns):\n",
    "    ax9.bar(x + i*width, metrics_normalized[metric], width, \n",
    "           label=metric, alpha=0.7)\n",
    "\n",
    "ax9.set_title('Complexity Metrics\\n(Normalized)')\n",
    "ax9.set_xlabel('Generator')\n",
    "ax9.set_ylabel('Normalized Score')\n",
    "ax9.set_xticks(x + width)\n",
    "ax9.set_xticklabels([name.replace('_', '\\n').title() for name in metrics_df.index])\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Advanced Synthetic Data Generation Analysis Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.savefig(f'{output_dir}/advanced_analysis_dashboard.png', \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Advanced analysis dashboard saved to {output_dir}/advanced_analysis_dashboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pattern Injection {#patterns}\n",
    "\n",
    "Demonstrate how to inject custom patterns into synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_custom_patterns(base_data, patterns):\n",
    "    \"\"\"Inject custom patterns into base synthetic data\"\"\"\n",
    "    modified_data = base_data.copy()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        pattern_type = pattern['type']\n",
    "        \n",
    "        if pattern_type == 'anomaly_spike':\n",
    "            # Inject sudden spikes\n",
    "            spike_times = np.random.choice(len(modified_data), \n",
    "                                         size=pattern.get('count', 5), \n",
    "                                         replace=False)\n",
    "            spike_magnitude = pattern.get('magnitude', 3) * modified_data.std()\n",
    "            \n",
    "            for spike_time in spike_times:\n",
    "                modified_data.iloc[spike_time] += spike_magnitude\n",
    "            \n",
    "            print(f\"   ‚úÖ Injected {len(spike_times)} anomaly spikes\")\n",
    "        \n",
    "        elif pattern_type == 'gradual_drift':\n",
    "            # Add gradual drift\n",
    "            start_idx = pattern.get('start', 0)\n",
    "            end_idx = pattern.get('end', len(modified_data))\n",
    "            drift_magnitude = pattern.get('magnitude', 2)\n",
    "            \n",
    "            drift_indices = range(start_idx, min(end_idx, len(modified_data)))\n",
    "            drift_values = np.linspace(0, drift_magnitude, len(drift_indices))\n",
    "            \n",
    "            for i, drift in zip(drift_indices, drift_values):\n",
    "                modified_data.iloc[i] += drift\n",
    "            \n",
    "            print(f\"   ‚úÖ Added gradual drift over {len(drift_indices)} points\")\n",
    "        \n",
    "        elif pattern_type == 'periodic_event':\n",
    "            # Add recurring events\n",
    "            period = pattern.get('period', 360)  # Every 6 hours\n",
    "            event_duration = pattern.get('duration', 30)  # 30 minutes\n",
    "            event_magnitude = pattern.get('magnitude', 1.5)\n",
    "            \n",
    "            event_count = 0\n",
    "            for start_idx in range(0, len(modified_data), period):\n",
    "                end_idx = min(start_idx + event_duration, len(modified_data))\n",
    "                event_pattern = event_magnitude * np.sin(np.linspace(0, np.pi, end_idx - start_idx))\n",
    "                \n",
    "                for i, offset in enumerate(range(start_idx, end_idx)):\n",
    "                    modified_data.iloc[offset] += event_pattern[i]\n",
    "                \n",
    "                event_count += 1\n",
    "            \n",
    "            print(f\"   ‚úÖ Added {event_count} periodic events\")\n",
    "        \n",
    "        elif pattern_type == 'seasonal_variation':\n",
    "            # Add seasonal variation\n",
    "            season_period = pattern.get('period', 1440)  # Daily\n",
    "            season_amplitude = pattern.get('amplitude', 2)\n",
    "            \n",
    "            t = np.arange(len(modified_data))\n",
    "            seasonal_pattern = season_amplitude * np.sin(2 * np.pi * t / season_period)\n",
    "            \n",
    "            modified_data += seasonal_pattern\n",
    "            \n",
    "            print(f\"   ‚úÖ Added seasonal variation with period {season_period}\")\n",
    "    \n",
    "    return modified_data\n",
    "\n",
    "# Define custom patterns to inject\n",
    "custom_patterns = [\n",
    "    {\n",
    "        'type': 'seasonal_variation',\n",
    "        'period': 1440,  # Daily cycle\n",
    "        'amplitude': 3\n",
    "    },\n",
    "    {\n",
    "        'type': 'periodic_event',\n",
    "        'period': 360,   # Every 6 hours\n",
    "        'duration': 60,  # 1 hour duration\n",
    "        'magnitude': 2\n",
    "    },\n",
    "    {\n",
    "        'type': 'anomaly_spike',\n",
    "        'count': 8,\n",
    "        'magnitude': 4\n",
    "    },\n",
    "    {\n",
    "        'type': 'gradual_drift',\n",
    "        'start': 1000,\n",
    "        'end': 1800,\n",
    "        'magnitude': 3\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply patterns to one of our datasets\n",
    "if datasets_advanced:\n",
    "    base_dataset_name = list(datasets_advanced.keys())[0]\n",
    "    base_data = datasets_advanced[base_dataset_name]['value']\n",
    "    \n",
    "    print(f\"üé® Injecting custom patterns into {base_dataset_name} data...\")\n",
    "    modified_data = inject_custom_patterns(base_data, custom_patterns)\n",
    "    \n",
    "    # Visualize the effect\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Original data\n",
    "    sample_size = 1440  # 24 hours\n",
    "    axes[0].plot(base_data.index[:sample_size], base_data.values[:sample_size], \n",
    "                'b-', linewidth=1, alpha=0.7, label='Original')\n",
    "    axes[0].set_title('Original Synthetic Data')\n",
    "    axes[0].set_ylabel('Temperature')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Modified data\n",
    "    axes[1].plot(modified_data.index[:sample_size], modified_data.values[:sample_size], \n",
    "                'r-', linewidth=1, alpha=0.7, label='With Custom Patterns')\n",
    "    axes[1].set_title('After Pattern Injection')\n",
    "    axes[1].set_ylabel('Temperature')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Comparison\n",
    "    axes[2].plot(base_data.index[:sample_size], base_data.values[:sample_size], \n",
    "                'b-', linewidth=1, alpha=0.6, label='Original')\n",
    "    axes[2].plot(modified_data.index[:sample_size], modified_data.values[:sample_size], \n",
    "                'r-', linewidth=1, alpha=0.6, label='Modified')\n",
    "    axes[2].set_title('Comparison: Original vs Modified')\n",
    "    axes[2].set_ylabel('Temperature')\n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/custom_pattern_injection.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Pattern injection visualization saved to {output_dir}/custom_pattern_injection.png\")\n",
    "    \n",
    "    # Analyze the changes\n",
    "    print(f\"\\nüìà Pattern Injection Analysis:\")\n",
    "    print(f\"   Original mean: {base_data.mean():.2f}\")\n",
    "    print(f\"   Modified mean: {modified_data.mean():.2f}\")\n",
    "    print(f\"   Original std: {base_data.std():.2f}\")\n",
    "    print(f\"   Modified std: {modified_data.std():.2f}\")\n",
    "    print(f\"   Maximum change: {(modified_data - base_data).abs().max():.2f}\")\n",
    "    print(f\"   Mean absolute change: {(modified_data - base_data).abs().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and Scalability Analysis {#performance}\n",
    "\n",
    "Analyze generation performance across different scales and complexities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "def benchmark_generation(generator, data_sizes, sensor_type='temperature'):\n",
    "    \"\"\"Benchmark generation performance across different data sizes\"\"\"\n",
    "    results = {'sizes': [], 'times': [], 'throughput': []}\n",
    "    \n",
    "    for size in data_sizes:\n",
    "        print(f\"   Testing {generator} with {size} data points...\")\n",
    "        \n",
    "        filename = f\"{output_dir}/benchmark_{generator}_{size}.json\"\n",
    "        \n",
    "        cmd = [\n",
    "            \"tsiot\", \"generate\",\n",
    "            \"--generator\", generator,\n",
    "            \"--sensor-type\", sensor_type,\n",
    "            \"--count\", str(size),\n",
    "            \"--frequency\", \"1m\",\n",
    "            \"--output\", filename\n",
    "        ]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "            generation_time = time.time() - start_time\n",
    "            throughput = size / generation_time if generation_time > 0 else 0\n",
    "            \n",
    "            results['sizes'].append(size)\n",
    "            results['times'].append(generation_time)\n",
    "            results['throughput'].append(throughput)\n",
    "            \n",
    "            print(f\"     ‚úÖ {generation_time:.2f}s ({throughput:.0f} points/sec)\")\n",
    "            \n",
    "            # Clean up benchmark file\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Error: {e}\")\n",
    "            # Use mock timing for demonstration\n",
    "            mock_time = size / 1000 + np.random.normal(0, 0.1)  # Simulate timing\n",
    "            mock_time = max(0.1, mock_time)  # Minimum time\n",
    "            \n",
    "            results['sizes'].append(size)\n",
    "            results['times'].append(mock_time)\n",
    "            results['throughput'].append(size / mock_time)\n",
    "            \n",
    "            print(f\"     üìä Mock timing: {mock_time:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test scenarios\n",
    "test_generators = ['statistical', 'timegan']\n",
    "test_sizes = [100, 500, 1000, 2000, 5000]\n",
    "\n",
    "print(\"üöÄ Performance Benchmarking:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "benchmark_results = {}\n",
    "for generator in test_generators:\n",
    "    print(f\"\\nüìä Benchmarking {generator.upper()} generator:\")\n",
    "    results = benchmark_generation(generator, test_sizes)\n",
    "    benchmark_results[generator] = results\n",
    "\n",
    "# Visualize performance results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Generation time vs data size\n",
    "for generator, results in benchmark_results.items():\n",
    "    axes[0, 0].plot(results['sizes'], results['times'], 'o-', \n",
    "                   label=generator.title(), linewidth=2, markersize=6)\n",
    "axes[0, 0].set_title('Generation Time vs Data Size')\n",
    "axes[0, 0].set_xlabel('Data Points')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput comparison\n",
    "for generator, results in benchmark_results.items():\n",
    "    axes[0, 1].plot(results['sizes'], results['throughput'], 's-', \n",
    "                   label=generator.title(), linewidth=2, markersize=6)\n",
    "axes[0, 1].set_title('Throughput vs Data Size')\n",
    "axes[0, 1].set_xlabel('Data Points')\n",
    "axes[0, 1].set_ylabel('Throughput (points/sec)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency comparison (log scale)\n",
    "for generator, results in benchmark_results.items():\n",
    "    efficiency = [t/s for t, s in zip(results['times'], results['sizes'])]\n",
    "    axes[1, 0].semilogy(results['sizes'], efficiency, '^-', \n",
    "                       label=f'{generator.title()} (sec/point)', linewidth=2, markersize=6)\n",
    "axes[1, 0].set_title('Generation Efficiency (Log Scale)')\n",
    "axes[1, 0].set_xlabel('Data Points')\n",
    "axes[1, 0].set_ylabel('Time per Point (sec/point)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage estimation (mock data)\n",
    "memory_usage = {}\n",
    "for generator in test_generators:\n",
    "    # Mock memory usage based on generator complexity\n",
    "    if generator == 'statistical':\n",
    "        base_memory = 50  # MB\n",
    "        scaling_factor = 0.001\n",
    "    else:  # timegan or other complex generators\n",
    "        base_memory = 200  # MB\n",
    "        scaling_factor = 0.005\n",
    "    \n",
    "    memory_usage[generator] = [base_memory + size * scaling_factor for size in test_sizes]\n",
    "\n",
    "for generator, memory in memory_usage.items():\n",
    "    axes[1, 1].plot(test_sizes, memory, 'd-', \n",
    "                   label=f'{generator.title()}', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_title('Estimated Memory Usage')\n",
    "axes[1, 1].set_xlabel('Data Points')\n",
    "axes[1, 1].set_ylabel('Memory (MB)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('TSIoT Performance Benchmarking Results', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/performance_benchmarking.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Performance benchmarking results saved to {output_dir}/performance_benchmarking.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Techniques Summary\n",
    "\n",
    "Comprehensive summary of advanced techniques and best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì ADVANCED SYNTHETIC DATA GENERATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüî¨ Techniques Explored:\")\n",
    "print(f\"   ‚úÖ Advanced generator comparison (Statistical, TimeGAN, WGAN-GP)\")\n",
    "print(f\"   ‚úÖ Multi-variate time series with cross-correlations\")\n",
    "print(f\"   ‚úÖ Custom pattern injection (anomalies, drifts, events)\")\n",
    "print(f\"   ‚úÖ Spectral and complexity analysis\")\n",
    "print(f\"   ‚úÖ Performance benchmarking across scales\")\n",
    "\n",
    "print(f\"\\nüìä Generator Performance Insights:\")\n",
    "if benchmark_results:\n",
    "    # Calculate average performance metrics\n",
    "    for generator, results in benchmark_results.items():\n",
    "        avg_throughput = np.mean(results['throughput'])\n",
    "        max_throughput = max(results['throughput'])\n",
    "        print(f\"   {generator.upper()}:\")\n",
    "        print(f\"     Average throughput: {avg_throughput:.0f} points/sec\")\n",
    "        print(f\"     Peak throughput: {max_throughput:.0f} points/sec\")\n",
    "        print(f\"     Recommended for: {advanced_configs.get(generator + '_standard', {}).get('use_case', 'General use')}\")\n",
    "\n",
    "print(f\"\\nüéØ Best Practices Identified:\")\n",
    "print(f\"   üìà Generator Selection:\")\n",
    "print(f\"     ‚Ä¢ Statistical: Fast, simple patterns, prototyping\")\n",
    "print(f\"     ‚Ä¢ TimeGAN: Complex temporal dependencies, high fidelity\")\n",
    "print(f\"     ‚Ä¢ WGAN-GP: Stable training, diverse outputs\")\n",
    "\n",
    "print(f\"\\n   üîó Multi-variate Generation:\")\n",
    "print(f\"     ‚Ä¢ Model cross-sensor correlations explicitly\")\n",
    "print(f\"     ‚Ä¢ Use domain knowledge for realistic relationships\")\n",
    "print(f\"     ‚Ä¢ Validate correlation preservation in synthetic data\")\n",
    "\n",
    "print(f\"\\n   üé® Pattern Engineering:\")\n",
    "print(f\"     ‚Ä¢ Layer patterns: seasonal + events + anomalies\")\n",
    "print(f\"     ‚Ä¢ Control pattern intensity and frequency\")\n",
    "print(f\"     ‚Ä¢ Validate pattern preservation post-injection\")\n",
    "\n",
    "print(f\"\\n   ‚ö° Performance Optimization:\")\n",
    "print(f\"     ‚Ä¢ Batch generation for large datasets\")\n",
    "print(f\"     ‚Ä¢ Consider memory constraints for complex generators\")\n",
    "print(f\"     ‚Ä¢ Profile generation time vs quality trade-offs\")\n",
    "\n",
    "print(f\"\\nüîç Quality Assessment Checklist:\")\n",
    "print(f\"   ‚òê Statistical fidelity (mean, std, skewness, kurtosis)\")\n",
    "print(f\"   ‚òê Distributional similarity (KS test, Wasserstein distance)\")\n",
    "print(f\"   ‚òê Temporal consistency (autocorrelation, stationarity)\")\n",
    "print(f\"   ‚òê Spectral characteristics (frequency domain analysis)\")\n",
    "print(f\"   ‚òê Cross-sensor correlations (multivariate validation)\")\n",
    "print(f\"   ‚òê Pattern preservation (seasonal, trend, anomalies)\")\n",
    "print(f\"   ‚òê Privacy assessment (distance from real data)\")\n",
    "\n",
    "print(f\"\\nüìö Advanced Applications:\")\n",
    "print(f\"   ü§ñ Machine Learning:\")\n",
    "print(f\"     ‚Ä¢ Data augmentation for IoT ML models\")\n",
    "print(f\"     ‚Ä¢ Stress testing with edge cases\")\n",
    "print(f\"     ‚Ä¢ Privacy-preserving model training\")\n",
    "\n",
    "print(f\"\\n   üè≠ Industrial Use Cases:\")\n",
    "print(f\"     ‚Ä¢ Equipment failure simulation\")\n",
    "print(f\"     ‚Ä¢ Process optimization scenarios\")\n",
    "print(f\"     ‚Ä¢ Predictive maintenance testing\")\n",
    "\n",
    "print(f\"\\n   üîí Privacy & Security:\")\n",
    "print(f\"     ‚Ä¢ Differential privacy techniques\")\n",
    "print(f\"     ‚Ä¢ Anonymization validation\")\n",
    "print(f\"     ‚Ä¢ Regulatory compliance testing\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Artifacts:\")\n",
    "artifacts = [\n",
    "    \"advanced_analysis_dashboard.png\",\n",
    "    \"custom_pattern_injection.png\", \n",
    "    \"performance_benchmarking.png\"\n",
    "]\n",
    "\n",
    "for artifact in artifacts:\n",
    "    if os.path.exists(os.path.join(output_dir, artifact)):\n",
    "        file_size = os.path.getsize(os.path.join(output_dir, artifact))\n",
    "        print(f\"   üìä {artifact} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüìÅ All Output Files:\")\n",
    "for file in sorted(os.listdir(output_dir)):\n",
    "    if file.endswith(('.png', '.json')):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"   üìÑ {file} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps for Advanced Users:\")\n",
    "print(f\"   üìñ Study domain-specific generator architectures\")\n",
    "print(f\"   üß™ Experiment with hyperparameter optimization\")\n",
    "print(f\"   üîß Develop custom evaluation metrics\")\n",
    "print(f\"   üìä Implement real-time generation pipelines\")\n",
    "print(f\"   ü§ù Contribute to TSIoT open source project\")\n",
    "\n",
    "print(f\"\\nüéâ Advanced techniques exploration complete\!\")\n",
    "print(f\"    You're now ready to tackle complex synthetic data challenges.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF < /dev/null