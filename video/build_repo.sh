#!/bin/bash

# Inferloop Synthetic Data (Video) - Complete Repository Builder
# Creates the entire enterprise-grade repository structure with all files and configurations

set -e  # Exit on any error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
REPO_NAME="inferloop-synthdata/video"
PROJECT_ROOT="$(pwd)/$REPO_NAME"
SCRIPT_VERSION="1.0.0"

# Helper functions
print_header() {
    echo -e "${BLUE}======================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}======================================${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_info() {
    echo -e "${CYAN}â„¹ï¸  $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_step() {
    echo -e "${PURPLE}ðŸ”¨ $1${NC}"
}

# Check prerequisites
check_prerequisites() {
    print_header "Checking Prerequisites"
    
    local missing_tools=()
    
    if ! command -v git &> /dev/null; then
        missing_tools+=("git")
    fi
    
    if ! command -v docker &> /dev/null; then
        missing_tools+=("docker")
    fi
    
    if ! command -v python3 &> /dev/null; then
        missing_tools+=("python3")
    fi
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        print_warning "Missing tools: ${missing_tools[*]}"
        print_info "These tools are recommended but not required for repository creation"
    else
        print_success "All prerequisites are available"
    fi
}

# Create directory structure
create_directories() {
    print_header "Creating Directory Structure"
    
    # Check if directory already exists
    if [ -d "$PROJECT_ROOT" ]; then
        print_warning "Directory $PROJECT_ROOT already exists"
        read -p "Do you want to remove it and continue? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            rm -rf "$PROJECT_ROOT"
            print_info "Removed existing directory"
        else
            print_error "Aborting to avoid overwriting existing directory"
            exit 1
        fi
    fi
    
    # Create main directory
    mkdir -p "$PROJECT_ROOT"
    cd "$PROJECT_ROOT"
    
    print_step "Creating core service directories..."
    
    # Core Services
    mkdir -p services/{ingestion-service,metrics-extraction-service,generation-service,validation-service,delivery-service,orchestration-service}/{src,tests,config,docs}
    
    print_step "Creating pipeline component directories..."
    
    # Pipeline Components
    mkdir -p pipeline/scrapers/{web-scrapers,api-connectors,file-processors}
    mkdir -p pipeline/processors/{video-analysis,metrics-calculation,quality-assessment}
    mkdir -p pipeline/generators/{unreal-engine,unity,omniverse,custom-models}
    mkdir -p pipeline/validators/{quality-metrics,compliance-checks,performance-tests}
    mkdir -p pipeline/distributors/{streaming-apis,batch-delivery,real-time-feeds}
    
    print_step "Creating vertical-specific directories..."
    
    # Vertical-Specific Modules
    mkdir -p verticals/autonomous-vehicles/{scenarios,validators,metrics}
    mkdir -p verticals/robotics/{environments,tasks,benchmarks}
    mkdir -p verticals/smart-cities/{urban-models,traffic-simulation,iot-integration}
    mkdir -p verticals/gaming/{procedural-generation,asset-management,performance-optimization}
    mkdir -p verticals/healthcare/{medical-scenarios,privacy-compliance,regulatory-validation}
    mkdir -p verticals/manufacturing/{factory-simulation,safety-scenarios,process-optimization}
    mkdir -p verticals/retail/{customer-behavior,store-layouts,inventory-simulation}
    
    print_step "Creating integration directories..."
    
    # Integration Layers
    mkdir -p integrations/{mcp-protocol,rest-apis,graphql-apis,grpc-services,webhooks,kafka-streams,websocket-feeds}
    
    print_step "Creating SDK directories..."
    
    # Client SDKs
    mkdir -p sdks/{python-sdk,javascript-sdk,go-sdk,rust-sdk,cli-tools}/{src,tests,examples,docs}
    
    print_step "Creating infrastructure directories..."
    
    # Infrastructure
    mkdir -p infrastructure/terraform/{modules,environments,scripts}
    mkdir -p infrastructure/kubernetes/{manifests,helm-charts,operators}
    mkdir -p infrastructure/docker/{services,base-images,compose-files}
    mkdir -p infrastructure/monitoring/{prometheus,grafana,alerting}
    mkdir -p infrastructure/logging/{elasticsearch,logstash,kibana}
    mkdir -p infrastructure/security/{rbac,policies,compliance}
    
    print_step "Creating configuration directories..."
    
    # Configuration Management
    mkdir -p config/environments/{development,staging,production}
    mkdir -p config/secrets/{vault-configs,key-management}
    mkdir -p config/{feature-flags,quality-thresholds,vertical-specific}
    
    print_step "Creating data management directories..."
    
    # Data Management
    mkdir -p data/schemas/{video-metadata,quality-metrics,validation-results}
    mkdir -p data/samples/{reference-videos,test-datasets,benchmarks}
    mkdir -p data/{migrations,seeds}
    
    print_step "Creating QA directories..."
    
    # Quality Assurance
    mkdir -p qa/test-suites/{unit-tests,integration-tests,e2e-tests}
    mkdir -p qa/performance-tests/{load-testing,stress-testing,capacity-planning}
    mkdir -p qa/quality-gates/{code-quality,security-scans,compliance-checks}
    mkdir -p qa/benchmarks/{industry-standards,custom-metrics,validation-frameworks}
    
    print_step "Creating documentation directories..."
    
    # Documentation
    mkdir -p docs/architecture/{system-design,api-specifications,deployment-guides}
    mkdir -p docs/user-guides/{getting-started,tutorials,best-practices}
    mkdir -p docs/developer-guides/{setup-instructions,contribution-guidelines,troubleshooting}
    mkdir -p docs/compliance/{privacy-policies,security-documentation,regulatory-compliance}
    
    print_step "Creating example directories..."
    
    # Examples and Demos
    mkdir -p examples/use-cases/{autonomous-driving,robotics-training,smart-city-planning}
    mkdir -p examples/integrations/{mcp-integration,cloud-deployment,edge-computing}
    mkdir -p examples/benchmarks/{performance-comparison,quality-validation,scalability-tests}
    
    print_step "Creating utility directories..."
    
    # Scripts and Utilities
    mkdir -p scripts/{setup,deployment,data-management,monitoring,backup-restore}
    
    print_step "Creating CI/CD directories..."
    
    # CI/CD
    mkdir -p .github/workflows
    mkdir -p .gitlab-ci
    mkdir -p jenkins/pipelines
    
    print_step "Creating storage directories..."
    
    # Storage and Cache
    mkdir -p storage/{object-store-configs,database-schemas,cache-configurations}
    
    # Create runtime directories
    mkdir -p {data/raw,data/processed,data/generated,logs,tmp}
    
    print_success "Directory structure created successfully!"
}

# Create core configuration files
create_core_files() {
    print_header "Creating Core Configuration Files"
    
    print_step "Creating README.md..."
    
    cat > "README.md" << 'EOF'
# ðŸŽ¬ Enterprise Video Synthesis Pipeline

A comprehensive platform for generating realistic synthetic video data through real-world data analysis, quality validation, and multi-vertical delivery.

## ðŸŽ¯ Overview

This platform provides end-to-end capabilities for:
- **Real-world data ingestion** from multiple sources (web scraping, APIs, uploads, streams)
- **Comprehensive metrics extraction** for quality benchmarking and analysis
- **Multi-engine synthetic video generation** using Unreal Engine, Unity, NVIDIA Omniverse
- **Rigorous quality validation** against real-world standards and compliance requirements
- **Multi-channel delivery** for ML/LLM training, agentic AI testing, and validation

## ðŸ—ï¸ Architecture

- **Microservices-based** architecture for scalability and maintainability
- **Multi-vertical support** for Autonomous Vehicles, Robotics, Smart Cities, Gaming, Healthcare, Manufacturing, Retail
- **Quality-first approach** with comprehensive validation against real-world benchmarks
- **Enterprise-grade security** and compliance (GDPR, HIPAA, industry standards)
- **Cloud-native deployment** with Kubernetes and comprehensive monitoring

## ðŸš€ Quick Start

```bash
# Setup development environment
./scripts/setup/dev-environment.sh

# Deploy local stack
make deploy

# Run example pipeline
./examples/use-cases/autonomous-driving/run-pipeline.sh

# Check health
./scripts/deployment/health-check.sh
```

## ðŸ“Š Quality Benchmarks

- **Label Accuracy**: >92% (exceeds industry standard)
- **Frame Lag**: <300ms (real-time capability)
- **PSNR**: >25 dB (high visual quality)
- **SSIM**: >0.8 (structural similarity)
- **Privacy Score**: 100% GDPR compliance
- **Bias Detection**: <0.1 threshold

## ðŸ¢ Supported Verticals

| Vertical | Use Cases | Quality Requirements |
|----------|-----------|---------------------|
| ðŸš— **Autonomous Vehicles** | Traffic scenarios, weather conditions, edge cases | 95% object accuracy, <100ms latency |
| ðŸ¤– **Robotics** | Manipulation tasks, HRI, industrial automation | 98% physics accuracy, Â±0.1mm precision |
| ðŸ™ï¸ **Smart Cities** | Urban planning, traffic optimization, IoT simulation | 100% privacy, 10K+ agent simulation |
| ðŸŽ® **Gaming** | Procedural content, NPC behavior, performance optimization | AAA visual quality, 60+ FPS |
| ðŸ¥ **Healthcare** | Medical scenarios, privacy-compliant training | HIPAA compliance, 99% medical accuracy |
| ðŸ­ **Manufacturing** | Factory simulation, safety scenarios, process optimization | 99.9% safety, Â±0.1mm precision |
| ðŸ›’ **Retail** | Customer behavior, store layouts, inventory simulation | Privacy-compliant, behavioral accuracy |

## ðŸ”Œ Integration Methods

- **REST APIs**: Standard HTTP endpoints for batch processing
- **GraphQL**: Flexible query interface for custom requirements
- **gRPC**: High-performance streaming for real-time applications
- **WebSockets**: Bidirectional communication for interactive apps
- **Kafka Streams**: Real-time data streaming and processing
- **Webhooks**: Event-driven notifications and updates
- **MCP Protocol**: Model Context Protocol for agentic AI integration
- **Native SDKs**: Python, JavaScript, Go, Rust client libraries

## ðŸ“š Documentation

- [Architecture Overview](docs/architecture/system-design/overview.md)
- [Getting Started Guide](docs/user-guides/getting-started.md)
- [API Documentation](docs/architecture/api-specifications/)
- [Deployment Guide](docs/architecture/deployment-guides/)
- [Developer Setup](docs/developer-guides/setup-instructions.md)

## ðŸ”§ Development

See [Developer Guide](docs/developer-guides/setup-instructions.md) for detailed setup instructions.

## ðŸ¤ Contributing

Please read our [Contribution Guidelines](docs/developer-guides/contribution-guidelines.md).

## ðŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

## ðŸŽ¯ Enterprise Features

- **Scalable Architecture**: Kubernetes-native with auto-scaling
- **Multi-Cloud Support**: AWS, GCP, Azure deployment options
- **Security**: Enterprise-grade security with RBAC and audit trails
- **Monitoring**: Comprehensive observability with Prometheus and Grafana
- **Compliance**: Built-in GDPR, HIPAA, and industry compliance
- **Performance**: Sub-second response times with global CDN distribution

## ðŸ“ž Support

For enterprise support and licensing, contact: enterprise@videosynth.ai
EOF

    print_step "Creating docker-compose.yml..."
    
    cat > "docker-compose.yml" << 'EOF'
version: '3.8'

services:
  # Core Services
  orchestration-service:
    build:
      context: ./services/orchestration-service
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - SERVICE_NAME=orchestration-service
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/video_pipeline
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - redis
      - postgres
      - kafka
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  ingestion-service:
    build:
      context: ./services/ingestion-service
      dockerfile: Dockerfile
    ports:
      - "8081:8080"
    environment:
      - SERVICE_NAME=ingestion-service
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/video_pipeline
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
    depends_on:
      - redis
      - postgres
      - minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  metrics-extraction-service:
    build:
      context: ./services/metrics-extraction-service
      dockerfile: Dockerfile
    ports:
      - "8082:8080"
    environment:
      - SERVICE_NAME=metrics-extraction-service
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/video_pipeline
      - OPENCV_ENABLED=true
      - FFMPEG_ENABLED=true
    depends_on:
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  generation-service:
    build:
      context: ./services/generation-service
      dockerfile: Dockerfile
    ports:
      - "8083:8080"
    environment:
      - SERVICE_NAME=generation-service
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/video_pipeline
      - UNREAL_ENGINE_PATH=/opt/unreal-engine
      - UNITY_PATH=/opt/unity
      - OMNIVERSE_URL=http://omniverse:8080
    depends_on:
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  validation-service:
    build:
      context: ./services/validation-service
      dockerfile: Dockerfile
    ports:
      - "8084:8080"
    environment:
      - SERVICE_NAME=validation-service
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/video_pipeline
      - MIN_LABEL_ACCURACY=0.92
      - MAX_FRAME_LAG_MS=300
    depends_on:
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  delivery-service:
    build:
      context: ./services/delivery-service
      dockerfile: Dockerfile
    ports:
      - "8085:8080"
    environment:
      - SERVICE_NAME=delivery-service
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/video_pipeline
      - CDN_ENDPOINT=http://cloudfront:8080
    depends_on:
      - redis
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Infrastructure Services
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=video_pipeline
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./storage/database-schemas:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 3

  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    ports:
      - "2181:2181"
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring Services
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructure/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./infrastructure/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources

  # Logging Services
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  redis_data:
  postgres_data:
  minio_data:
  zookeeper_data:
  prometheus_data:
  grafana_data:
  elasticsearch_data:

networks:
  default:
    name: video-pipeline-network
    driver: bridge
EOF

    print_step "Creating Makefile..."
    
    cat > "Makefile" << 'EOF'
.PHONY: help setup build test deploy clean start stop restart logs status

# Default target
help:
	@echo "ðŸŽ¬ Enterprise Video Synthesis Pipeline"
	@echo ""
	@echo "Available targets:"
	@echo "  setup     - Setup development environment"
	@echo "  build     - Build all services"
	@echo "  test      - Run all tests"
	@echo "  deploy    - Deploy to local environment"
	@echo "  start     - Start all services"
	@echo "  stop      - Stop all services"
	@echo "  restart   - Restart all services"
	@echo "  logs      - Show logs for all services"
	@echo "  status    - Show status of all services"
	@echo "  clean     - Clean up resources"

setup:
	@echo "ðŸ”§ Setting up development environment..."
	./scripts/setup/dev-environment.sh

build:
	@echo "ðŸ—ï¸ Building all services..."
	docker-compose build

test:
	@echo "ðŸ§ª Running tests..."
	./scripts/setup/run-tests.sh

deploy: build
	@echo "ðŸš€ Deploying local stack..."
	./scripts/deployment/local-deploy.sh

start:
	@echo "â–¶ï¸ Starting all services..."
	docker-compose up -d

stop:
	@echo "â¹ï¸ Stopping all services..."
	docker-compose down

restart: stop start

logs:
	@echo "ðŸ“‹ Showing logs..."
	docker-compose logs -f

status:
	@echo "ðŸ“Š Service status..."
	docker-compose ps
	@echo ""
	@echo "ðŸ” Running health checks..."
	./scripts/deployment/health-check.sh

clean:
	@echo "ðŸ§¹ Cleaning up..."
	docker-compose down -v
	docker system prune -f
	@echo "âœ… Cleanup complete"

# Development helpers
dev-start:
	@echo "ðŸ”§ Starting development environment..."
	docker-compose up -d redis postgres minio kafka zookeeper
	@echo "âœ… Infrastructure services started"

dev-stop:
	@echo "ðŸ”§ Stopping development environment..."
	docker-compose down

# Production helpers
prod-deploy:
	@echo "ðŸš€ Deploying to production..."
	kubectl apply -f infrastructure/kubernetes/manifests/

prod-status:
	@echo "ðŸ“Š Production status..."
	kubectl get pods -n video-pipeline
EOF

    print_step "Creating requirements.txt..."
    
    cat > "requirements.txt" << 'EOF'
# Core Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
sqlalchemy==2.0.23
alembic==1.12.1

# Async and Queue Processing
celery==5.3.4
redis==5.0.1
kafka-python==2.0.2
asyncio-mqtt==0.13.0

# Video Processing and Computer Vision
opencv-python==4.8.1
ffmpeg-python==0.2.0
pillow==10.1.0
numpy==1.25.2
scikit-image==0.22.0

# Machine Learning and AI
torch==2.1.1
torchvision==0.16.1
transformers==4.35.2
scikit-learn==1.3.2
tensorflow==2.14.0

# Database
asyncpg==0.29.0
psycopg2-binary==2.9.9

# Object Storage and Cloud
boto3==1.34.0
minio==7.2.0
azure-storage-blob==12.19.0
google-cloud-storage==2.10.0

# Monitoring and Observability
prometheus-client==0.19.0
structlog==23.2.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-fastapi==0.42b0

# HTTP and API
httpx==0.25.2
aiohttp==3.9.1
websockets==12.0
requests==2.31.0

# Data Validation and Serialization
marshmallow==3.20.1
jsonschema==4.19.2

# Configuration and Environment
python-dotenv==1.0.0
click==8.1.7
typer==0.9.0

# Security and Authentication
cryptography==41.0.7
PyJWT==2.8.0
passlib[bcrypt]==1.7.4

# Image and Video Quality Metrics
lpips==0.1.4
pytorch-fid==0.3.0

# Testing and Development
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
pytest-mock==3.12.0
factory-boy==3.3.0

# Code Quality
black==23.11.0
flake8==6.1.0
mypy==1.7.1
isort==5.12.0
pre-commit==3.5.0

# Documentation
mkdocs==1.5.3
mkdocs-material==9.4.7
mkdocs-mermaid2-plugin==1.1.1

# Utilities
python-multipart==0.0.6
python-slugify==8.0.1
humanize==4.8.0
EOF

    print_step "Creating .env.example..."
    
    cat > ".env.example" << 'EOF'
# Environment Configuration
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO
SERVICE_VERSION=1.0.0

# Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=video_pipeline
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
DATABASE_URL=postgresql://postgres:password@localhost:5432/video_pipeline

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_URL=redis://localhost:6379/0

# Object Storage Configuration
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=video-pipeline
MINIO_SECURE=false

# Message Queue Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_PREFIX=video_pipeline

# Video Generation Configuration
UNREAL_ENGINE_PATH=/opt/unreal-engine
UNITY_PATH=/opt/unity
OMNIVERSE_URL=http://localhost:8080

# Quality Thresholds
MIN_LABEL_ACCURACY=0.92
MAX_FRAME_LAG_MS=300
MIN_SEMANTIC_CONSISTENCY=0.85
MIN_OBJECT_DETECTION_PRECISION=0.88

# API Keys for External Services
KAGGLE_API_KEY=your_kaggle_api_key_here
KAGGLE_USERNAME=your_kaggle_username_here

# AWS Configuration (if using AWS services)
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-west-2
AWS_S3_BUCKET=your-s3-bucket

# Google Cloud Configuration (if using GCP services)
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
GCP_PROJECT_ID=your-gcp-project-id
GCP_STORAGE_BUCKET=your-gcs-bucket

# Azure Configuration (if using Azure services)
AZURE_STORAGE_CONNECTION_STRING=your_azure_connection_string
AZURE_STORAGE_CONTAINER=your-azure-container

# Monitoring Configuration
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
GRAFANA_ADMIN_PASSWORD=admin

# Security Configuration
SECRET_KEY=your-super-secret-key-change-this-in-production
JWT_SECRET_KEY=your-jwt-secret-key
JWT_ALGORITHM=HS256
JWT_EXPIRATION_HOURS=24

# API Rate Limiting
RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_BURST=20

# Video Processing Configuration
MAX_VIDEO_SIZE_GB=10
MAX_PROCESSING_TIME_MINUTES=60
SUPPORTED_VIDEO_FORMATS=mp4,webm,avi,mov

# Compliance Configuration
GDPR_COMPLIANCE_ENABLED=true
HIPAA_COMPLIANCE_ENABLED=false
AUDIT_LOGGING_ENABLED=true

# Performance Configuration
MAX_CONCURRENT_JOBS=10
WORKER_POOL_SIZE=4
CACHE_TTL_SECONDS=3600

# Notification Configuration
EMAIL_SMTP_HOST=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_USERNAME=your-email@gmail.com
EMAIL_PASSWORD=your-email-password
SLACK_WEBHOOK_URL=https://hooks.slack.com/your-webhook-url

# Development Configuration
DEV_MODE=true
HOT_RELOAD=true
AUTO_MIGRATION=true
SEED_DATABASE=true
EOF

    print_step "Creating .gitignore..."
    
    cat > ".gitignore" << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
env/
ENV/
env.bak/
venv.bak/

# Environment Variables
.env
.env.local
.env.development
.env.production
.env.staging

# IDE and Editors
.vscode/
.idea/
*.swp
*.swo
*~
.project
.pydevproject

# Operating System
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Runtime Data
pids
*.pid
*.seed
*.pid.lock

# Data Directories
data/raw/
data/processed/
data/generated/
data/uploads/
tmp/
temp/

# Database
*.db
*.sqlite
*.sqlite3

# Docker
*.tar
.dockerignore

# Coverage Reports
.coverage
htmlcov/
coverage.xml
*.cover
*.py,cover
.coverage.*

# Testing
.pytest_cache/
.tox/
.nox/
.cache/
nosetests.xml
coverage.xml

# Documentation
docs/_build/
site/

# Node.js (for frontend components)
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*

# Dependency Directories
jspm_packages/

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variables file
.env.test

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Secrets and Keys
*.pem
*.key
*.crt
*.p12
*.pfx
secrets/
keys/

# Terraform
*.tfstate
*.tfstate.*
.terraform/
.terraform.lock.hcl

# Kubernetes
*.kubeconfig

# Local configuration
local.yaml
local.json

# Backup files
*.bak
*.backup

# Video files (large)
*.mp4
*.avi
*.mkv
*.mov
*.wmv
*.flv
*.webm

# Model files (large)
*.pth
*.pt
*.h5
*.pb
*.onnx

# Jupyter Notebook
.ipynb_checkpoints

# pyenv
.python-version

# pipenv
Pipfile.lock

# Poetry
poetry.lock

# Celery
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/
EOF

    print_step "Creating LICENSE..."
    
    cat > "LICENSE" << 'EOF'
Apache License
Version 2.0, January 2004
http://www.apache.org/licenses/

TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

1. Definitions.

   "License" shall mean the terms and conditions for use, reproduction,
   and distribution as defined by Sections 1 through 9 of this document.

   "Licensor" shall mean the copyright owner or entity granting the License.

   "Legal Entity" shall mean the union of the acting entity and all
   other entities that control, are controlled by, or are under common
   control with that entity. For the purposes of this definition,
   "control" means (i) the power, direct or indirect, to cause the
   direction or management of such entity, whether by contract or
   otherwise, or (ii) ownership of fifty percent (50%) or more of the
   outstanding shares, or (iii) beneficial ownership of such entity.

   "You" (or "Your") shall mean an individual or Legal Entity
   exercising permissions granted by this License.

   "Source" form shall mean the preferred form for making modifications,
   including but not limited to software source code, documentation
   source, and configuration files.

   "Object" form shall mean any form resulting from mechanical
   transformation or translation of a Source form, including but
   not limited to compiled object code, generated documentation,
   and conversions to other media types.

   "Work" shall mean the work of authorship, whether in Source or
   Object form, made available under the License, as indicated by a
   copyright notice that is included in or attached to the work
   (which shall not include communication that is conspicuously
   marked or otherwise designated in writing by the copyright owner
   as "Not a Contribution").

   "Derivative Works" shall mean any work, whether in Source or Object
   form, that is based upon (or derived from) the Work and for which the
   editorial revisions, annotations, elaborations, or other modifications
   represent, as a whole, an original work of authorship. For the purposes
   of this License, Derivative Works shall not include works that remain
   separable from, or merely link (or bind by name) to the interfaces of,
   the Work and derivative works thereof.

   "Contribution" shall mean any work of authorship, including
   the original version of the Work and any modifications or additions
   to that Work or Derivative Works thereof, that is intentionally
   submitted to Licensor for inclusion in the Work by the copyright owner
   or by an individual or Legal Entity authorized to submit on behalf of
   the copyright owner. For the purpose of this definition, "submitted"
   means any form of electronic, verbal, or written communication sent
   to the Licensor or its representatives, including but not limited to
   communication on electronic mailing lists, source code control
   systems, and issue tracking systems that are managed by, or on behalf
   of, the Licensor for the purpose of discussing and improving the Work,
   but excluding communication that is conspicuously marked or otherwise
   designated in writing by the copyright owner as "Not a Contribution".

2. Grant of Copyright License. Subject to the terms and conditions of
   this License, each Contributor hereby grants to You a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   copyright license to use, reproduce, modify, distribute, and perform
   the Work and to prepare Derivative Works based upon the Work.

3. Grant of Patent License. Subject to the terms and conditions of
   this License, each Contributor hereby grants to You a perpetual,
   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
   (except as stated in this section) patent license to make, have made,
   use, offer to sell, sell, import, and otherwise transfer the Work,
   where such license applies only to those patent claims licensable
   by such Contributor that are necessarily infringed by their
   Contribution(s) alone or by combination of their Contribution(s)
   with the Work to which such Contribution(s) was submitted.

Copyright 2024 Enterprise Video Synthesis Pipeline Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
EOF

    print_success "Core configuration files created!"
}

# Create service files
create_service_files() {
    print_header "Creating Service Files"
    
    print_step "Creating ingestion service..."
    
    # Ingestion Service
    cat > "services/ingestion-service/src/main.py" << 'EOF'
"""
Video Data Ingestion Service
Handles scraping and ingestion of real-world video data from multiple sources
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from pydantic import BaseModel, HttpUrl
from typing import List, Optional, Dict, Any
import asyncio
import logging
import uvicorn
from datetime import datetime
import hashlib
import json

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Video Ingestion Service",
    description="Enterprise video data ingestion from multiple sources",
    version="1.0.0"
)

class DataSource(BaseModel):
    source_type: str  # web, api, upload, stream
    url: Optional[HttpUrl] = None
    credentials: Optional[Dict[str, str]] = None
    scraping_config: Optional[Dict[str, Any]] = None
    quality_filters: Optional[Dict[str, float]] = None

class IngestionJob(BaseModel):
    job_id: str
    source: DataSource
    status: str
    progress: float
    metadata: Dict[str, Any]
    created_at: datetime
    updated_at: datetime

# In-memory storage for demo (use Redis/DB in production)
active_jobs: Dict[str, IngestionJob] = {}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "ingestion-service", "version": "1.0.0"}

@app.post("/api/v1/ingest/start")
async def start_ingestion(source: DataSource, background_tasks: BackgroundTasks):
    """Start data ingestion from specified source"""
    job_id = f"ingest_{hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]}"
    
    job = IngestionJob(
        job_id=job_id,
        source=source,
        status="queued",
        progress=0.0,
        metadata={"source_type": source.source_type},
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    
    active_jobs[job_id] = job
    background_tasks.add_task(process_ingestion, job_id)
    
    logger.info(f"Started ingestion job {job_id} for source type {source.source_type}")
    return {"job_id": job_id, "status": "queued"}

@app.get("/api/v1/ingest/status/{job_id}")
async def get_job_status(job_id: str):
    """Get ingestion job status"""
    if job_id not in active_jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return active_jobs[job_id]

@app.get("/api/v1/ingest/jobs")
async def list_jobs():
    """List all ingestion jobs"""
    return {"jobs": list(active_jobs.values()), "total": len(active_jobs)}

@app.post("/api/v1/ingest/upload")
async def upload_video(file: UploadFile = File(...), metadata: Optional[str] = None):
    """Upload video file directly"""
    if not file.filename.lower().endswith(('.mp4', '.webm', '.avi', '.mov')):
        raise HTTPException(status_code=400, detail="Unsupported video format")
    
    # Process upload
    job_id = f"upload_{hashlib.md5(file.filename.encode()).hexdigest()[:8]}"
    
    # Save file and create job
    job = IngestionJob(
        job_id=job_id,
        source=DataSource(source_type="upload", url=None),
        status="processing",
        progress=0.0,
        metadata={"filename": file.filename, "size": 0},
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    
    active_jobs[job_id] = job
    logger.info(f"Processing upload {file.filename} as job {job_id}")
    
    return {"job_id": job_id, "filename": file.filename, "status": "processing"}

async def process_ingestion(job_id: str):
    """Background task to process ingestion"""
    job = active_jobs[job_id]
    
    try:
        job.status = "processing"
        job.updated_at = datetime.now()
        
        # Simulate processing steps
        processing_steps = [
            ("Initializing", 10),
            ("Downloading", 30),
            ("Validating", 60),
            ("Processing", 80),
            ("Storing", 90),
            ("Completing", 100)
        ]
        
        for step_name, progress in processing_steps:
            await asyncio.sleep(2)  # Simulate work
            job.progress = progress
            job.metadata["current_step"] = step_name
            job.updated_at = datetime.now()
            logger.info(f"Job {job_id}: {step_name} ({progress}%)")
        
        job.status = "completed"
        job.metadata["completed_at"] = datetime.now().isoformat()
        
    except Exception as e:
        job.status = "failed"
        job.metadata["error"] = str(e)
        logger.error(f"Job {job_id} failed: {e}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
EOF

    # Create Dockerfile for ingestion service
    cat > "services/ingestion-service/Dockerfile" << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY src/ ./src/

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Expose port
EXPOSE 8080

# Run the application
CMD ["python", "src/main.py"]
EOF

    cat > "services/ingestion-service/requirements.txt" << 'EOF'
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6
httpx==0.25.2
aiofiles==23.2.0
EOF

    print_step "Creating orchestration service..."
    
    # Orchestration Service
    cat > "services/orchestration-service/src/main.py" << 'EOF'
"""
Orchestration Service
Coordinates the entire video synthesis pipeline workflow
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
import logging
import uvicorn
from datetime import datetime
import json
import httpx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Orchestration Service",
    description="Pipeline orchestration and workflow management",
    version="1.0.0"
)

class PipelineRequest(BaseModel):
    vertical: str
    data_sources: List[Dict[str, Any]]
    generation_config: Dict[str, Any]
    quality_requirements: Dict[str, float]
    delivery_config: Dict[str, Any]

class PipelineStatus(BaseModel):
    pipeline_id: str
    status: str
    current_stage: str
    progress: float
    stages: Dict[str, Dict[str, Any]]
    created_at: datetime
    updated_at: datetime

# In-memory storage
active_pipelines: Dict[str, PipelineStatus] = {}

# Service endpoints
SERVICES = {
    "ingestion": "http://ingestion-service:8080",
    "metrics": "http://metrics-extraction-service:8080", 
    "generation": "http://generation-service:8080",
    "validation": "http://validation-service:8080",
    "delivery": "http://delivery-service:8080"
}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "orchestration-service", "version": "1.0.0"}

@app.post("/api/v1/pipeline/start")
async def start_pipeline(request: PipelineRequest, background_tasks: BackgroundTasks):
    """Start complete video synthesis pipeline"""
    pipeline_id = f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(str(request)) % 10000:04d}"
    
    pipeline = PipelineStatus(
        pipeline_id=pipeline_id,
        status="started",
        current_stage="ingestion",
        progress=0.0,
        stages={
            "ingestion": {"status": "pending", "progress": 0.0},
            "metrics_extraction": {"status": "pending", "progress": 0.0},
            "generation": {"status": "pending", "progress": 0.0},
            "validation": {"status": "pending", "progress": 0.0},
            "delivery": {"status": "pending", "progress": 0.0}
        },
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    
    active_pipelines[pipeline_id] = pipeline
    background_tasks.add_task(execute_pipeline, pipeline_id, request)
    
    logger.info(f"Started pipeline {pipeline_id} for vertical {request.vertical}")
    return {"pipeline_id": pipeline_id, "status": "started"}

@app.get("/api/v1/pipeline/status/{pipeline_id}")
async def get_pipeline_status(pipeline_id: str):
    """Get pipeline status"""
    if pipeline_id not in active_pipelines:
        raise HTTPException(status_code=404, detail="Pipeline not found")
    
    return active_pipelines[pipeline_id]

@app.get("/api/v1/pipeline/list")
async def list_pipelines():
    """List all pipelines"""
    return {"pipelines": list(active_pipelines.values()), "total": len(active_pipelines)}

async def execute_pipeline(pipeline_id: str, request: PipelineRequest):
    """Execute the complete pipeline workflow"""
    pipeline = active_pipelines[pipeline_id]
    
    try:
        # Stage 1: Ingestion
        await execute_stage(pipeline, "ingestion", "Ingesting data sources", 
                          lambda: call_service("ingestion", "/api/v1/ingest/start", request.data_sources))
        
        # Stage 2: Metrics Extraction
        await execute_stage(pipeline, "metrics_extraction", "Extracting quality metrics",
                          lambda: call_service("metrics", "/api/v1/metrics/extract", {}))
        
        # Stage 3: Generation
        await execute_stage(pipeline, "generation", "Generating synthetic video",
                          lambda: call_service("generation", "/api/v1/generate/video", request.generation_config))
        
        # Stage 4: Validation
        await execute_stage(pipeline, "validation", "Validating output quality",
                          lambda: call_service("validation", "/api/v1/validate", request.quality_requirements))
        
        # Stage 5: Delivery
        await execute_stage(pipeline, "delivery", "Delivering final output",
                          lambda: call_service("delivery", "/api/v1/deliver", request.delivery_config))
        
        pipeline.status = "completed"
        pipeline.current_stage = "completed"
        pipeline.progress = 100.0
        
    except Exception as e:
        pipeline.status = "failed"
        pipeline.stages[pipeline.current_stage]["error"] = str(e)
        logger.error(f"Pipeline {pipeline_id} failed at stage {pipeline.current_stage}: {e}")
    
    pipeline.updated_at = datetime.now()

async def execute_stage(pipeline: PipelineStatus, stage_name: str, description: str, task_func):
    """Execute a pipeline stage"""
    pipeline.current_stage = stage_name
    pipeline.stages[stage_name]["status"] = "running"
    pipeline.stages[stage_name]["description"] = description
    pipeline.updated_at = datetime.now()
    
    logger.info(f"Pipeline {pipeline.pipeline_id}: Starting {stage_name}")
    
    # Simulate stage execution
    for progress in range(0, 101, 20):
        await asyncio.sleep(1)
        pipeline.stages[stage_name]["progress"] = progress
        stage_progress = sum(stage["progress"] for stage in pipeline.stages.values()) / len(pipeline.stages)
        pipeline.progress = stage_progress
        pipeline.updated_at = datetime.now()
    
    pipeline.stages[stage_name]["status"] = "completed"
    pipeline.stages[stage_name]["progress"] = 100.0
    
    logger.info(f"Pipeline {pipeline.pipeline_id}: Completed {stage_name}")

async def call_service(service_name: str, endpoint: str, data: Any):
    """Call another service"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(f"{SERVICES[service_name]}{endpoint}", json=data)
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error(f"Failed to call {service_name}{endpoint}: {e}")
        # For demo, don't fail - just log
        return {"status": "simulated"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
EOF

    # Create Dockerfile for orchestration service
    cat > "services/orchestration-service/Dockerfile" << 'EOF'
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/

RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

EXPOSE 8080

CMD ["python", "src/main.py"]
EOF

    cat > "services/orchestration-service/requirements.txt" << 'EOF'
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
httpx==0.25.2
EOF

    print_success "Service files created!"
}

# Create script files
create_script_files() {
    print_header "Creating Script Files"
    
    print_step "Creating development environment setup script..."
    
    cat > "scripts/setup/dev-environment.sh" << 'EOF'
#!/bin/bash

# Development Environment Setup Script

set -e

echo "ðŸš€ Setting up Enterprise Video Synthesis Pipeline development environment..."

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# Check prerequisites
echo "âœ… Checking prerequisites..."

# Check Docker
if ! command -v docker &> /dev/null; then
    print_error "Docker is required but not installed. Please install Docker first."
    echo "Visit: https://docs.docker.com/get-docker/"
    exit 1
fi

# Check Docker Compose
if ! command -v docker-compose &> /dev/null; then
    print_error "Docker Compose is required but not installed. Please install Docker Compose first."
    echo "Visit: https://docs.docker.com/compose/install/"
    exit 1
fi

# Check Python
if ! command -v python3 &> /dev/null; then
    print_warning "Python 3 is recommended for local development."
    print_info "You can still use the Docker-based development environment."
else
    print_success "Python 3 found: $(python3 --version)"
fi

print_success "Prerequisites check passed!"

# Setup Python virtual environment (optional)
if command -v python3 &> /dev/null; then
    echo "ðŸ Setting up Python virtual environment..."
    
    if [ ! -d "venv" ]; then
        python3 -m venv venv
        print_success "Created Python virtual environment"
    else
        print_info "Virtual environment already exists"
    fi
    
    # Activate and install dependencies
    source venv/bin/activate
    pip install --upgrade pip
    pip install -r requirements.txt
    print_success "Installed Python dependencies"
else
    print_warning "Skipping Python setup (Python not found)"
fi

# Copy environment configuration
echo "âš™ï¸ Setting up environment configuration..."
if [ ! -f .env ]; then
    cp .env.example .env
    print_success "Created .env file from template"
    print_warning "Please update .env file with your specific configuration"
else
    print_info ".env file already exists"
fi

# Create necessary directories
echo "ðŸ“ Creating data directories..."
mkdir -p {data/raw,data/processed,data/generated,logs,tmp}
print_success "Created data directories"

# Initialize Docker images
echo "ðŸ³ Pulling Docker images..."
docker-compose pull
print_success "Docker images pulled"

# Setup git hooks (if in git repo)
if [ -d .git ]; then
    echo "ðŸ”§ Setting up git hooks..."
    if [ -f scripts/setup/pre-commit-hook.sh ]; then
        cp scripts/setup/pre-commit-hook.sh .git/hooks/pre-commit
        chmod +x .git/hooks/pre-commit
        print_success "Git hooks installed"
    fi
fi

# Create sample data (optional)
echo "ðŸ“Š Setting up sample data..."
mkdir -p data/samples
# Add sample files here if needed

print_success "Development environment setup complete!"
echo ""
echo "ðŸš€ Next steps:"
echo "  1. Update .env file with your configuration"
echo "  2. Run 'make deploy' to start the local stack"
echo "  3. Visit http://localhost:8080 for the API documentation"
echo "  4. Check service health with 'make status'"
echo ""
echo "ðŸ“š Available commands:"
echo "  make help     - Show all available commands"
echo "  make build    - Build all services"
echo "  make start    - Start all services"
echo "  make stop     - Stop all services"
echo "  make logs     - View service logs"
echo "  make status   - Check service status"
EOF

    chmod +x scripts/setup/dev-environment.sh

    print_step "Creating deployment script..."
    
    cat > "scripts/deployment/local-deploy.sh" << 'EOF'
#!/bin/bash

# Local Deployment Script

set -e

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

print_step() {
    echo -e "${YELLOW}ðŸ”¨ $1${NC}"
}

echo "ðŸš€ Deploying Enterprise Video Synthesis Pipeline locally..."

# Check if .env exists
if [ ! -f .env ]; then
    print_info "Creating .env from template..."
    cp .env.example .env
fi

# Start infrastructure services first
print_step "Starting infrastructure services..."
docker-compose up -d redis postgres minio kafka zookeeper

# Wait for services to be ready
print_step "Waiting for infrastructure services to be ready..."
sleep 15

# Check infrastructure health
print_step "Checking infrastructure health..."
max_attempts=30
attempt=0

while [ $attempt -lt $max_attempts ]; do
    if docker-compose ps | grep -q "healthy\|Up"; then
        break
    fi
    sleep 2
    attempt=$((attempt + 1))
done

# Initialize databases and storage
print_step "Initializing databases and storage..."

# Wait for PostgreSQL
until docker-compose exec -T postgres pg_isready -U postgres; do
    print_info "Waiting for PostgreSQL..."
    sleep 2
done

# Create MinIO bucket
docker-compose exec -T minio mc alias set local http://localhost:9000 minioadmin minioadmin
docker-compose exec -T minio mc mb local/video-pipeline || true

# Start application services
print_step "Starting application services..."
docker-compose up -d

# Wait for services to be healthy
print_step "Waiting for application services to be healthy..."
sleep 30

# Run health checks
print_step "Running health checks..."
./scripts/deployment/health-check.sh

print_success "Deployment complete!"
echo ""
echo "ðŸŒ Services available at:"
echo "  â€¢ API Gateway (Orchestration): http://localhost:8080"
echo "  â€¢ Ingestion Service: http://localhost:8081"
echo "  â€¢ Metrics Service: http://localhost:8082"
echo "  â€¢ Generation Service: http://localhost:8083"
echo "  â€¢ Validation Service: http://localhost:8084"
echo "  â€¢ Delivery Service: http://localhost:8085"
echo ""
echo "ðŸ”§ Management interfaces:"
echo "  â€¢ Grafana Dashboard: http://localhost:3000 (admin/admin)"
echo "  â€¢ MinIO Console: http://localhost:9001 (minioadmin/minioadmin)"
echo "  â€¢ Prometheus: http://localhost:9090"
echo "  â€¢ Kibana: http://localhost:5601"
echo ""
echo "ðŸ“Š Check status with: make status"
echo "ðŸ“‹ View logs with: make logs"
EOF

    chmod +x scripts/deployment/local-deploy.sh

    print_step "Creating health check script..."
    
    cat > "scripts/deployment/health-check.sh" << 'EOF'
#!/bin/bash

# Health Check Script

set -e

# Colors
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

echo "ðŸ” Running health checks..."

services=(
    "orchestration-service:8080:Orchestration Service"
    "ingestion-service:8081:Ingestion Service"
    "metrics-extraction-service:8082:Metrics Extraction Service"
    "generation-service:8083:Generation Service"
    "validation-service:8084:Validation Service"
    "delivery-service:8085:Delivery Service"
)

infrastructure=(
    "redis:6379:Redis"
    "postgres:5432:PostgreSQL"
    "minio:9000:MinIO"
    "kafka:9092:Kafka"
    "prometheus:9090:Prometheus"
    "grafana:3000:Grafana"
)

failed_services=()
failed_infrastructure=()

# Check application services
echo ""
print_info "Checking application services..."
for service in "${services[@]}"; do
    IFS=':' read -r service_name port display_name <<< "$service"
    
    print_info "Checking $display_name..."
    
    if curl -f -s --max-time 10 "http://localhost:$port/health" > /dev/null 2>&1; then
        print_success "$display_name is healthy"
    else
        print_error "$display_name is unhealthy"
        failed_services+=("$display_name")
    fi
done

# Check infrastructure services
echo ""
print_info "Checking infrastructure services..."
for service in "${infrastructure[@]}"; do
    IFS=':' read -r service_name port display_name <<< "$service"
    
    print_info "Checking $display_name..."
    
    case $service_name in
        "redis")
            if docker-compose exec -T redis redis-cli ping | grep -q "PONG"; then
                print_success "$display_name is healthy"
            else
                print_error "$display_name is unhealthy"
                failed_infrastructure+=("$display_name")
            fi
            ;;
        "postgres")
            if docker-compose exec -T postgres pg_isready -U postgres | grep -q "accepting connections"; then
                print_success "$display_name is healthy"
            else
                print_error "$display_name is unhealthy"
                failed_infrastructure+=("$display_name")
            fi
            ;;
        *)
            if curl -f -s --max-time 10 "http://localhost:$port" > /dev/null 2>&1; then
                print_success "$display_name is healthy"
            else
                print_error "$display_name is unhealthy"
                failed_infrastructure+=("$display_name")
            fi
            ;;
    esac
done

# Check Docker containers
echo ""
print_info "Checking Docker container status..."
docker-compose ps

# Summary
echo ""
if [ ${#failed_services[@]} -eq 0 ] && [ ${#failed_infrastructure[@]} -eq 0 ]; then
    print_success "All services are healthy! ðŸŽ‰"
    echo ""
    print_info "You can now:"
    echo "  â€¢ Visit http://localhost:8080 for API documentation"
    echo "  â€¢ Check Grafana at http://localhost:3000 (admin/admin)"
    echo "  â€¢ Access MinIO at http://localhost:9001 (minioadmin/minioadmin)"
    echo "  â€¢ Run example workflows in the examples/ directory"
    exit 0
else
    echo "âŒ Health check failed!"
    if [ ${#failed_services[@]} -gt 0 ]; then
        print_error "Failed application services: ${failed_services[*]}"
    fi
    if [ ${#failed_infrastructure[@]} -gt 0 ]; then
        print_error "Failed infrastructure services: ${failed_infrastructure[*]}"
    fi
    echo ""
    print_info "Troubleshooting:"
    echo "  â€¢ Check logs with: make logs"
    echo "  â€¢ Restart services with: make restart"
    echo "  â€¢ View container status with: docker-compose ps"
    exit 1
fi
EOF

    chmod +x scripts/deployment/health-check.sh

    print_step "Creating test runner script..."
    
    cat > "scripts/setup/run-tests.sh" << 'EOF'
#!/bin/bash

# Test Runner Script

set -e

echo "ðŸ§ª Running Enterprise Video Synthesis Pipeline tests..."

# Activate virtual environment if it exists
if [ -d "venv" ]; then
    source venv/bin/activate
    echo "âœ… Activated virtual environment"
fi

# Install test dependencies if needed
pip install pytest pytest-asyncio pytest-cov pytest-mock

# Run unit tests
echo "ðŸ”¬ Running unit tests..."
pytest qa/test-suites/unit-tests/ -v --cov=services --cov-report=html --cov-report=term

# Run integration tests (if services are running)
if curl -f -s http://localhost:8080/health > /dev/null 2>&1; then
    echo "ðŸ”— Running integration tests..."
    pytest qa/test-suites/integration-tests/ -v
else
    echo "âš ï¸  Skipping integration tests (services not running)"
    echo "   Start services with 'make deploy' to run integration tests"
fi

# Run code quality checks
echo "ðŸ“ Running code quality checks..."
if command -v black &> /dev/null; then
    black --check services/
fi

if command -v flake8 &> /dev/null; then
    flake8 services/
fi

if command -v mypy &> /dev/null; then
    mypy services/
fi

echo "âœ… All tests completed!"
EOF

    chmod +x scripts/setup/run-tests.sh

    print_success "Script files created!"
}

# Create configuration files
create_configuration_files() {
    print_header "Creating Configuration Files"
    
    print_step "Creating quality thresholds configuration..."
    
    cat > "config/quality-thresholds/default.yaml" << 'EOF'
# Quality Thresholds Configuration for Video Synthesis Pipeline

# Video Quality Metrics
video_quality:
  min_resolution: "720p"
  min_fps: 15
  max_compression_ratio: 0.8
  min_bitrate_kbps: 1000
  
  # Advanced quality metrics
  min_psnr_db: 25.0
  min_ssim: 0.8
  max_lpips: 0.3
  min_vmaf: 70.0

# Content Quality Metrics
content_quality:
  min_label_accuracy: 0.92
  max_frame_lag_ms: 300
  min_semantic_consistency: 0.85
  min_object_detection_precision: 0.88
  min_motion_consistency: 0.8
  min_temporal_coherence: 0.75

# Validation Metrics
validation_metrics:
  structural_similarity: 0.7
  perceptual_hash_similarity: 0.6
  motion_consistency: 0.8
  temporal_coherence: 0.75
  color_consistency: 0.8

# Compliance Requirements
compliance:
  privacy_score: 1.0
  bias_detection_threshold: 0.1
  ethical_compliance: true
  gdpr_compliant: true
  hipaa_compliant: false  # Enable for healthcare vertical

# Performance Requirements
performance:
  max_processing_time_minutes: 60
  max_memory_usage_gb: 16
  max_gpu_usage_percent: 85
  min_throughput_videos_per_hour: 10

# Vertical-Specific Overrides
verticals:
  autonomous_vehicles:
    content_quality:
      min_label_accuracy: 0.95
      min_object_detection_precision: 0.95
    performance:
      max_frame_lag_ms: 100
  
  healthcare:
    compliance:
      hipaa_compliant: true
      privacy_score: 1.0
    content_quality:
      min_label_accuracy: 0.99
  
  gaming:
    video_quality:
      min_fps: 60
      min_resolution: "1080p"
    performance:
      max_frame_lag_ms: 16  # 60 FPS
EOF

    print_step "Creating vertical-specific configurations..."
    
    cat > "verticals/autonomous-vehicles/config.yaml" << 'EOF'
# Autonomous Vehicles Vertical Configuration

name: "Autonomous Vehicles"
description: "Self-driving car training and testing scenarios"

# Supported Scenarios
scenarios:
  traffic:
    - highway_driving
    - urban_navigation
    - intersection_handling
    - lane_changing
    - overtaking_maneuvers
  
  weather_conditions:
    - clear_weather
    - light_rain
    - heavy_rain
    - snow
    - fog
    - ice_conditions
  
  lighting_conditions:
    - daylight
    - dusk
    - night_driving
    - dawn
    - artificial_lighting
  
  emergency_situations:
    - emergency_braking
    - obstacle_avoidance
    - pedestrian_crossing
    - animal_encounters
    - construction_zones

# Quality Requirements
quality_metrics:
  object_detection_accuracy: 0.95
  lane_detection_precision: 0.98
  traffic_sign_recognition: 0.99
  pedestrian_detection: 0.97
  vehicle_classification: 0.94

# Simulation Parameters
simulation_parameters:
  vehicle_types:
    - sedan
    - suv
    - truck
    - motorcycle
    - bus
    - bicycle
  
  road_types:
    - highway
    - urban_street
    - rural_road
    - parking_lot
    - tunnel
    - bridge
  
  traffic_density:
    - low: "0-20 vehicles/km"
    - medium: "20-50 vehicles/km"
    - high: "50-100 vehicles/km"
    - extreme: "100+ vehicles/km"

# Safety Requirements
safety_requirements:
  critical_safety_level: true
  real_time_processing: true
  edge_case_coverage: 0.9
  fail_safe_behavior: true

# Compliance Standards
compliance_standards:
  - ISO_26262  # Functional Safety
  - SAE_J3016  # Automation Levels
  - NCAP_2025  # Safety Assessment
  - GDPR       # Privacy

# Integration Partners
integration_partners:
  simulation_engines:
    - CARLA
    - AirSim
    - SUMO
    - Gazebo
  
  hardware_platforms:
    - NVIDIA_DRIVE
    - Intel_Mobileye
    - Qualcomm_Snapdragon
    - Tesla_FSD_Chip
  
  software_frameworks:
    - ROS2
    - Apollo
    - Autoware
    - OpenPilot
EOF

    cat > "verticals/robotics/config.yaml" << 'EOF'
# Robotics Vertical Configuration

name: "Robotics"
description: "Robot training and human-robot interaction scenarios"

# Supported Scenarios
scenarios:
  manipulation_tasks:
    - object_grasping
    - assembly_operations
    - sorting_tasks
    - precision_placement
    - tool_usage
  
  navigation_tasks:
    - obstacle_avoidance
    - path_planning
    - slam_scenarios
    - multi_robot_coordination
    - dynamic_environments
  
  human_robot_interaction:
    - collaborative_tasks
    - handover_scenarios
    - gesture_recognition
    - voice_interaction
    - safety_zones

# Quality Requirements
quality_metrics:
  manipulation_precision_mm: 0.1
  navigation_accuracy_cm: 1.0
  object_recognition_accuracy: 0.95
  human_detection_accuracy: 0.98
  safety_compliance: 0.99

# Robot Types
robot_types:
  industrial:
    - robotic_arms
    - assembly_robots
    - welding_robots
    - painting_robots
  
  service:
    - cleaning_robots
    - delivery_robots
    - reception_robots
    - medical_assistants
  
  mobile:
    - autonomous_mobile_robots
    - warehouse_robots
    - inspection_robots
    - search_rescue_robots

# Environment Types
environments:
  - factory_floor
  - warehouse
  - office_space
  - home_environment
  - outdoor_terrain
  - laboratory
  - hospital
  - retail_store

# Integration Requirements
integration_requirements:
  ros_compatibility: true
  real_time_control: true
  safety_certified: true
  modular_design: true
EOF

    print_step "Creating Prometheus configuration..."
    
    mkdir -p infrastructure/monitoring/prometheus
    cat > "infrastructure/monitoring/prometheus/prometheus.yml" << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'orchestration-service'
    static_configs:
      - targets: ['orchestration-service:8080']
    metrics_path: '/metrics'
    
  - job_name: 'ingestion-service'
    static_configs:
      - targets: ['ingestion-service:8080']
    metrics_path: '/metrics'
    
  - job_name: 'metrics-extraction-service'
    static_configs:
      - targets: ['metrics-extraction-service:8080']
    metrics_path: '/metrics'
    
  - job_name: 'generation-service'
    static_configs:
      - targets: ['generation-service:8080']
    metrics_path: '/metrics'
    
  - job_name: 'validation-service'
    static_configs:
      - targets: ['validation-service:8080']
    metrics_path: '/metrics'
    
  - job_name: 'delivery-service'
    static_configs:
      - targets: ['delivery-service:8080']
    metrics_path: '/metrics'
EOF

    print_success "Configuration files created!"
}

# Create documentation files
create_documentation_files() {
    print_header "Creating Documentation Files"
    
    print_step "Creating architecture overview..."
    
    cat > "docs/architecture/system-design/overview.md" << 'EOF'
# System Architecture Overview

## High-Level Architecture

The Enterprise Video Synthesis Pipeline consists of six core microservices orchestrated to provide end-to-end video generation capabilities:

### Core Services

1. **Orchestration Service** (Port 8080)
   - Coordinates the entire pipeline workflow
   - Manages inter-service communication
   - Provides main API gateway functionality
   - Handles pipeline status and monitoring

2. **Ingestion Service** (Port 8081)
   - Scrapes video data from web sources
   - Handles API integrations (Kaggle, AWS Open Data)
   - Processes file uploads
   - Manages streaming data capture

3. **Metrics Extraction Service** (Port 8082)
   - Analyzes real-world video quality
   - Extracts object detection benchmarks
   - Calculates motion and temporal metrics
   - Provides baseline quality standards

4. **Generation Service** (Port 8083)
   - Integrates with Unreal Engine, Unity, Omniverse
   - Generates synthetic video content
   - Applies real-world parameter optimization
   - Manages rendering pipelines

5. **Validation Service** (Port 8084)
   - Validates synthetic content against real-world metrics
   - Performs quality assurance testing
   - Checks compliance requirements
   - Generates improvement recommendations

6. **Delivery Service** (Port 8085)
   - Provides multiple access methods (REST, streaming, SDKs)
   - Manages content distribution
   - Handles customer authentication
   - Implements rate limiting and quotas

### Data Flow

```
Real-world Data â†’ Ingestion â†’ Metrics Extraction â†’ Generation â†’ Validation â†’ Delivery
                                    â†“
                            Quality Benchmarks â† â†’ Synthetic Video
```

### Infrastructure Components

- **Redis**: Caching and session management
- **PostgreSQL**: Metadata and configuration storage
- **MinIO**: Object storage for video files
- **Kafka**: Event streaming and messaging
- **Prometheus**: Metrics collection
- **Grafana**: Monitoring dashboards

### Quality Assurance

The pipeline implements a rigorous quality framework:

- **Real-world Baseline**: Extract metrics from authentic video data
- **Multi-dimensional Validation**: Quality, content, compliance, performance
- **Continuous Monitoring**: Real-time quality tracking
- **Adaptive Improvement**: Automatic parameter optimization

### Scalability Features

- **Microservices Architecture**: Independent scaling and deployment
- **Event-driven Processing**: Asynchronous pipeline execution
- **Cloud-native Design**: Kubernetes-ready containerization
- **Multi-cloud Support**: AWS, GCP, Azure compatibility

### Security & Compliance

- **Privacy by Design**: Built-in GDPR compliance
- **Role-based Access Control**: Granular permissions
- **Audit Logging**: Comprehensive activity tracking
- **Data Encryption**: End-to-end security

## Next Steps

- [Getting Started Guide](../../user-guides/getting-started.md)
- [API Documentation](../api-specifications/)
- [Deployment Guide](../deployment-guides/)
EOF

    print_step "Creating getting started guide..."
    
    mkdir -p docs/user-guides/getting-started
    cat > "docs/user-guides/getting-started.md" << 'EOF'
# Getting Started Guide

Welcome to the Enterprise Video Synthesis Pipeline! This guide will help you get up and running quickly.

## Prerequisites

Before you begin, ensure you have:

- **Docker** (version 20.0 or later)
- **Docker Compose** (version 2.0 or later)
- **Git** for cloning the repository
- **8GB+ RAM** for local development
- **20GB+ storage** for video processing

## Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
git clone <repository-url>
cd enterprise-video-synthesis-pipeline

# Setup development environment
./scripts/setup/dev-environment.sh
```

### 2. Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit configuration (optional for quick start)
# nano .env
```

### 3. Deploy Local Stack

```bash
# Build and deploy all services
make deploy

# Check deployment status
make status
```

### 4. Verify Installation

```bash
# Run health checks
./scripts/deployment/health-check.sh

# Access the API documentation
open http://localhost:8080/docs
```

## First Pipeline Run

### Option 1: Web Interface

1. Visit http://localhost:8080
2. Navigate to the pipeline creation interface
3. Select a vertical (e.g., "autonomous_vehicles")
4. Configure your data sources and quality requirements
5. Start the pipeline and monitor progress

### Option 2: API Call

```bash
curl -X POST "http://localhost:8080/api/v1/pipeline/start" \
  -H "Content-Type: application/json" \
  -d '{
    "vertical": "autonomous_vehicles",
    "data_sources": [
      {
        "source_type": "web",
        "url": "https://example.com/traffic-videos"
      }
    ],
    "generation_config": {
      "engine": "unreal",
      "duration_seconds": 60,
      "resolution": "1920x1080"
    },
    "quality_requirements": {
      "min_label_accuracy": 0.92,
      "max_frame_lag_ms": 300
    },
    "delivery_config": {
      "format": "mp4",
      "delivery_method": "download"
    }
  }'
```

### Option 3: SDK Usage

```python
from video_synth_sdk import VideoClient

# Initialize client
client = VideoClient(api_url="http://localhost:8080")

# Start pipeline
pipeline = client.start_pipeline(
    vertical="robotics",
    data_sources=[{"source_type": "upload", "file": "robot_task.mp4"}],
    quality_requirements={"min_precision": 0.98}
)

# Monitor progress
status = client.get_pipeline_status(pipeline.id)
print(f"Status: {status.current_stage} - {status.progress}%")
```

## Understanding the Pipeline

### Stage 1: Data Ingestion
- Scrapes or receives real-world video data
- Validates format and quality
- Stores in object storage (MinIO)

### Stage 2: Metrics Extraction
- Analyzes video quality (PSNR, SSIM, LPIPS)
- Detects objects and motion patterns
- Establishes quality benchmarks

### Stage 3: Synthetic Generation
- Generates synthetic video using selected engine
- Applies real-world parameters
- Optimizes for target vertical

### Stage 4: Validation
- Compares synthetic video against real-world metrics
- Performs compliance checks
- Generates quality report

### Stage 5: Delivery
- Provides access via multiple channels
- Implements authentication and rate limiting
- Delivers final content

## Monitoring and Debugging

### View Service Logs
```bash
# All services
make logs

# Specific service
docker-compose logs -f orchestration-service
```

### Check Service Health
```bash
# Quick health check
make status

# Detailed health check
./scripts/deployment/health-check.sh
```

### Access Monitoring Dashboards
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)

## Troubleshooting

### Common Issues

1. **Services Won't Start**
   ```bash
   # Check Docker daemon
   docker info
   
   # Restart services
   make restart
   ```

2. **Port Conflicts**
   ```bash
   # Check port usage
   netstat -tulpn | grep :8080
   
   # Modify ports in docker-compose.yml if needed
   ```

3. **Out of Memory**
   ```bash
   # Check Docker memory
   docker system df
   
   # Increase Docker memory limit to 8GB+
   ```

4. **Permission Errors**
   ```bash
   # Fix file permissions
   sudo chown -R $USER:$USER .
   chmod +x scripts/**/*.sh
   ```

## Next Steps

- Explore [API Documentation](../api-specifications/)
- Try [Example Use Cases](../../examples/)
- Learn about [Vertical Configurations](../../verticals/)
- Set up [Production Deployment](../deployment-guides/)

## Support

- Documentation: `/docs/`
- Examples: `/examples/`
- Issues: GitHub Issues
- Community: Discord/Slack
EOF

    print_success "Documentation files created!"
}

# Create example files
create_example_files() {
    print_header "Creating Example Files"
    
    print_step "Creating autonomous driving example..."
    
    mkdir -p examples/use-cases/autonomous-driving
    cat > "examples/use-cases/autonomous-driving/run-pipeline.sh" << 'EOF'
#!/bin/bash

# Autonomous Driving Pipeline Example

echo "ðŸš— Running Autonomous Driving Video Synthesis Pipeline Example"

# Configuration
API_URL="http://localhost:8080"
PIPELINE_CONFIG='{
  "vertical": "autonomous_vehicles",
  "data_sources": [
    {
      "source_type": "web",
      "url": "https://example.com/traffic-datasets",
      "quality_filters": {
        "min_resolution": "1080p",
        "min_duration": 30
      }
    }
  ],
  "generation_config": {
    "engine": "unreal",
    "scenarios": ["highway_driving", "urban_navigation", "weather_conditions"],
    "duration_seconds": 120,
    "resolution": "1920x1080",
    "weather_conditions": ["clear", "rain", "fog"],
    "traffic_density": "medium",
    "vehicle_count": 15
  },
  "quality_requirements": {
    "min_label_accuracy": 0.95,
    "max_frame_lag_ms": 100,
    "min_object_detection_precision": 0.95,
    "safety_critical": true
  },
  "delivery_config": {
    "format": "mp4",
    "delivery_method": "streaming",
    "include_annotations": true
  }
}'

# Start pipeline
echo "ðŸš€ Starting autonomous driving pipeline..."
RESPONSE=$(curl -s -X POST "$API_URL/api/v1/pipeline/start" \
  -H "Content-Type: application/json" \
  -d "$PIPELINE_CONFIG")

PIPELINE_ID=$(echo $RESPONSE | jq -r '.pipeline_id')
echo "ðŸ“‹ Pipeline ID: $PIPELINE_ID"

# Monitor progress
echo "ðŸ“Š Monitoring pipeline progress..."
while true; do
  STATUS=$(curl -s "$API_URL/api/v1/pipeline/status/$PIPELINE_ID")
  CURRENT_STATUS=$(echo $STATUS | jq -r '.status')
  PROGRESS=$(echo $STATUS | jq -r '.progress')
  STAGE=$(echo $STATUS | jq -r '.current_stage')
  
  echo "Status: $CURRENT_STATUS | Stage: $STAGE | Progress: $PROGRESS%"
  
  if [ "$CURRENT_STATUS" = "completed" ] || [ "$CURRENT_STATUS" = "failed" ]; then
    break
  fi
  
  sleep 10
done

echo "âœ… Pipeline $CURRENT_STATUS!"

# Show final results
if [ "$CURRENT_STATUS" = "completed" ]; then
  echo "ðŸŽ‰ Autonomous driving video synthesis completed successfully!"
  echo "ðŸ“Š Final status:"
  curl -s "$API_URL/api/v1/pipeline/status/$PIPELINE_ID" | jq '.'
else
  echo "âŒ Pipeline failed. Check logs for details."
fi
EOF

    chmod +x examples/use-cases/autonomous-driving/run-pipeline.sh

    print_step "Creating robotics example..."
    
    mkdir -p examples/use-cases/robotics-training
    cat > "examples/use-cases/robotics-training/manipulation_tasks.py" << 'EOF'
#!/usr/bin/env python3
"""
Robotics Manipulation Tasks Example
Demonstrates synthetic video generation for robot training
"""

import requests
import json
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RoboticsVideoGenerator:
    def __init__(self, api_url="http://localhost:8080"):
        self.api_url = api_url
        
    def generate_manipulation_dataset(self):
        """Generate a complete manipulation training dataset"""
        
        config = {
            "vertical": "robotics",
            "data_sources": [
                {
                    "source_type": "reference",
                    "scenarios": ["grasping", "assembly", "sorting"],
                    "quality_filters": {
                        "min_precision_mm": 0.1,
                        "min_success_rate": 0.9
                    }
                }
            ],
            "generation_config": {
                "engine": "unity",
                "robot_type": "robotic_arm",
                "tasks": [
                    "object_grasping",
                    "precision_placement", 
                    "assembly_operations"
                ],
                "objects": [
                    "bottles", "boxes", "tools", "electronic_components"
                ],
                "environments": ["factory_floor", "laboratory"],
                "duration_seconds": 300,
                "variations": 50
            },
            "quality_requirements": {
                "min_label_accuracy": 0.98,
                "max_frame_lag_ms": 50,
                "manipulation_precision_mm": 0.1,
                "physics_accuracy": 0.95
            },
            "delivery_config": {
                "format": "mp4",
                "include_annotations": True,
                "include_joint_states": True,
                "delivery_method": "sdk"
            }
        }
        
        # Start pipeline
        logger.info("Starting robotics manipulation pipeline...")
        response = requests.post(
            f"{self.api_url}/api/v1/pipeline/start",
            json=config
        )
        
        if response.status_code != 200:
            logger.error(f"Failed to start pipeline: {response.text}")
            return None
            
        pipeline_id = response.json()["pipeline_id"]
        logger.info(f"Pipeline started with ID: {pipeline_id}")
        
        # Monitor progress
        return self.monitor_pipeline(pipeline_id)
    
    def monitor_pipeline(self, pipeline_id):
        """Monitor pipeline progress"""
        while True:
            response = requests.get(
                f"{self.api_url}/api/v1/pipeline/status/{pipeline_id}"
            )
            
            if response.status_code != 200:
                logger.error("Failed to get pipeline status")
                break
                
            status = response.json()
            current_status = status["status"]
            progress = status["progress"]
            stage = status["current_stage"]
            
            logger.info(f"Status: {current_status} | Stage: {stage} | Progress: {progress}%")
            
            if current_status in ["completed", "failed"]:
                break
                
            time.sleep(15)
        
        if current_status == "completed":
            logger.info("ðŸŽ‰ Robotics dataset generation completed!")
            return status
        else:
            logger.error("âŒ Pipeline failed")
            return None

def main():
    generator = RoboticsVideoGenerator()
    result = generator.generate_manipulation_dataset()
    
    if result:
        print("âœ… Robotics training dataset ready!")
        print(f"ðŸ“Š Generated videos with manipulation precision: {result['metadata'].get('precision', 'N/A')}")
    else:
        print("âŒ Dataset generation failed")

if __name__ == "__main__":
    main()
EOF

    chmod +x examples/use-cases/robotics-training/manipulation_tasks.py

    print_success "Example files created!"
}

# Create CI/CD files
create_cicd_files() {
    print_header "Creating CI/CD Files"
    
    print_step "Creating GitHub Actions workflow..."
    
    cat > ".github/workflows/ci-cd.yml" << 'EOF'
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  DOCKER_REGISTRY: ghcr.io
  IMAGE_PREFIX: inferloop-synthdata

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, "3.10", "3.11"]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-cov pytest-mock
    
    - name: Run unit tests
      run: |
        pytest qa/test-suites/unit-tests/ -v --cov=services --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  lint:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install black flake8 mypy isort
    
    - name: Run black
      run: black --check services/
    
    - name: Run flake8
      run: flake8 services/
    
    - name: Run isort
      run: isort --check-only services/
    
    - name: Run mypy
      run: mypy services/

  build:
    needs: [test, lint]
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.DOCKER_REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
    
    - name: Build and push Docker images
      run: |
        services=("orchestration-service" "ingestion-service" "metrics-extraction-service" "generation-service" "validation-service" "delivery-service")
        
        for service in "${services[@]}"; do
          echo "Building $service..."
          docker buildx build \
            --platform linux/amd64,linux/arm64 \
            --tag ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/$service:${{ github.sha }} \
            --tag ${{ env.DOCKER_REGISTRY }}/${{ github.repository }}/$service:latest \
            --push \
            ./services/$service
        done

  integration-test:
    needs: build
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Compose
      run: |
        sudo apt-get update
        sudo apt-get install docker-compose
    
    - name: Start services
      run: |
        docker-compose up -d
        sleep 60
    
    - name: Run health checks
      run: |
        ./scripts/deployment/health-check.sh
    
    - name: Run integration tests
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pytest qa/test-suites/integration-tests/ -v
    
    - name: Cleanup
      if: always()
      run: |
        docker-compose down -v

  security-scan:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  deploy-staging:
    needs: [test, lint, build, integration-test]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add deployment commands here
        # kubectl apply -f infrastructure/kubernetes/manifests/staging/
    
    - name: Run smoke tests
      run: |
        echo "Running smoke tests on staging..."
        # Add smoke test commands here

  deploy-production:
    needs: [test, lint, build, integration-test, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add production deployment commands here
        # kubectl apply -f infrastructure/kubernetes/manifests/production/
    
    - name: Run post-deployment tests
      run: |
        echo "Running post-deployment verification..."
        # Add verification commands here
EOF

    print_success "CI/CD files created!"
}

# Final setup and summary
final_setup() {
    print_header "Final Setup and Summary"
    
    print_step "Setting executable permissions..."
    find scripts/ -name "*.sh" -exec chmod +x {} \;
    find examples/ -name "*.sh" -exec chmod +x {} \;
    find examples/ -name "*.py" -exec chmod +x {} \;
    
    print_step "Creating additional service files..."
    # Create placeholder Dockerfiles for remaining services
    for service in metrics-extraction-service generation-service validation-service delivery-service; do
        if [ ! -f "services/$service/Dockerfile" ]; then
            cp "services/ingestion-service/Dockerfile" "services/$service/Dockerfile"
            cp "services/ingestion-service/requirements.txt" "services/$service/requirements.txt"
        fi
    done
    
    print_step "Initializing git repository (if not already initialized)..."
    if [ ! -d .git ]; then
        git init
        git add .
        git commit -m "Initial commit: Enterprise Video Synthesis Pipeline"
    fi
    
    print_success "Repository setup completed successfully!"
}

# Print final summary
print_final_summary() {
    print_header "ðŸŽ‰ Enterprise Video Synthesis Pipeline Created Successfully!"
    
    echo ""
    print_success "Repository Structure: $PROJECT_ROOT"
    echo ""
    print_info "ðŸ“‚ Created Components:"
    echo "  â€¢ 6 Core Microservices with Docker containers"
    echo "  â€¢ 7 Industry Verticals with specific configurations"
    echo "  â€¢ 8 Integration Methods (REST, GraphQL, gRPC, etc.)"
    echo "  â€¢ Complete Infrastructure as Code (Terraform, Kubernetes)"
    echo "  â€¢ Comprehensive Monitoring and Logging"
    echo "  â€¢ CI/CD Pipelines (GitHub Actions)"
    echo "  â€¢ Extensive Documentation and Examples"
    echo ""
    print_info "ðŸš€ Quick Start Commands:"
    echo "  cd $REPO_NAME"
    echo "  ./scripts/setup/dev-environment.sh"
    echo "  make deploy"
    echo "  make status"
    echo ""
    print_info "ðŸŒ Service Endpoints (after deployment):"
    echo "  â€¢ API Gateway: http://localhost:8080"
    echo "  â€¢ Grafana: http://localhost:3000 (admin/admin)"
    echo "  â€¢ MinIO: http://localhost:9001 (minioadmin/minioadmin)"
    echo "  â€¢ Prometheus: http://localhost:9090"
    echo ""
    print_info "ðŸ“š Key Directories:"
    echo "  â€¢ services/ - Core microservices"
    echo "  â€¢ verticals/ - Industry-specific configurations"
    echo "  â€¢ examples/ - Use case demonstrations"
    echo "  â€¢ docs/ - Comprehensive documentation"
    echo "  â€¢ scripts/ - Automation and utilities"
    echo ""
    print_info "ðŸŽ¯ Supported Verticals:"
    echo "  â€¢ ðŸš— Autonomous Vehicles (95% accuracy, <100ms latency)"
    echo "  â€¢ ðŸ¤– Robotics (98% precision, Â±0.1mm accuracy)"
    echo "  â€¢ ðŸ™ï¸ Smart Cities (GDPR compliant, 10K+ agents)"
    echo "  â€¢ ðŸŽ® Gaming (AAA quality, 60+ FPS)"
    echo "  â€¢ ðŸ¥ Healthcare (HIPAA compliant, 99% accuracy)"
    echo "  â€¢ ðŸ­ Manufacturing (99.9% safety, industrial grade)"
    echo "  â€¢ ðŸ›’ Retail (Privacy compliant, behavioral accuracy)"
    echo ""
    print_info "ðŸ“Š Quality Benchmarks:"
    echo "  â€¢ Label Accuracy: >92%"
    echo "  â€¢ Frame Lag: <300ms"
    echo "  â€¢ PSNR: >25 dB"
    echo "  â€¢ SSIM: >0.8"
    echo "  â€¢ Privacy Score: 100%"
    echo ""
    print_success "ðŸŽ¬ Ready to generate enterprise-grade synthetic video data!"
    echo ""
    print_info "Next steps:"
    echo "  1. Navigate to the project: cd $REPO_NAME"
    echo "  2. Review and customize .env configuration"
    echo "  3. Run the setup script: ./scripts/setup/dev-environment.sh"
    echo "  4. Deploy the stack: make deploy"
    echo "  5. Try example workflows in examples/"
    echo ""
    print_info "For support and documentation:"
    echo "  â€¢ README.md - Project overview"
    echo "  â€¢ docs/ - Detailed documentation"
    echo "  â€¢ examples/ - Working examples"
    echo "  â€¢ make help - Available commands"
}

# Main execution
main() {
    print_header "Enterprise Video Synthesis Pipeline - Repository Builder v$SCRIPT_VERSION"
    
    check_prerequisites
    create_directories
    create_core_files
    create_service_files
    create_script_files
    create_configuration_files
    create_documentation_files
    create_example_files
    create_cicd_files
    final_setup
    print_final_summary
}

# Run main function
main "$@"
