"""\nBatch processing examples for high-volume document generation.\n\nDemonstrates efficient batch processing techniques including:\n- Parallel document generation\n- Progress tracking and monitoring\n- Error handling and retry logic\n- Resource optimization\n- Export and delivery workflows\n"""\n\nimport asyncio\nimport json\nimport time\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional, AsyncIterator\nfrom concurrent.futures import ThreadPoolExecutor\nimport aiofiles\n\nfrom structured_docs_synth import SynthClient\nfrom structured_docs_synth.delivery.export import BatchExporter\nfrom structured_docs_synth.orchestration.monitoring import ProgressTracker\nfrom structured_docs_synth.core.exceptions import GenerationError\n\n\nclass BatchProcessor:\n    """High-performance batch document processor."""\n    \n    def __init__(self, api_key: str = None, max_concurrent: int = 10):\n        """\n        Initialize batch processor.\n        \n        Args:\n            api_key: API key for authentication\n            max_concurrent: Maximum concurrent processing tasks\n        """\n        self.client = SynthClient(api_key=api_key)\n        self.max_concurrent = max_concurrent\n        self.progress_tracker = ProgressTracker()\n        self.exporter = BatchExporter()\n        self.error_log = []\n    \n    async def process_large_dataset(\n        self,\n        input_file: str,\n        document_type: str,\n        output_dir: str,\n        batch_size: int = 100\n    ) -> Dict[str, Any]:\n        """\n        Process a large dataset file and generate documents.\n        \n        Args:\n            input_file: Path to input data file (CSV, JSON, or Excel)\n            document_type: Type of document to generate\n            output_dir: Directory for output files\n            batch_size: Number of records per batch\n        \n        Returns:\n            Processing results and statistics\n        """\n        print(f"Processing large dataset: {input_file}")\n        start_time = time.time()\n        \n        # Create output directory\n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        # Initialize tracking\n        job_id = self.progress_tracker.create_job(\n            name=f"Batch_{document_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",\n            total_items=0  # Will update after counting\n        )\n        \n        results = {\n            "job_id": job_id,\n            "processed": 0,\n            "succeeded": 0,\n            "failed": 0,\n            "documents": [],\n            "errors": []\n        }\n        \n        try:\n            # Load and process data in batches\n            async for batch in self._read_data_batches(input_file, batch_size):\n                batch_results = await self._process_batch(\n                    batch=batch,\n                    document_type=document_type,\n                    output_dir=output_path,\n                    job_id=job_id\n                )\n                \n                # Update results\n                results["processed"] += len(batch)\n                results["succeeded"] += batch_results["succeeded"]\n                results["failed"] += batch_results["failed"]\n                results["documents"].extend(batch_results["documents"])\n                results["errors"].extend(batch_results["errors"])\n                \n                # Update progress\n                await self.progress_tracker.update_progress(\n                    job_id,\n                    processed=results["processed"]\n                )\n        \n        except Exception as e:\n            results["errors"].append({\n                "type": "processing_error",\n                "message": str(e),\n                "timestamp": datetime.now().isoformat()\n            })\n        \n        finally:\n            # Finalize job\n            processing_time = time.time() - start_time\n            await self.progress_tracker.complete_job(job_id)\n            \n            results["statistics"] = {\n                "total_time_seconds": processing_time,\n                "documents_per_second": results["succeeded"] / processing_time if processing_time > 0 else 0,\n                "success_rate": results["succeeded"] / results["processed"] if results["processed"] > 0 else 0,\n                "average_time_per_document": processing_time / results["succeeded"] if results["succeeded"] > 0 else 0\n            }\n        \n        return results\n    \n    async def parallel_generation(\n        self,\n        templates: List[str],\n        data_source: str,\n        output_formats: List[str] = ["pdf", "json"]\n    ) -> Dict[str, Any]:\n        """\n        Generate documents in parallel using multiple templates.\n        \n        Args:\n            templates: List of template names to use\n            data_source: Data source identifier or path\n            output_formats: Desired output formats\n        \n        Returns:\n            Generation results\n        """\n        print(f"Starting parallel generation with {len(templates)} templates")\n        \n        # Create tasks for parallel execution\n        tasks = []\n        semaphore = asyncio.Semaphore(self.max_concurrent)\n        \n        # Load data\n        data_items = await self._load_data_source(data_source)\n        total_tasks = len(templates) * len(data_items)\n        \n        print(f"Generating {total_tasks} documents ({len(data_items)} items × {len(templates)} templates)")\n        \n        results = {\n            "total_tasks": total_tasks,\n            "completed": 0,\n            "by_template": {template: [] for template in templates}\n        }\n        \n        async def generate_with_template(template: str, data_item: Dict[str, Any]):\n            async with semaphore:\n                try:\n                    document = await self.client.generate_document(\n                        template=template,\n                        data=data_item,\n                        formats=output_formats\n                    )\n                    results["by_template"][template].append(document)\n                    results["completed"] += 1\n                    \n                    # Progress update\n                    if results["completed"] % 10 == 0:\n                        print(f"Progress: {results['completed']}/{total_tasks} documents generated")\n                    \n                    return document\n                    \n                except Exception as e:\n                    self.error_log.append({\n                        "template": template,\n                        "data_id": data_item.get("id", "unknown"),\n                        "error": str(e)\n                    })\n                    return None\n        \n        # Create all tasks\n        for template in templates:\n            for data_item in data_items:\n                task = generate_with_template(template, data_item)\n                tasks.append(task)\n        \n        # Execute all tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n        \n        return results\n    \n    async def stream_process(\n        self,\n        data_stream: AsyncIterator[Dict[str, Any]],\n        document_type: str,\n        output_handler: callable\n    ) -> Dict[str, Any]:\n        """\n        Process documents from a streaming data source.\n        \n        Args:\n            data_stream: Async iterator of data items\n            document_type: Type of document to generate\n            output_handler: Async function to handle generated documents\n        \n        Returns:\n            Processing statistics\n        """\n        print("Starting stream processing...")\n        \n        stats = {\n            "processed": 0,\n            "succeeded": 0,\n            "failed": 0,\n            "start_time": time.time()\n        }\n        \n        # Process stream with concurrent workers\n        queue = asyncio.Queue(maxsize=self.max_concurrent * 2)\n        workers = []\n        \n        async def worker(worker_id: int):\n            while True:\n                try:\n                    # Get item from queue\n                    item = await asyncio.wait_for(queue.get(), timeout=5.0)\n                    if item is None:  # Poison pill\n                        break\n                    \n                    # Generate document\n                    try:\n                        document = await self.client.generate_document(\n                            document_type=document_type,\n                            data=item\n                        )\n                        \n                        # Handle output\n                        await output_handler(document)\n                        stats["succeeded"] += 1\n                        \n                    except Exception as e:\n                        stats["failed"] += 1\n                        self.error_log.append({\n                            "item_id": item.get("id", "unknown"),\n                            "error": str(e),\n                            "worker_id": worker_id\n                        })\n                    \n                    finally:\n                        stats["processed"] += 1\n                        queue.task_done()\n                        \n                except asyncio.TimeoutError:\n                    continue\n        \n        # Start workers\n        for i in range(self.max_concurrent):\n            worker_task = asyncio.create_task(worker(i))\n            workers.append(worker_task)\n        \n        # Feed data to queue\n        async for item in data_stream:\n            await queue.put(item)\n        \n        # Signal workers to stop\n        for _ in range(self.max_concurrent):\n            await queue.put(None)\n        \n        # Wait for workers to complete\n        await asyncio.gather(*workers)\n        \n        stats["processing_time"] = time.time() - stats["start_time"]\n        stats["throughput"] = stats["processed"] / stats["processing_time"] if stats["processing_time"] > 0 else 0\n        \n        return stats\n    \n    async def export_batch_results(\n        self,\n        documents: List[Dict[str, Any]],\n        export_format: str = "zip",\n        include_metadata: bool = True\n    ) -> Dict[str, Any]:\n        """\n        Export batch processing results.\n        \n        Args:\n            documents: List of generated documents\n            export_format: Export format (zip, tar, folder)\n            include_metadata: Include generation metadata\n        \n        Returns:\n            Export results\n        """\n        print(f"Exporting {len(documents)} documents as {export_format}")\n        \n        export_job = await self.exporter.create_export_job(\n            documents=documents,\n            format=export_format,\n            options={\n                "include_metadata": include_metadata,\n                "compress": True,\n                "organize_by_type": True\n            }\n        )\n        \n        # Monitor export progress\n        while export_job["status"] == "processing":\n            await asyncio.sleep(1)\n            export_job = await self.exporter.get_job_status(export_job["id"])\n            print(f"Export progress: {export_job['progress']}%")\n        \n        if export_job["status"] == "completed":\n            print(f"Export completed: {export_job['output_path']}")\n        else:\n            print(f"Export failed: {export_job.get('error', 'Unknown error')}")\n        \n        return export_job\n    \n    async def retry_failed_documents(\n        self,\n        failed_items: List[Dict[str, Any]],\n        max_retries: int = 3\n    ) -> Dict[str, Any]:\n        """\n        Retry generation for failed documents.\n        \n        Args:\n            failed_items: List of failed document generation attempts\n            max_retries: Maximum number of retry attempts\n        \n        Returns:\n            Retry results\n        """\n        print(f"Retrying {len(failed_items)} failed documents")\n        \n        retry_results = {\n            "total_retried": len(failed_items),\n            "succeeded": 0,\n            "failed": 0,\n            "permanent_failures": []\n        }\n        \n        for item in failed_items:\n            retries = 0\n            success = False\n            \n            while retries < max_retries and not success:\n                try:\n                    # Exponential backoff\n                    if retries > 0:\n                        await asyncio.sleep(2 ** retries)\n                    \n                    # Retry generation\n                    document = await self.client.generate_document(\n                        document_type=item["document_type"],\n                        data=item["data"],\n                        formats=item.get("formats", ["pdf"])\n                    )\n                    \n                    retry_results["succeeded"] += 1\n                    success = True\n                    \n                except Exception as e:\n                    retries += 1\n                    if retries >= max_retries:\n                        retry_results["failed"] += 1\n                        retry_results["permanent_failures"].append({\n                            "item": item,\n                            "final_error": str(e),\n                            "attempts": retries\n                        })\n        \n        return retry_results\n    \n    async def _read_data_batches(\n        self,\n        file_path: str,\n        batch_size: int\n    ) -> AsyncIterator[List[Dict[str, Any]]]:\n        """Read data file in batches."""\n        file_path = Path(file_path)\n        \n        if file_path.suffix == ".json":\n            async with aiofiles.open(file_path, 'r') as f:\n                content = await f.read()\n                data = json.loads(content)\n                \n                # Yield in batches\n                for i in range(0, len(data), batch_size):\n                    yield data[i:i + batch_size]\n        \n        elif file_path.suffix == ".csv":\n            # For CSV, we'd use an async CSV reader\n            import csv\n            batch = []\n            \n            async with aiofiles.open(file_path, 'r') as f:\n                async for line in f:\n                    # Simple CSV parsing (in production, use proper CSV library)\n                    if batch and len(batch) >= batch_size:\n                        yield batch\n                        batch = []\n                    batch.append(json.loads(line))  # Assuming JSON lines\n                \n                if batch:\n                    yield batch\n    \n    async def _process_batch(\n        self,\n        batch: List[Dict[str, Any]],\n        document_type: str,\n        output_dir: Path,\n        job_id: str\n    ) -> Dict[str, Any]:\n        """Process a single batch of documents."""\n        batch_results = {\n            "succeeded": 0,\n            "failed": 0,\n            "documents": [],\n            "errors": []\n        }\n        \n        # Process items concurrently within batch\n        semaphore = asyncio.Semaphore(self.max_concurrent)\n        \n        async def process_item(item: Dict[str, Any], index: int):\n            async with semaphore:\n                try:\n                    # Generate document\n                    document = await self.client.generate_document(\n                        document_type=document_type,\n                        data=item,\n                        formats=["pdf", "json"]\n                    )\n                    \n                    # Save to output directory\n                    doc_id = document["id"]\n                    for format_type, content in document["outputs"].items():\n                        output_file = output_dir / f"{doc_id}.{format_type}"\n                        async with aiofiles.open(output_file, 'wb') as f:\n                            await f.write(content)\n                    \n                    batch_results["succeeded"] += 1\n                    batch_results["documents"].append({\n                        "id": doc_id,\n                        "path": str(output_dir / doc_id),\n                        "formats": list(document["outputs"].keys())\n                    })\n                    \n                except Exception as e:\n                    batch_results["failed"] += 1\n                    batch_results["errors"].append({\n                        "item_index": index,\n                        "error": str(e),\n                        "data": item\n                    })\n        \n        # Process all items in batch\n        tasks = [\n            process_item(item, i)\n            for i, item in enumerate(batch)\n        ]\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n        \n        return batch_results\n    \n    async def _load_data_source(self, data_source: str) -> List[Dict[str, Any]]:\n        """Load data from various sources."""\n        # This is a simplified implementation\n        # In production, support multiple data sources (DB, API, files)\n        \n        if data_source.startswith("file://"):\n            file_path = data_source.replace("file://", "")\n            async with aiofiles.open(file_path, 'r') as f:\n                content = await f.read()\n                return json.loads(content)\n        \n        elif data_source.startswith("api://"):\n            # Would implement API data fetching\n            pass\n        \n        else:\n            # Assume it's a file path\n            async with aiofiles.open(data_source, 'r') as f:\n                content = await f.read()\n                return json.loads(content)\n\n\n# Example usage\nasync def main():\n    # Initialize batch processor\n    processor = BatchProcessor(max_concurrent=20)\n    \n    # Example 1: Process large dataset\n    print("\n=== Large Dataset Processing ===")\n    results = await processor.process_large_dataset(\n        input_file="data/customers.json",\n        document_type="invoice",\n        output_dir="output/invoices",\n        batch_size=50\n    )\n    print(f"Processed {results['processed']} documents")\n    print(f"Success rate: {results['statistics']['success_rate']:.2%}")\n    print(f"Throughput: {results['statistics']['documents_per_second']:.2f} docs/sec")\n    \n    # Example 2: Parallel generation with multiple templates\n    print("\n=== Parallel Template Generation ===")\n    parallel_results = await processor.parallel_generation(\n        templates=["invoice_standard", "invoice_detailed", "invoice_summary"],\n        data_source="data/transactions.json",\n        output_formats=["pdf", "docx", "json"]\n    )\n    print(f"Generated {parallel_results['completed']} documents using {len(parallel_results['by_template'])} templates")\n    \n    # Example 3: Stream processing\n    print("\n=== Stream Processing ===")\n    \n    async def data_stream_generator():\n        """Simulate a data stream."""\n        for i in range(100):\n            yield {\n                "id": f"stream_{i}",\n                "amount": 100 + i * 10,\n                "customer": f"Customer {i}"\n            }\n            await asyncio.sleep(0.1)  # Simulate streaming delay\n    \n    async def output_handler(document):\n        """Handle streamed document output."""\n        print(f"Received document: {document['id']}")\n        # Would save or forward the document\n    \n    stream_stats = await processor.stream_process(\n        data_stream=data_stream_generator(),\n        document_type="receipt",\n        output_handler=output_handler\n    )\n    print(f"Stream processing completed: {stream_stats['succeeded']} documents")\n    print(f"Throughput: {stream_stats['throughput']:.2f} docs/sec")\n    \n    # Example 4: Export results\n    print("\n=== Batch Export ===")\n    if results["documents"]:\n        export_result = await processor.export_batch_results(\n            documents=results["documents"][:10],  # Export first 10\n            export_format="zip",\n            include_metadata=True\n        )\n        print(f"Export status: {export_result['status']}")\n    \n    # Example 5: Retry failed documents\n    if results["errors"]:\n        print("\n=== Retrying Failed Documents ===")\n        retry_results = await processor.retry_failed_documents(\n            failed_items=results["errors"][:5],  # Retry first 5\n            max_retries=3\n        )\n        print(f"Retry results: {retry_results['succeeded']} succeeded, {retry_results['failed']} failed")\n\n\nif __name__ == "__main__":\n    asyncio.run(main())