"""\nPrometheus exporter for AI model metrics.\n\nCollects and exposes metrics related to AI model performance, inference times,\naccuracy, and resource utilization for synthetic document generation.\n"""\n\nimport asyncio\nimport logging\nimport time\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nimport psutil\nimport GPUtil\n\nfrom prometheus_client import (\n    Counter, Gauge, Histogram, Summary,\n    start_http_server, generate_latest,\n    CollectorRegistry, CONTENT_TYPE_LATEST\n)\nfrom prometheus_client.core import GaugeMetricFamily, CounterMetricFamily\n\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\n# Metric definitions\nmodel_inference_counter = Counter(\n    'ai_model_inference_total',\n    'Total number of model inference requests',\n    ['model_name', 'model_version', 'task_type']\n)\n\nmodel_inference_duration = Histogram(\n    'ai_model_inference_duration_seconds',\n    'Time spent in model inference',\n    ['model_name', 'model_version', 'task_type'],\n    buckets=(0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0)\n)\n\nmodel_accuracy_gauge = Gauge(\n    'ai_model_accuracy',\n    'Current model accuracy score',\n    ['model_name', 'model_version', 'metric_type']\n)\n\nmodel_error_counter = Counter(\n    'ai_model_errors_total',\n    'Total number of model errors',\n    ['model_name', 'model_version', 'error_type']\n)\n\nmodel_load_time = Gauge(\n    'ai_model_load_time_seconds',\n    'Time taken to load the model',\n    ['model_name', 'model_version']\n)\n\nmodel_memory_usage = Gauge(\n    'ai_model_memory_usage_bytes',\n    'Memory usage of loaded models',\n    ['model_name', 'model_version']\n)\n\ngpu_utilization = Gauge(\n    'ai_gpu_utilization_percent',\n    'GPU utilization percentage',\n    ['gpu_id', 'gpu_name']\n)\n\ngpu_memory_usage = Gauge(\n    'ai_gpu_memory_usage_bytes',\n    'GPU memory usage',\n    ['gpu_id', 'gpu_name', 'memory_type']\n)\n\ntoken_usage_counter = Counter(\n    'ai_token_usage_total',\n    'Total tokens processed',\n    ['model_name', 'token_type']  # token_type: input, output\n)\n\nbatch_processing_gauge = Gauge(\n    'ai_batch_size',\n    'Current batch size being processed',\n    ['model_name', 'task_type']\n)\n\nmodel_cache_hits = Counter(\n    'ai_model_cache_hits_total',\n    'Number of cache hits for model predictions',\n    ['model_name', 'cache_type']\n)\n\nmodel_cache_misses = Counter(\n    'ai_model_cache_misses_total',\n    'Number of cache misses for model predictions',\n    ['model_name', 'cache_type']\n)\n\n\nclass AIMetricsCollector:\n    """Custom collector for AI-specific metrics."""\n    \n    def __init__(self):\n        self.model_registry = {}\n        self.start_time = time.time()\n    \n    def register_model(self, model_name: str, model_version: str, model_info: Dict[str, Any]):\n        """Register a model for monitoring."""\n        self.model_registry[f"{model_name}:{model_version}"] = {\n            'name': model_name,\n            'version': model_version,\n            'info': model_info,\n            'loaded_at': datetime.now(),\n            'inference_count': 0,\n            'error_count': 0,\n            'total_tokens': 0\n        }\n        \n        # Record model load time\n        if 'load_time' in model_info:\n            model_load_time.labels(\n                model_name=model_name,\n                model_version=model_version\n            ).set(model_info['load_time'])\n    \n    def record_inference(self, model_name: str, model_version: str, task_type: str,\n                        duration: float, success: bool = True, tokens: Dict[str, int] = None):\n        """Record model inference metrics."""\n        # Update counters\n        model_inference_counter.labels(\n            model_name=model_name,\n            model_version=model_version,\n            task_type=task_type\n        ).inc()\n        \n        # Record duration\n        model_inference_duration.labels(\n            model_name=model_name,\n            model_version=model_version,\n            task_type=task_type\n        ).observe(duration)\n        \n        # Update model registry\n        model_key = f"{model_name}:{model_version}"\n        if model_key in self.model_registry:\n            self.model_registry[model_key]['inference_count'] += 1\n            \n            if not success:\n                self.model_registry[model_key]['error_count'] += 1\n        \n        # Record token usage\n        if tokens:\n            for token_type, count in tokens.items():\n                token_usage_counter.labels(\n                    model_name=model_name,\n                    token_type=token_type\n                ).inc(count)\n                \n                if model_key in self.model_registry:\n                    self.model_registry[model_key]['total_tokens'] += count\n    \n    def record_error(self, model_name: str, model_version: str, error_type: str):\n        """Record model error."""\n        model_error_counter.labels(\n            model_name=model_name,\n            model_version=model_version,\n            error_type=error_type\n        ).inc()\n    \n    def update_accuracy(self, model_name: str, model_version: str, \n                       metric_type: str, accuracy: float):\n        """Update model accuracy metrics."""\n        model_accuracy_gauge.labels(\n            model_name=model_name,\n            model_version=model_version,\n            metric_type=metric_type\n        ).set(accuracy)\n    \n    def update_gpu_metrics(self):\n        """Update GPU utilization metrics."""\n        try:\n            gpus = GPUtil.getGPUs()\n            for gpu in gpus:\n                # GPU utilization\n                gpu_utilization.labels(\n                    gpu_id=str(gpu.id),\n                    gpu_name=gpu.name\n                ).set(gpu.load * 100)\n                \n                # GPU memory\n                gpu_memory_usage.labels(\n                    gpu_id=str(gpu.id),\n                    gpu_name=gpu.name,\n                    memory_type='used'\n                ).set(gpu.memoryUsed * 1024 * 1024)  # Convert to bytes\n                \n                gpu_memory_usage.labels(\n                    gpu_id=str(gpu.id),\n                    gpu_name=gpu.name,\n                    memory_type='free'\n                ).set(gpu.memoryFree * 1024 * 1024)\n                \n        except Exception as e:\n            logger.warning(f"Failed to collect GPU metrics: {e}")\n    \n    def update_model_memory(self, model_name: str, model_version: str, memory_bytes: int):\n        """Update model memory usage."""\n        model_memory_usage.labels(\n            model_name=model_name,\n            model_version=model_version\n        ).set(memory_bytes)\n    \n    def update_batch_size(self, model_name: str, task_type: str, batch_size: int):\n        """Update current batch processing size."""\n        batch_processing_gauge.labels(\n            model_name=model_name,\n            task_type=task_type\n        ).set(batch_size)\n    \n    def record_cache_hit(self, model_name: str, cache_type: str = 'prediction'):\n        """Record cache hit."""\n        model_cache_hits.labels(\n            model_name=model_name,\n            cache_type=cache_type\n        ).inc()\n    \n    def record_cache_miss(self, model_name: str, cache_type: str = 'prediction'):\n        """Record cache miss."""\n        model_cache_misses.labels(\n            model_name=model_name,\n            cache_type=cache_type\n        ).inc()\n    \n    def collect(self):\n        """Collect custom metrics."""\n        # Model-specific metrics\n        for model_key, model_data in self.model_registry.items():\n            name = model_data['name']\n            version = model_data['version']\n            \n            # Inference rate (per minute)\n            uptime_minutes = (time.time() - self.start_time) / 60\n            inference_rate = model_data['inference_count'] / max(uptime_minutes, 1)\n            \n            yield GaugeMetricFamily(\n                'ai_model_inference_rate_per_minute',\n                'Model inference rate per minute',\n                value=inference_rate,\n                labels=['model_name', 'model_version'],\n                unit='per_minute'\n            )\n            \n            # Error rate\n            error_rate = (model_data['error_count'] / max(model_data['inference_count'], 1)) * 100\n            \n            yield GaugeMetricFamily(\n                'ai_model_error_rate_percent',\n                'Model error rate percentage',\n                value=error_rate,\n                labels=['model_name', 'model_version']\n            )\n            \n            # Average tokens per request\n            avg_tokens = model_data['total_tokens'] / max(model_data['inference_count'], 1)\n            \n            yield GaugeMetricFamily(\n                'ai_model_avg_tokens_per_request',\n                'Average tokens per request',\n                value=avg_tokens,\n                labels=['model_name', 'model_version']\n            )\n\n\nclass AIMetricsExporter:\n    """Main exporter class for AI metrics."""\n    \n    def __init__(self, port: int = 9090):\n        self.port = port\n        self.collector = AIMetricsCollector()\n        self.registry = CollectorRegistry()\n        self.registry.register(self.collector)\n        self.running = False\n    \n    async def start(self):\n        """Start the metrics exporter."""\n        try:\n            # Start HTTP server\n            start_http_server(self.port, registry=self.registry)\n            logger.info(f"AI Metrics Exporter started on port {self.port}")\n            \n            self.running = True\n            \n            # Start background tasks\n            await asyncio.gather(\n                self._update_system_metrics(),\n                self._cleanup_old_metrics()\n            )\n            \n        except Exception as e:\n            logger.error(f"Failed to start exporter: {e}")\n            raise\n    \n    async def _update_system_metrics(self):\n        """Periodically update system metrics."""\n        while self.running:\n            try:\n                # Update GPU metrics\n                self.collector.update_gpu_metrics()\n                \n                # Update CPU metrics\n                cpu_percent = psutil.cpu_percent(interval=1)\n                memory = psutil.virtual_memory()\n                \n                # You can add these as custom metrics if needed\n                \n            except Exception as e:\n                logger.error(f"Error updating system metrics: {e}")\n            \n            await asyncio.sleep(10)  # Update every 10 seconds\n    \n    async def _cleanup_old_metrics(self):\n        """Cleanup old metrics periodically."""\n        while self.running:\n            try:\n                # Implement cleanup logic for old model entries\n                current_time = datetime.now()\n                \n                # Remove models not used in last 24 hours\n                for model_key in list(self.collector.model_registry.keys()):\n                    model_data = self.collector.model_registry[model_key]\n                    if (current_time - model_data['loaded_at']).days > 1:\n                        if model_data['inference_count'] == 0:\n                            del self.collector.model_registry[model_key]\n                            logger.info(f"Removed inactive model: {model_key}")\n                \n            except Exception as e:\n                logger.error(f"Error in cleanup: {e}")\n            \n            await asyncio.sleep(3600)  # Cleanup every hour\n    \n    def stop(self):\n        """Stop the exporter."""\n        self.running = False\n        logger.info("AI Metrics Exporter stopped")\n\n\n# Example usage\nif __name__ == "__main__":\n    async def main():\n        exporter = AIMetricsExporter(port=9090)\n        \n        # Register some example models\n        exporter.collector.register_model(\n            "gpt-document-generator",\n            "v1.0",\n            {"load_time": 5.2, "parameters": "175B"}\n        )\n        \n        exporter.collector.register_model(\n            "layout-analyzer",\n            "v2.1",\n            {"load_time": 2.8, "parameters": "50M"}\n        )\n        \n        # Simulate some metrics\n        for i in range(10):\n            # Record inference\n            exporter.collector.record_inference(\n                "gpt-document-generator",\n                "v1.0",\n                "text_generation",\n                duration=0.5 + (i * 0.1),\n                tokens={"input": 100, "output": 250}\n            )\n            \n            # Update accuracy\n            exporter.collector.update_accuracy(\n                "layout-analyzer",\n                "v2.1",\n                "iou_score",\n                0.85 + (i * 0.01)\n            )\n            \n            await asyncio.sleep(1)\n        \n        # Start exporter\n        await exporter.start()\n    \n    asyncio.run(main())